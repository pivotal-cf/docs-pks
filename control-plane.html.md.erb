---
title: Tanzu Kubernetes Grid Integrated Edition Architecture
owner: TKGI
---

This topic describes how <%= vars.product_full %> manages the deployment of Kubernetes clusters.

##<a id="overview"></a><%= vars.product_short %> Overview

An <%= vars.product_short %> environment consists of a <%= vars.k8s_runtime_abbr %> Control Plane 
and one or more workload clusters.  

<%= vars.product_short %> administrators use the <%= vars.k8s_runtime_abbr %> Control Plane to 
deploy and manage Kubernetes clusters. The workload clusters run the apps pushed by developers.  

The following illustrates the interaction between <%= vars.product_short %> components:  
<br>
<%= image_tag('images/tkgi-overview-ha.png') %>
<%# Image source: https://docs.google.com/drawings/d/1TZkaTSCiddEE7mZtOTjTg6jBuDAy0D3CI9JY56HBIAY/edit %>

Administrators access the <%= vars.k8s_runtime_abbr %> Control Plane 
through the <%= vars.k8s_runtime_abbr %> Command Line Interface (<%= vars.k8s_runtime_abbr %> CLI) installed on their local workstations.  

Within the <%= vars.k8s_runtime_abbr %> Control Plane the <%= vars.control_plane %> and <%= vars.k8s_runtime_abbr %> Broker use BOSH to execute the requested cluster management functions. 
For information about the <%= vars.k8s_runtime_abbr %> Control Plane, see [<%= vars.k8s_runtime_abbr %> Control Plane Overview](#control-plane) below. 
For instructions on installing the <%= vars.k8s_runtime_abbr %> CLI, see [Installing the <%= vars.k8s_runtime_abbr %> CLI](installing-cli.html).  

Kubernetes deploys and manages workloads on Kubernetes clusters.
Administrators use the  Kubernetes CLI, `kubectl`, to direct Kubernetes 
from their local workstations. 
For information about `kubectl`, see [Overview of kubectl]
(https://kubernetes.io/docs/reference/kubectl/overview/) in the Kubernetes documentation.  

##<a id="cluster-management"></a><a id="control-plane"></a><%= vars.k8s_runtime_abbr %> Control Plane Overview

The <%= vars.k8s_runtime_abbr %> Control Plane manages the lifecycle of Kubernetes clusters deployed 
using <%= vars.product_short %>. 

The control plane provides the following via the <%= vars.control_plane %>:

* View cluster plans
* Create clusters
* View information about clusters
* Obtain credentials to deploy workloads to clusters
* Scale clusters
* Delete clusters
* Create and manage network profiles for VMware NSX-T

In addition, the <%= vars.k8s_runtime_abbr %> Control Plane can upgrade all existing clusters using the **Upgrade all clusters** BOSH errand.
For more information, see [Upgrade Kubernetes Clusters](upgrade.html#upgrade-instances) in _Upgrading <%= vars.product_short %> (Antrea and Flannel Networking)_.

<br>
TKGI Control Plane is hosted on a pair of VM groups:

* The [<%= vars.control_plane %> VM Group](#tkgi-api-vm) for hosting cluster management services.
* The [<%= vars.control_plane_db %> VM Cluster](#tkgi-db-vm) to store cluster management data. 

###<a id="tkgi-api-vm"></a><%= vars.control_plane %> VM Group

The instances in the <%= vars.control_plane %> VM Group host the following services:

* User Account and Authentication (UAA)  
* <%= vars.control_plane %>  
* <%= vars.k8s_runtime_abbr %> Broker
* Billing and Telemetry 

The following sections describe UAA, <%= vars.control_plane %>, and <%= vars.k8s_runtime_abbr %> Broker services,
the primary services hosted on the <%= vars.control_plane %> VM.

####<a id="uaa"></a>UAA

When a user logs in to or logs out of the <%= vars.control_plane %> through the <%= vars.k8s_runtime_abbr %> CLI, the <%= vars.k8s_runtime_abbr %> CLI communicates with UAA to authenticate them.
The <%= vars.control_plane %> permits only authenticated users to manage Kubernetes clusters.
For more information about authenticating, see [<%= vars.control_plane %> Authentication](api-auth.html).

UAA must be configured with the appropriate users and user permissions.
For more information, see [Managing <%= vars.product_short %> Users with UAA](manage-users.html).

####<a id="tkgi-api"></a><%= vars.control_plane %>

Through the <%= vars.k8s_runtime_abbr %> CLI, users instruct the <%= vars.control_plane %> service to deploy, scale up, and delete Kubernetes clusters as well as show cluster details and plans.
The <%= vars.control_plane %> can also write Kubernetes cluster credentials to a local kubeconfig file, which enables users to connect to a cluster through `kubectl`.

On AWS, GCP, and vSphere without NSX-T deployments the <%= vars.k8s_runtime_abbr %> CLI communicates with the 
<%= vars.control_plane %> within the control plane via the <%= vars.control_plane %> Load Balancer. 
On vSphere with NSX-T deployments the <%= vars.control_plane %> host is accessible via a DNAT rule. 
For information about enabling the <%= vars.control_plane %> on vSphere with NSX-T, see the 
[Share the <%= vars.control_plane %> Endpoint](installing-nsx-t.html#retrieve-endpoint) section in 
_Installing <%= vars.product_short %> on vSphere with NSX-T Integration_.

The <%= vars.control_plane %> sends all cluster management requests, except read-only requests, to the <%= vars.k8s_runtime_abbr %> Broker.

####<a id="tkgi-broker"></a><%= vars.k8s_runtime_abbr %> Broker

When the <%= vars.control_plane %> receives a request to modify a Kubernetes cluster, it instructs the <%= vars.k8s_runtime_abbr %> Broker to make the requested change.

The <%= vars.k8s_runtime_abbr %> Broker consists of an [On-Demand Service Broker](https://docs.pivotal.io/svc-sdk/odb/index.html) and a Service Adapter. The <%= vars.k8s_runtime_abbr %> Broker generates a BOSH manifest and instructs the BOSH Director to deploy or delete the Kubernetes cluster.

For <%= vars.product_short %> deployments on vSphere with NSX-T, there is an additional component, the <%= vars.product_short %> NSX-T Proxy Broker.
The <%= vars.control_plane %> communicates with the <%= vars.k8s_runtime_abbr %> NSX-T Proxy Broker, which in turn communicates with the NSX Manager to provision the Node Networking resources.
The <%= vars.k8s_runtime_abbr %> NSX-T Proxy Broker then forwards the request to the On-Demand Service Broker to deploy the cluster.

###<a id="tkgi-db-vm"></a><%= vars.control_plane_db %> VM Cluster

The instances in the <%= vars.control_plane_db %> VM Cluster host MySQL, proxy, and other data-related services. 
These data-related functions persist <%= vars.k8s_runtime_abbr %> Control Plane data for the the following services:

* <%= vars.control_plane %>
* UAA
* Billing
* Telemetry

##<a id="overview-ha"></a>High Availability Modes

<%= vars.product_short %> can be configured for <%= vars.k8s_runtime_abbr %> Control Plane and workload high availability.   

####<a id="control-plane-ha"></a><%= vars.k8s_runtime_abbr %> Control Plane High Availability Mode (Beta)

The <%= vars.k8s_runtime_abbr %> Control Plane can be configured in either standard or high availability (beta) modes.

* In standard mode:  
    * The <%= vars.control_plane %> is hosted on the `pivotal-container-service` VM.
    * The <%= vars.control_plane_db %> is hosted on the `pks-db` VM.
* In high availability mode (beta):  
    * The <%= vars.control_plane %> is hosted on multiple `pivotal-container-service` VMs.
    * The <%= vars.control_plane_db %> is hosted on three `pks-db` VMs.
<p class="note warning"><strong>Warning:</strong> High availability mode is a beta feature.
	Do not scale your <strong><%= vars.k8s_runtime_abbr %> API</strong> or <strong><%= vars.k8s_runtime_abbr %> Database</strong>
	to more than one instance in production environments.
</p>

The following illustrates the interaction between <%= vars.product_short %> components in high availability mode:  
<br>
<%= image_tag('images/tkgi-overview-ha.png') %>
<%# Image source: https://docs.google.com/drawings/d/1hTTmoBpkcjvZJTwlwsXMy9fn91x3KGfIeM4_dzIrW60/edit %>

You establish HA mode during the resource configuation phase of TKGI tile deployment. You can change the number of instances from 1 to 2 or 3 for the TKGI API, and from 1 to 3 for the TKGI Database. Once you set HA mode and increase the number of instances beyond 1, you cannot decrease the number of instances.

  <img src="images/tkgi-ha.png" width="725">

##<a id='windows-ha'></a>Windows Worker-Based Kubernetes Cluster High Availability

Windows worker-based cluster Linux nodes can be configured in either standard or high availability modes.

* In standard mode, a single control plane/etcd node and a single Linux worker manage a cluster's Windows Kubernetes VMs.  
* In high availability mode, 
multiple control plane/etcd and Linux worker nodes manage a cluster's Windows Kubernetes VMs.  

The following illustrates the interaction between the 
<%= vars.product_short %> Management Plane and Windows worker-based Kubernetes clusters:  
<br>
<%= image_tag('images/overview-windows-ha-linux-workers.png') %>
<%# Image source: https://docs.google.com/drawings/d/1ec0T8iZx3P8Uf-dxnl1dLesuvtabTwapXClf2kxZ3qA/edit %>
<br>
To configure <%= vars.product_short %> Windows worker-based clusters for high availability, set these fields in the **Plan** pane as described in [Plans](windows-workers.html#plans) in _Configuring Windows Worker-Based Kubernetes Clusters_:

* **Enable HA Linux workers**
* **Master/ETCD Node Instances**
* **Worker Node Instances**