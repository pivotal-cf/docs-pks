---
title: Preparing NSX-T Before Deploying PKS
owner: PKS
---

<strong><%= modified_date %></strong>

Before you install Pivotal Container Service (PKS) on vSphere with NSX-T integration, you must prepare your NSX-T environment.

In addition to fulfilling the prerequisites specified in [vSphere with NSX-T Prerequisites and Resource Requirements](vsphere-nsxt-requirements.html), follow the steps below.

##<a id='plan'></a> Step 1: Plan for Network Subnets and IP Blocks

Before you install PKS on vSphere with NSX-T, you should plan for the CIDRs and IP blocks that you are using in your deployment.

###<a id='plan-cidrs'></a>Plan Network CIDRs

Plan for the following network CIDRs in the IPv4 address space according to the instructions in the VMware [NSX-T documentation](https://docs.vmware.com/en/VMware-NSX-T/index.html).

* **VTEP CIDRs**: One or more of these networks host your GENEVE Tunnel Endpoints on your NSX Transport Nodes. Size the networks to support all of your expected Host and Edge Transport Nodes. For example, a CIDR of `192.168.1.0/24` provides 254 usable IPs. This is used when creating the `ip-pool-vteps` in Step 3.
* **PKS MANAGEMENT CIDR**: This small network is used to access PKS management components such as Ops Manager and the PKS Service VM. For example, a CIDR of `10.172.1.0/28` provides 14 usable IPs. For the [No-NAT deployment topologies](nsxt-topologies.html#topology-no-nat-virtual-switch), this is a corporate routable subnet /28. For the [NAT deployment topology](nsxt-topologies.html#topology-nat), this is a non-routable subnet /28, and DNAT needs to be configured in NSX-T to access the PKS management components.
* **PKS LB CIDR**: This network provides your load balancing address space for each Kubernetes cluster created by PKS. The network also provides IP addresses for Kubernetes API access and Kubernetes exposed services. For example, `10.172.2.0/24` provides 256 usable IPs. This network is used when creating the `ip-pool-vips` described in [Create NSX Network Objects](#create-objects-network-objects), or when the services are deployed. You enter this network in the **Floating IP Pool ID** field in the **Networking** pane of the PKS tile.

Refer to the instructions in the VMware [NSX-T documentation](https://docs.vmware.com/en/VMware-NSX-T/index.html) to ensure that your network topology enables the following communications:

* vCenter, NSX-T components, and ESXi hosts must be able to communicate with each other.
* The Ops Manager Director VM must be able to communicate with vCenter and the NSX Manager.
* The Ops Manager Director VM must be able to communicate with all nodes in all Kubernetes clusters.
* Each PKS-provisioned Kubernetes cluster deploys the NSX-T Node Agent and the Kube Proxy that run as BOSH-managed processes on each worker node.
In addition, the NSX-T Container Plugin (NCP) runs as a BOSH-managed process on the Kubernetes master node.

    <p class="note"><strong>Note</strong>:
    As of PKS v1.1.5, NCP runs as a BOSH-managed process on the Kubernetes master node.
    Previously, each Kubernetes cluster deployed by PKS ran NCP as a pod that communicated with the NSX Manager.
    For more information about NCP,
    see <a href="https://docs.vmware.com/en/VMware-NSX-T/2.0/com.vmware.nsxt.ncp_kubernetes.doc/GUID-52A92986-0FDF-43A5-A7BB-C037889F7559.html">Overview of NSX-T Container Plug-In</a>.</p>

    In a multi-master PKS deployment, the NCP process runs on all master nodes. However, the process is active only on one master node.
    If the NCP process on an active master is unresponsive, BOSH activates another NCP process.

###<a id='plan-ip-blocks'></a>Plan IP Blocks

When you install PKS on NSX-T, you are required to specify the **Pods IP Block ID** and **Nodes IP Block ID** in the **Networking** pane of the PKS tile. These IDs map to the two IP blocks you must configure in NSX-T: the Pods IP Block for Kubernetes pods, and the Node IP Block for Kubernetes nodes (VMs). For more information, see the [Networking](installing-nsx-t.html#networking) section of _Installing PKS on vSphere with NSX-T Integration_.

  <img src="images/nsxt/nsxt-ip-blocks.png" alt="Required IP Blocks for NSX-T">

####<a id='pods-ip-block'></a>Pods IP Block

Each time a Kubernetes namespace is created, a subnet from the **Pods IP Block** is allocated. The subnet size carved out from this block is /24, which means a maximum of 256 pods can be created per namespace. When a Kubernetes cluster is deployed by PKS, by default 3 namespaces are created. Often additional namespaces will be created by operators to faciliate cluster use. As a result, when creating the **Pods IP Block**, you must use a CIDR range larger than /24 to ensure that NSX has enough IP addresses to allocate for all pods. The recommended size is /16. For more information, see [Create NSX Network Objects](#create-objects-network-objects) below.

  <img src="images/nsxt/pods-ip-block.png" alt="Pods IP Block">

<p class="note"><strong>Note</strong>: By default <strong>Pods IP Block</strong> should be non-routable, meaning a block of private IP addresses. After PKS is deployed, you can define a network profile that specifies a routable IP block for pods. The routable IP block will override the default non-routable <strong>Pods IP Block</strong> when a cluster is deployed using that network profile. See <a href="network-profiles#routable-pods">Routable Pods</a> in <em>Using Network Profiles (NSX-T Only)</em> for more information.</p>

####<a id='pods-ip-block'></a>Nodes IP Block

Each Kubernetes cluster deployed by PKS owns a /24 subnet. To deploy multiple Kubernetes clusters, set the **Nodes IP Block ID** in the **Networking** pane of the PKS tile to larger than /24. The recommended size is /16. For more information, see [Create NSX Network Objects](#create-objects-network-objects) below.

<p class="note"><strong>Note</strong>: You can use a smaller nodes block size for no-NAT environments with a limited number of routable subnets.
For example, /20 allows up to 16 Kubernetes clusters to be created.</p>

  <img src="images/nsxt/nodes-ip-block.png" alt="Nodes IP Block">

####<a id='reserved-ip-blocks'></a>Reserved IP Blocks

Do not use any of the IP blocks listed in this section for pods or nodes.
If you create Kubernetes clusters with any of the blocks listed below, the Kubernetes worker nodes cannot reach Harbor or internal Kubernetes services.

The Docker daemon on the Kubernetes worker node uses the subnet in the following CIDR range.
Do not use IP addresses in the following CIDR range:

  * 172.17.0.1/16
  * 172.18.0.1/16
  * 172.19.0.1/16
  * 172.20.0.1/16
  * 172.21.0.1/16
  * 172.22.0.1/16

If PKS is deployed with Harbor, Harbor uses the following CIDR ranges for its internal Docker bridges.
Do not use IP addresses in the following CIDR range:

  * 172.18.0.0/16
  * 172.19.0.0/16
  * 172.20.0.0/16
  * 172.21.0.0/16
  * 172.22.0.0/16

Each Kubernetes cluster uses the following subnet for Kubernetes services.
Do not use the following IP block for the Nodes IP Block:

  * 10.100.200.0/24

##<a id='deploy-nsx-t'></a> Step 2: Deploy NSX-T

Deploy NSX-T according to the instructions in the VMware [NSX-T documentation](https://docs.vmware.com/en/VMware-NSX-T/index.html).

<p class="note"><strong>Note</strong>: In general, accept default settings unless instructed otherwise.</p>

1. Deploy the NSX Manager. For more information, see [NSX Manager Installation](https://docs.vmware.com/en/VMware-NSX-T/2.2/com.vmware.nsxt.install.doc/GUID-A65FE3DD-C4F1-47EC-B952-DEDF1A3DD0CF.html) in the VMware NSX-T documentation.
1. Deploy NSX Controllers. For more information, see [NSX Controller Installation and Clustering](https://docs.vmware.com/en/VMware-NSX-T/2.2/com.vmware.nsxt.install.doc/GUID-447C0417-A37B-4C2E-965E-499F52587160.html) in the VMware NSX-T documentation.
1. Join the NSX Controllers to the NSX Manager. For more information, see [Join NSX Controllers with the NSX Manager](https://docs.vmware.com/en/VMware-NSX-T/2.2/com.vmware.nsxt.install.doc/GUID-05434745-7D74-4DA8-A68E-9FE17093DA7B.html) in the VMware NSX-T documentation.
1. Initialize the Control Cluster. For more information, see [Initialize the Control Cluster to Create a Control Cluster Master](https://docs.vmware.com/en/VMware-NSX-T/2.2/com.vmware.nsxt.install.doc/GUID-273F6344-7212-4105-9FBA-A872CD75803F.html) in the VMware NSX-T documentation.
1. Add your ESXi hosts to the NSX-T Fabric. For more information, see [Add a Hypervisor Host to the NSX-T Fabric](https://docs.vmware.com/en/VMware-NSX-T/2.2/com.vmware.nsxt.install.doc/GUID-8C0EEC08-3A63-4918-A5E2-7A94AD50B0E6.html) in the VMware NSX-T documentation. Each host must have at least one **free nic/vmnic** not already used by other vSwitches on the ESXi host for use with NSX Host Transport Nodes.
1. Deploy NSX Edge VMs. We recommend at least two VMs. For more information, see [NSX Edge Installation](https://docs.vmware.com/en/VMware-NSX-T/2.2/com.vmware.nsxt.install.doc/GUID-11417AA2-5EBC-49C7-8A86-EB94604261A6.html) in the VMware NSX-T documentation. Each deployed NSX Edge VM requires free resources in your vSphere environment to provide 8 vCPU, 16 GB of RAM, and 120 GB of storage. When deploying, you must connect the vNICs of the NSX Edge VMs to an appropriate PortGroup for your environment by completing the following steps:
  1. Connect the first Edge interface to your environment's PortGroup/VLAN where your Edge Management IP can route and communicate with the NSX Manager.
  1. Connect the second Edge interface to your environment's PortGroup/VLAN where your GENEVE VTEPs can route and communicate with each other. Your **VTEP CIDR** should be routable to this PortGroup.
  1. Connect the third Edge interface to your environment's PortGroup/VLAN where your T0 uplink interface is located.
  1. Join the NSX Edge VMs to the NSX-T Fabric. For more information, see [Join NSX Edge with the Management Plane](https://docs.vmware.com/en/VMware-NSX-T/2.2/com.vmware.nsxt.install.doc/GUID-11BB4CF9-BC1D-4A76-A32A-AD4C98CBF25B.html) in the VMware NSX-T documentation.

##<a id='create-objects'></a> Step 3: Create the NSX-T Objects Required for PKS

Create the NSX-T objects (network objects, logical switches, NSX Edge, and logical routers) needed for PKS deployment according to the instructions in the VMware [NSX-T documentation](https://docs.vmware.com/en/VMware-NSX-T/index.html).

###<a id='create-objects-network-objects'></a> 3.1: Create NSX Network Objects

1. Create two NSX IP pools. For more information, see [Create an IP Pool for Tunnel Endpoint IP Addresses](https://docs.vmware.com/en/VMware-NSX-T/2.2/com.vmware.nsxt.install.doc/GUID-E7F7322D-D09B-481A-BD56-F1270D7C9692.html) in the VMware NSX-T documentation. Configuration details for the NSX IP pools:
  * One NSX IP pool for GENEVE Tunnel Endpoints `ip-pool-vteps,` within the usable range of the **VTEP CIDR** created in Step 1, to be used with NSX Transport Nodes that you create later in this section
  * One NSX IP pool for NSX Load Balancing VIPs `ip-pool-vips,` within the usable range of the **PKS LB CIDR** created in Step 1, to be used with the T0 Logical Router that you create later in this section
1. Create two NSX Transport Zones (TZs). For more information, see [Create Transport Zones](https://docs.vmware.com/en/VMware-NSX-T/2.2/com.vmware.nsxt.install.doc/GUID-F739DC79-4358-49F4-9C58-812475F33A66.html) in the VMware NSX-T documentation. Configuration details for the NSX TZs:
  * One NSX TZ for PKS control plane Services and Kubernetes Cluster deployment overlay networks named `tz-overlay` and the associated N-VDS `hs-overlay`. Select **Standard**.
  * One NSX TZ for NSX Edge uplinks (ingress/egress) for PKS Kubernetes clusters named `tz-vlan` and the associated N-VDS `hs-vlan`. Select **Standard**.
1. If the default uplink profile is not applicable in your deployment, create your own NSX uplink host profile. For more information, see [Create an Uplink Profile](https://docs.vmware.com/en/VMware-NSX-T/2.2/com.vmware.nsxt.install.doc/GUID-50FDFDFB-F660-4269-9503-39AE2BBA95B4.html) in the VMware NSX-T documentation.
1. Create NSX Host Transport Nodes. For more information, see [Create a Host Transport Node](https://docs.vmware.com/en/VMware-NSX-T/2.2/com.vmware.nsxt.install.doc/GUID-D7CA778B-6554-4A23-879D-4BC336E01031.html) in the VMware NSX-T documentation. Configuration details:
  * For each host in the NSX-T Fabric, create a node named `tnode-host-NUMBER`. For example, if you have three hosts in the NSX-T Fabric, create three nodes named `tnode-host-1`, `tnode-host-2`, and `tnode-host-3`.
  * Add the `tz-overlay` NSX Transport Zone to each NSX Host Transport Node.
  <p class="note"><strong>Note</strong>: The Transport Nodes must be placed on free host NICs not already used by other vSwitches on the ESXi host. Use the <code>ip-pool-vteps</code> IP pool that allows them to route and communicate with each other, as well as other Edge Transport Nodes, to build GENEVE tunnels.</p>
1. Create NSX IP blocks. You must create separate NSX IP blocks for the node networks and the pod networks. The subnets for both nodes and pods should have a size of 256 (/16). For more information about planning IP blocks, see the [Plan IP Blocks](#plan-ip-blocks) section above. For more information about creating NSX IP blocks in NSX Manager, see [Manage IP Blocks](https://docs.vmware.com/en/VMware-NSX-T/2.2/com.vmware.nsxt.admin.doc/GUID-46C7B20D-4BE4-400E-AF39-1ADFE945DE38.html) in the VMware NSX-T documentation. Configuration details:
  * One NSX IP Block named `node-network-ip-block`. PKS uses this block to assign address space to Kubernetes master and worker nodes when new clusters are deployed or a cluster increases its scale.
  * One NSX IP Block named `pod-network-ip-block`. The NSX-T Container Plug-in (NCP) uses this block to assign address space to Kubernetes pods through the Container Networking Interface (CNI).

###<a id='create-objects-logical-switches'></a> 3.2: Create Logical Switches

1. Create the following NSX Logical Switches. For more information, see [Create a Logical Switch](https://docs.vmware.com/en/VMware-NSX-T/2.2/com.vmware.nsxt.admin.doc/GUID-23194F9A-416A-40EA-B9F7-346B391C3EF8.html) in the VMware NSX-T documentation. Configuration details for the Logical Switches:
  * One for T0 ingress/egress uplink port `ls-pks-uplink`
  * One for the PKS Management Network `ls-pks-mgmt`
    <p class="note"><strong>Note</strong>: This network is required for the <a href="nsxt-topologies.html#topology-nat">NAT deployment topology</a> and <a href="nsxt-topologies.html#topology-no-nat-logical-switch">No-NAT with Logical Switch deployment topology</a>. If you are deploying the <a href="nsxt-topologies.html#topology-no-nat-virtual-switch">No-NAT with Virtual Switch topology</a>, you can skip this step.</p>
1. Attach your first NSX Logical Switch to the `tz-vlan` NSX Transport Zone.
1. Attach your second NSX Logical Switch to the `tz-overlay` NSX Transport Zone.

###<a id='create-objects-create-nsx-edge'></a> 3.3: Create NSX Edge Objects

1. Create NSX Edge Transport Nodes. For more information, see [Create an NSX Edge Transport Node](https://docs.vmware.com/en/VMware-NSX-T/2.2/com.vmware.nsxt.install.doc/GUID-53295329-F02F-44D7-A6E0-2E3A9FAE6CF9.html) in the VMware NSX-T documentation.
1. Add both `tz-vlan` and `tz-overlay` NSX Transport Zones to the NSX Edge Transport Nodes. Controller Connectivity and Manager Connectivity should be **UP**.
1. Refer to the MAC addresses of the Edge VM interfaces you deployed to deploy your virtual NSX Edges:
  1. Connect the `hs-overlay` N-VDS to the vNIC (`fp-eth#`) that matches the MAC address of the second NIC from your deployed Edge VM.
  1. Connect the `hs-vlan` N-VDS to the vNIC (`fp-eth#`) that matches the MAC address of the third NIC from your deployed Edge VM.
1. Create an NSX Edge cluster named `edge-cluster-pks`. For more information, see [Create an NSX Edge Cluster](https://docs.vmware.com/en/VMware-NSX-T/2.2/com.vmware.nsxt.install.doc/GUID-898099FC-4ED2-4553-809D-B81B494B67E7.html) in the VMware NSX-T documentation.
1. Add the NSX Edge Transport Nodes to the cluster.

###<a id='create-objects-logical-routers'></a> 3.4: Create Logical Routers

####<a id='create-objects-logical-router-pks'></a> Create T0 Logical Router for PKS
T0 routers are edge routers that help route data between your non-NSX-T (such as a Physical Network) and the NSX-T network. PKS currently supports only a single T0 router per instance.

1. Create a Tier-0 (T0) logical router named `t0-pks`. For more information, see [Create a Tier-0 Logical Router](https://docs.vmware.com/en/VMware-NSX-T/2.2/com.vmware.nsxt.admin.doc/GUID-7891E6E7-606D-4F79-8AB7-BC01898F9FE7.html) in the VMware NSX-T documentation. Configuration details:
  * Select `edge-cluster-pks` for the cluster.
  * Set **High Availability Mode** to **Active-Standby**. NAT rules are be applied on T0 by NCP. If not set **Active-Standby**, the router does not support NAT rule configuration.

1. Attach the T0 logical router to the `ls-pks-uplink` logical switch you created previously. For more information, see [Connect a Tier-0 Logical Router to a VLAN Logical Switch](https://docs.vmware.com/en/VMware-NSX-T/2.2/com.vmware.nsxt.admin.doc/GUID-D641380B-4C8E-4C8A-AF64-4261A266ACA4.html) in the VMware NSX-T documentation. Create a logical router port for `ls-pks-uplink` and assign an IP address and CIDR that your environment uses to route to all PKS assigned IP pools and IP blocks.
1. Configure T0 routing to the rest of your environment using the appropriate routing protocol for your environment or by using static routes. For more information, see [Tier-0 Logical Router](https://docs.vmware.com/en/VMware-NSX-T/2.2/com.vmware.nsxt.admin.doc/GUID-3F163DEE-1EE6-4D80-BEBF-8D109FDB577C.html) in the VMware NSX-T documentation. The CIDR used in `ip-pool-vips` *must* route to the IP you just assigned to your t0 uplink interface.

####<a id='nsx-edge-ha'></a> (Optional) Configure NSX Edge for High Availability (HA)

You can configure NSX Edge for high availability (HA) using Active/Standby mode to support failover, as shown in the following figure.

![NSX Edge High Availability](images/vsphere/nsxt-edge-ha.png)

To configure NSX Edge for HA, complete the following steps:

<p class="note"><strong>Note</strong>: All IP addresses must belong to the same subnet.</p>

Step 1: On the T0 router, create a second uplink attached to the second Edge transport node:

<table>
  <tr>
    <th>Setting</th>
    <th>First Uplink</th>
    <th>Second Uplink</th>
  </tr>
  <tr>
    <td>IP Address/Mask</td>
    <td>uplink&#95;1&#95;ip</td>
    <td>uplink&#95;2&#95;ip</td>
  </tr>
  <tr>
    <td>URPF Mode</td>
    <td>None (optional)</td>
    <td>None (optional)</td>
  </tr>
  <tr>
    <td>Transport Node</td>
    <td>edge-TN1</td>
    <td>edge-TN2</td>
  </tr>
  <tr>
    <td>LS</td>
    <td>uplink-LS1</td>
    <td>uplink-LS1</td>
  </tr>
</table>

Step 2: On the T0 router, create the HA VIP:

<table>
  <tr>
    <th>Setting</th>
    <th>HA VIP</th>
  </tr>
  <tr>
    <td>VIP address</td>
    <td>[ha&#95;vip&#95;ip]</td>
  </tr>
  <tr>
    <td>Uplinks ports</td>
    <td>uplink-1 and uplink-2</td>
  </tr>
</table>

The HA VIP becomes the official IP for the T0 router uplink. External router devices peering with the T0 router _must_ use this IP address.

Step 3: On the physical router, configure the next hop to point to the HA VIP address.

Step 4: You can verify your setup by running the following commands:

<pre class="terminal">
nsx-edge-n&gt; get high-availability channels
nsx-edge-n&gt; get high-availability channels stats
nsx-edge-n&gt; get logical-router
nsx-edge-n&gt; get logical-router ROUTER-UUID high-availability status
</pre>

####<a id='create-objects-logical-router-t1-pks'></a> Create T1 Logical Router for PKS Management VMs

1. Create a Tier-1 (T1) logical router for PKS management VMs named `t1-pks-mgmt`. For more information, see [Create a Tier-1 Logical Router](https://docs.vmware.com/en/VMware-NSX-T/2.2/com.vmware.nsxt.admin.doc/GUID-A6042263-374F-4292-892E-BC86876325A4.html) in the VMware NSX-T documentation. Configuration details:
  * Link to the `t0-pks` logical router you created in a previous step.
  * Select `edge-cluster-pks` for the cluster. <br/>
  <p class="note"><strong>Note</strong>: Skip this step if you are deploying the No-NAT with Virtual Switch topology. This Logical Router is required for the <a href="nsxt-topologies.html#topology-nat">NAT deployment topology</a> and No-NAT with Logical Switch deployment topology.</p>
1. Create a logical router port for `ls-pks-mgmt` and assign the following CIDR block: `10.172.1.0/28`. For more information, see [Connect a Tier-0 Logical Router to a VLAN Logical Switch](https://docs.vmware.com/en/VMware-NSX-T/2.2/com.vmware.nsxt.admin.doc/GUID-D641380B-4C8E-4C8A-AF64-4261A266ACA4.html) in the VMware NSX-T documentation.
1. Configure route advertisement on the T1 as follows. For more information, see [Configure Route Advertisement on a Tier-1 Logical Router](https://docs.vmware.com/en/VMware-NSX-T/2.2/com.vmware.nsxt.admin.doc/GUID-B6AF9E08-1334-4D3A-B8ED-D0CAB3B563FB.html) in the VMware NSX-T documentation. Configuration details:
   * Enable **Status**.
   * Enable **Advertise All NSX Connected Routes**.
   * Enable **Advertise All NAT Routes**.
   * Enable **Advertise All LB VIP Routes**.

####<a id='configure-nat-logical-router-pks-mgmt'></a> Configure NAT Rules for PKS Management VMs

<p class="note"><strong>Note</strong>: This step applies to the <a href="nsxt-topologies.html#topology-nat">NAT Topology</a> only. Skip this step for <a href="nsxt-topologies.html#topology-no-nat-virtual-switch">No-NAT with Virtual Switch (VSS/VDS) Topology</a> and <a href="nsxt-topologies.html#topology-no-nat-logical-switch">No-NAT with Logical Switch (NSX-T) Topology</a>.</p>

Create the following NAT rules for the Mgmt T0. For more information, see [Tier-0 NAT](https://docs.vmware.com/en/VMware-NSX-T/2.2/com.vmware.nsxt.admin.doc/GUID-C24C751C-74CE-47BE-93B6-E5B7C646C343.html) in the VMware NSX-T documentation. Configuration details:

<table>
  <tr>
    <th>Type</th>
    <th>For</th>
  </tr>
  <tr>
    <td>DNAT</td>
    <td>External -> Ops Manager</td>
  </tr>
  <tr>
    <td>DNAT</td>
    <td>External -> Pivotal Container Service</td>
  </tr>
  <tr>
    <td>SNAT</td>
    <td>Ops Manager & BOSH Director -> DNS</td>
  </tr>
  <tr>
    <td>SNAT</td>
    <td>Ops Manager & BOSH Director -> NTP</td>
  </tr>
  <tr>
    <td>SNAT</td>
    <td>Ops Manager & BOSH Director -> vCenter</td>
  </tr>
  <tr>
    <td>SNAT</td>
    <td>Ops Manager & BOSH Director -> ESXi</td>
  </tr>
  <tr>
    <td>SNAT</td>
    <td>Ops Manager & BOSH Director -> NSX-T Manager</td>
  </tr>
</table>

The Destination NAT (DNAT) rule on the T0 maps an external IP address from the **PKS MANAGEMENT CIDR** to the IP where you deploy Ops Manager on the `ls-pks-mgmt` logical switch. For example, a DNAT rule that maps `10.172.1.2` to `172.31.0.2`, where `172.31.0.2` is the IP address you assign to Ops Manager when connected to `ls-pks-mgmt`. Later, you create another DNAT rule to map an external IP address from the **PKS MANAGEMENT CIDR** to the PKS endpoint.

The Source NAT (SNAT) rule on the T0 allows the PKS Management VMs to communicate with your vCenter and NSX Manager environments. For example, an SNAT rule that maps `172.31.0.0/24` to `10.172.1.1`, where `10.172.1.1` is a routable IP address from your **PKS MANAGEMENT CIDR**. For more information, see [Configure Source NAT on a Tier-1 Router](https://docs.vmware.com/en/VMware-NSX-T/2.2/com.vmware.nsxt.admin.doc/GUID-31CEF010-0C34-4C10-9443-13A0EAAABFD6.html) in the VMware NSX-T documentation.

<p class="note"><strong>Note</strong>: Ops Manager and BOSH must use the NFCP protocol to the actual ESX hosts to which it is uploading stemcells. Specifically, <strong>Ops Manager & BOSH Director -> ESXi</strong>.</p>

<p class="note"><strong>Note</strong>: Limit the Destination CIDR for the SNAT rules to the subnets that contain your vCenter and NSX Manager IP addresses.</p>
