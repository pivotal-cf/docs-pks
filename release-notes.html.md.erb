---
title: Enterprise PKS Release Notes
owner: PKS
topictype: releasenotes
---

<strong><%= modified_date %></strong>

This topic contains release notes for <%= vars.product_full %> v1.4.x.

## <a id="v1.4.0"></a>v1.4.0

**Release Date**: April 25, 2019

### <a id="v1.4.0-snapshot"></a>Product Snapshot

<table class="nice">
    <th>Element</th>
    <th>Details</th>
    <tr>
        <td>Version</td>
        <td>v1.4.0</td>
    </tr>
    <tr>
        <td>Release date</td>
        <td>April 25, 2019</td>
    </tr>
    <tr>
        <td>Compatible Ops Manager versions</td>
        <td>v2.4.3+, v2.5.0+</td>
    </tr>
    <tr>
        <td>Stemcell version</td>
        <td>v250.25</td>
    </tr>
    <tr>
        <td>Kubernetes version</td>
        <td>v1.13.5</td>
    </tr>
    <tr>
        <td>On-Demand Broker version</td>
        <td>v0.26.0</td>
    </tr>
    <tr>
        <td>NSX-T versions <strong>&#42;</strong></td>
        <td>v2.3.1, v2.4.0.1</td>
    </tr>
    <tr>
        <td>NCP version</td>
        <td>v2.4.0</td>
    </tr>
    <tr>
        <td>Docker version</td>
        <td>v18.06.3-ce<br><a href="https://github.com/cloudfoundry-incubator/docker-boshrelease/">CFCR</a></td>
    </tr>
    <tr>
        <td>Compatible BBR version</td>
        <td>v1.5.0+</td>
    </tr>
</table>

<p class="note"><strong>WARNING:</strong> NSX-T v2.4 implements a new <em>password expiration policy</em>. 
By default the administrator password expires after 90 days. If the password expires, 
you cannot deploy new Kubernetes clusters using PKS with NSX-T. For more information, refer to the
following VMware KB article: <a href="https://kb.vmware.com/s/article/70691">https://kb.vmware.com/s/article/70691</a>. 
See also <a href="./nsxt-deploy-24.html#nsxt-24-password">Updating the NSX Manager Password for NSX-T v2.4</a> in the 
<%= vars.product_short %> documentation.</p>

<p class="note"><strong>Note:</strong> NSX-T v2.4 implements a new user interface (UI) based on the NSX Policy API. 
PKS v1.4.0 does not support the NSX Policy API. Any objects created via the new Policy-based UI cannot be used with PKS v1.4.0.
If you are installing PKS v1.4.0 with NSX-T v2.4.x, or upgrading to PKS v1.4.0 and NSX-T 2.4.x, 
you must use the "Advanced Networking" tab in NSX Manager to create, read, update, and delete 
all networking objects required for PKS.</p>

<p class="note"><strong>Note:</strong> PKS v1.4.0 on Azure is compatible only with Ops Manager v2.4.2 and v2.4.3.
Before deploying PKS v1.4.0 on Azure, you must install Ops Manager v2.4.2 or v2.4.3.</p>

### <a id='vsphere-reqs'></a> vSphere Version Requirements

For <%= vars.product_short %> installations on vSphere or on vSphere with NSX-T Data Center, refer to the <a href="https://www.vmware.com/resources/compatibility/sim/interop_matrix.php#interop&356=&175=&1=">VMware Product Interoperability Matrices</a>.</p>

### <a id="v1.4.0-iaas"></a>Feature Support by IaaS

<table>
  <tr>
    <th></th>
    <th>AWS</th>
    <th>Azure</th>
    <th>GCP</th>
    <th>vSphere</th>
    <th>vSphere with NSX-T</th>
  </tr>

  <tr>
    <th>Automatic Kubernetes Cluster API load balancer</th>
    <td></td>
    <td></sup></td>
    <td></td>
    <td></td>
    <td>&check;</td>
  </tr>
  <tr>
    <th>HTTP proxy</th>
    <td></td>
    <td></sup></td>
    <td></td>
    <td>&check;</td>
    <td>&check;</td>
  </tr>
  <tr>
    <th>Multi-AZ storage</th>
    <td></td>
    <td></sup></td>
    <td></td>
    <td>&check;</td>
    <td>&check;</td>
  </tr>
  <tr>
    <th>Per-namespace subnets</th>
    <td></td>
    <td></sup></td>
    <td></td>
    <td></td>
    <td>&check;</td>
  </tr>
  <tr>
    <th>Service <code>type:LoadBalancer</code></th>
    <td>&check;<sup>&#42;</sup></td>
    <td>&check;</sup></td>
    <td>&check;</td>
    <td></td>
    <td>&check;</td>
  </tr>
</table>

<sup>&#42;</sup> For more information about configuring Service `type:LoadBalancer` on AWS, see the [Access Workloads Using an Internal AWS Load Balancer](deploy-workloads.html#internal-lb) section of _Deploying and Exposing Basic Workloads_.

### <a id="v1.4.0-upgrade"></a>Upgrade Path

The supported upgrade paths to PKS v1.4.0 are from PKS v1.3.2 and later.

To upgrade, see [Upgrading <%= vars.product_short %>](upgrade-pks.html) and [Upgrading <%= vars.product_short %> with NSX-T](upgrade-pks-nsxt.html).

When upgrading to NSX-T 2.4:

- Use the official VMware NSX-T Data Center 2.4 build.
- Apply the NSX-T v2.4.0.1 hot-patch. For more information, see [KB article 67499](https://kb.vmware.com/s/article/67449) in the VMware Knowledge Base.
- To obtain the NSX-T v2.4.0.1 hot-patch, open a support ticket with VMware Global Support Services (GSS) for NSX-T Engineering.

### <a id="v1.4.0-whats-new"></a>What's New

<%= vars.product_short %> v1.4.0 adds the following:

* Operators can configure up to ten sets of resource types, or _plans_, in the Enterprise PKS tile. All
plans except the first can made available or unavailable to developers deploying clusters. Plan 1
must be configured and made available as a default for developers. For more information, see the _Plans_ section of the installation topic for your IaaS, such as [Installing <%= vars.product_short %> on vSphere with NSX-T](./installing-nsx-t.html#plans).

* Operators can deploy up to 5 master nodes per plan. For more information, see the _Plans_ section of the installation topic for your IaaS, such as [Installing <%= vars.product_short %> on vSphere with NSX-T](./installing-nsx-t.html#plans).

* Operators can install PKS and Pivotal Application Service (PAS) on the same instance of Ops
Manager.

* Improved workflow for managing cluster access. For more information, see [Grant Cluster Access](manage-users.html#cluster-access) in _Managing Users in <%= vars.product_short %> with UAA_.

* Operators can create webhook ClusterSink resources. A webhook ClusterSink resource batches logs into 1 second units, wraps the resulting payload in JSON, and uses the POST method to deliver the logs to the address of your log management service. For more information see, [Create a Webhook ClusterSink Resource with YAML and kubectl](create-sinks.html#webhook-cluster-sink) in _Creating Sinks_.

* Operators can set quotas for maximum memory and CPU utilization in a PKS deployment. For more information, see [Managing Resource Usage](resource-usage.html). This is a beta feature.
    <%= partial 'beta-component' %>

* Operators can enable the PodSecurityPolicy admission plugin on a per-plan basis requiring cluster users to have policy, role, and role binding permissions to deploy pod workloads. See [Pod Security Policies](./pod-security-policy.html) for more information.

* Operators can enable the SecurityContextDeny admission plugin on a per-plan basis to prohibit the use of security context configurations on pods and containers. See [Security Context Deny](./security-context-deny.html) for more information.

* Operators can enable the DenyEscalatingExec admission plugin on a per-plan basis to prohibit the use of certain commands for containers that allow host access. See [Deny Escalating Execution](./deny-escalating-execution.html) for more information.

* Operators using vSphere can use HostGroups to define Availability Zones (AZs) for clusters in BOSH. See [Using vSphere Host Groups](./vsphere-host-group.html).

* Operators using vSphere can configure compute profiles to specify which vSphere
resources are used when deploying Kubernetes clusters in a PKS deployment.
For more information, see [Using Compute Profiles (vSphere Only)](compute-profiles.html).
    <%= partial 'beta-component' %>

* Operators using vSphere with NSX-T can update a Network Profile and add to or reorder the **Pods IP Block** IDs. For more information, see the [Change the Network Profile for a Cluster](./network-profiles.html#update-profile) section of _Using Network Profiles (NSX-T Only)_.

### <a id="v1.4.0-known-issues"></a>Breaking Changes and Known Issues

<p class="note breaking"><strong>Breaking Change:</strong> PKS v1.4.0 Sink Log log-entries now record cluster name as event-source rather than host ID.</p>

<%= vars.product_short %> v1.4.0 has the following known issues:

#### <a name="security-group"></a>Azure Default Security Group Is Not Automatically Assigned to Cluster VMs

**Symptom**

You experience issues when configuring a load balancer for a multi-master Kubernetes cluster or creating a service of type `LoadBalancer`.
Additionally, in the Azure portal, the **VM** > **Networking** page does not display
any inbound and outbound traffic rules for your cluster VMs.

**Explanation**

As part of configuring the <%= vars.product_tile %> tile for Azure, you enter **Default Security Group** in the **Kubernetes Cloud Provider** pane.
When you create a Kubernetes cluster, <%= vars.product_short %> automatically assigns this security group to each VM in the cluster.
However, in <%= vars.product_short %> v1.4, the automatic assignment may not occur.

As a result, your inbound and outbound traffic rules defined in the security group are not applied to the cluster VMs.

**Workaround**

If you experience this issue, manually assign the default security group to each VM NIC in your cluster.

#### <a id='first-az'></a>Cluster Creation Fails When First AZ Runs out of Resources

**Symptom**

If the first availability zone (AZ) used by a plan with multiple AZs runs out of
resources, cluster creation fails with an error like the following:

<pre class="terminal">
L Error: CPI error 'Bosh::Clouds::CloudError' with message 'No valid placement found for requested memory: 4096
</pre>

**Explanation**

BOSH creates VMs for your <%= vars.product_short %> deployment using a round-robin
algorithm, creating the first VM in the first AZ that your plan uses.
If the AZ runs out of resources, cluster creation fails because BOSH cannot create
the cluster VM.

For example, if your three AZs each have enough resources for ten VMs, and you
create two clusters with four worker VMs each, BOSH creates VMs in the
following AZs:

<table>
  <tr>
    <th></th>
    <th>AZ1</th>
    <th>AZ2</th>
    <th>AZ3</th>
  </tr>
  <tr>
    <th>Cluster 1</th>
    <td>Worker VM 1</td>
    <td>Worker VM 2</td>
    <td>Worker VM 3</td>
  </tr>
  <tr>
    <td></td>
    <td>Worker VM 4</td>
    <td></td>
    <td></td>
  </tr>
  <tr>
    <th>Cluster 2</th>
    <td>Worker VM 1</td>
    <td>Worker VM 2</td>
    <td>Worker VM 3</td>
  </tr>
  <tr>
    <td></td>
    <td>Worker VM 4</td>
    <td></td>
    <td></td>
</table>

In this scenario, AZ1 has twice as many VMs as AZ2 or AZ3.

#### <a id='azure-worker-comm'></a>Azure Worker Node Communication Fails after Upgrade

**Symptom**

Outbound communication from a worker node VM fails after an upgrade to <%= vars.product_short %> v1.4.0.

**Explanation**

<%= vars.product_short %> 1.4.0 uses Azure Availability Sets to improve the uptime of workloads and worker nodes in the event of Azure platform failures. Worker node
VMs are distributed evenly across Availability Sets.

Azure Standard SKU Load Balancers are recommended for the Kubernetes control plane and Kubernetes ingress and egress. This load balancer type provides an IP address for outbound communication using SNAT.

During an upgrade, when BOSH rebuilds a given worker instance in an Availability Set,
Azure can time out while re-attaching the worker node network interface to the
back-end pool of the Standard SKU Load Balancer.

For more information, see [Outbound connections in Azure](https://docs.microsoft.com/en-us/azure/load-balancer/load-balancer-outbound-connections) in the Azure documentation.

**Workaround**

You can manually re-attach the worker instance to the back-end pool of the Azure Standard SKU Load Balancer in your Azure console.
