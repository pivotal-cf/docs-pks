---
title: Enterprise PKS Release Notes
owner: PKS
topictype: releasenotes
---

<strong><%= modified_date %></strong>

This topic contains release notes for <%= vars.product_full %> v1.5.0.

## <a id="v1.5.0"></a>v1.5.0

**Release Date**: TBD
### <a id="v1.5.0-snapshot"></a>Product Snapshot

<table class="nice">
    <th>Element</th>
    <th>Details</th>
    <tr>
        <td>Version</td>
        <td>v1.5.0</td>
    </tr>
    <tr>
        <td>Release date</td>
        <td>TBD</td>
    </tr>
    <tr>
        <td>Compatible Ops Manager versions</td>
        <td><%= vars.ops_man_version_2_5 %> or <%= vars.ops_man_version_2_6 %></td>
    </tr>
    <tr>
        <td>Stemcell version</td>
        <td>v</td>
    </tr>
    <tr>
        <td>Kubernetes version</td>
        <td>v</td>
    </tr>
    <tr>
        <td>On-Demand Broker version</td>
        <td>v</td>
    </tr>
    <tr>
        <td>NSX-T versions <strong>&#42;</strong></td>
        <td>v</td>
    </tr>
    <tr>
        <td>NCP version</td>
        <td>v</td>
    </tr>
    <tr>
        <td>Docker version</td>
        <td>v<br><a href="https://github.com/cloudfoundry-incubator/docker-boshrelease/">CFCR</a></td>
    </tr>
    <tr>
        <td>Backup and Restore SDK version</td>
        <td>v</td>
    </tr>
</table>

<p class="note"><strong>Note:</strong> NSX-T v2.4 implements a new user interface (UI) based on the NSX Policy API.
PKS v1.5.0 does not support the NSX Policy API. Any objects created through the new UI cannot be used with PKS v1.5.0.
If you are installing PKS v1.5.0 with NSX-T v2.4.x or upgrading to PKS v1.5.0 and NSX-T 2.4.x,
you must use the <strong>Advanced Networking</strong> tab in NSX Manager to create, read, update, and delete
all networking objects required for <%= vars.product_short %>.</p>

### <a id='vsphere-reqs'></a> vSphere Version Requirements

For <%= vars.product_short %> installations on vSphere or on vSphere with NSX-T Data Center, refer to the <a href="https://www.vmware.com/resources/compatibility/sim/interop_matrix.php#interop&356=&175=&1=">VMware Product Interoperability Matrices</a>.</p>

### <a id="v1.5.0-iaas"></a>Feature Support by IaaS

<table>
  <tr>
    <th></th>
    <th>AWS</th>
    <th>Azure</th>
    <th>GCP</th>
    <th>vSphere</th>
    <th>vSphere with NSX-T</th>
  </tr>

  <tr>
    <th>Automatic Kubernetes Cluster API load balancer</th>
    <td></td>
    <td></td>
    <td></td>
    <td></td>
    <td>&check;</td>
  </tr>
  <tr>
    <th>HTTP proxy</th>
    <td></td>
    <td></td>
    <td></td>
    <td>&check;</td>
    <td>&check;</td>
  </tr>
  <tr>
    <th>Multi-AZ storage</th>
    <td></td>
    <td></td>
    <td></td>
    <td>&check;</td>
    <td>&check;</td>
  </tr>
  <tr>
    <th>Per-namespace subnets</th>
    <td></td>
    <td></td>
    <td></td>
    <td></td>
    <td>&check;</td>
  </tr>
  <tr>
    <th>Service <code>type:LoadBalancer</code></th>
    <td>&check;<sup>&#42;</sup></td>
    <td>&check;</td>
    <td>&check;</td>
    <td></td>
    <td>&check;</td>
  </tr>
  <tr>
    <th>Windows Worker-based Cluster</th>
    <td></td>
    <td></sup></td>
    <td></td>
    <td>&check;<sup>&#42;</sup><sup>&#42;</sup></td>
    <td></td>
  </tr>
</table>

<sup>&#42;</sup> For more information about configuring Service `type:LoadBalancer` on AWS, see the [Access Workloads Using an Internal AWS Load Balancer](deploy-workloads.html#internal-lb) section of _Deploying and Exposing Basic Workloads_.
<sup>&#42;</sup><sup>&#42;</sup> Windows worker-based Kubernetes cluster support in PKS v1.5 requires vSphere with Flannel.

### <a id="v1.5.0-upgrade"></a>Upgrade Path

The supported upgrade paths to PKS v1.5.0 are from PKS v1.4.1 and later.

To upgrade, see [Upgrading <%= vars.product_short %>](upgrade-pks.html) and [Upgrading <%= vars.product_short %> with NSX-T](upgrade-pks-nsxt.html).

### <a id="v1.5.0-whats-new"></a>Features

New features and changes in this release:

* Operators can provision a Windows worker-based Kubernetes cluster on vSphere with Flannel.
Windows worker-based clusters in <%= vars.product_short %> 1.5 currently do not support NSX-T integration.
* Operators can set the lifetime for the refresh and access tokens for Kubernetes clusters.
You can configure the token lifetimes to meet your organization's security and compliance needs.
For instructions about configuring the access and refresh token for your Kubernetes clusters,
see the [UAA](./installing-pks-vsphere.html#uaa) section in the _Installing_ topic for your IaaS.
* Operators can configure prefixes for OpenID Connect (OIDC) users
and groups to avoid name conflicts with existing Kubernetes system users.
Pivotal recommends adding prefixes to ensure OIDC users and groups
do not gain unintended privileges on clusters. For instructions about configuring OIDC prefixes,
see the [Configure OpenID Connect](./installing-pks-vsphere.html#configure-oidc) section
in the _Installing_ topic for your IaaS.
* Operators can configure an external SAML identity provider for user authentication and authorization.
 For instructions about configuring an external SAML identity provider,
 see the [Configure SAML as an Identity Provider](./installing-pks-vsphere.html#configure-saml)
 section in the _Installing_ topic for your IaaS.
* Operators can upgrade Kubernetes clusters separately from the <%= vars.product_tile %> tile.
For instructions on upgrading Kubernetes clusters, see [Upgrading Clusters](./upgrade-clusters.html).
* Operators can configure the Telgraf agent to send master/etcd node metrics
to a third-party monitoring service. For more information,
see [Monitoring Master/etcd Node VMs](./monitor-etcd.html).
* Operators can configure the default node drain behavior.
You can use this feature to resolve hanging or failed cluster upgrades.
For more information about configuring node drain behavior,
see [Worker Node Hangs Indefinitely](./troubleshoot-issues.html#upgrade-drain-hangs) in _Troubleshooting_
and [Configure Node Drain Behavior](./checklist.html#configure-node-drain)
in Upgrade Preparation Checklist for <%= vars.product_short %> v1.5.
* VMwareâ€™s Customer Experience Improvement Program (CEIP) and the Pivotal Telemetry Program (Telemetry)
are now enabled in <%= vars.product_short %> by default.
This includes both new installations and upgrades.
For information about configuring CEIP and Telemetry in the <%= vars.product_tile %> tile,
see [CEIP and Telemetry](installing-pks-vsphere.html#telemetry) in the _Installing_ topic for your IaaS.

### <a id="v1.5.0-known-issues"></a>Breaking Changes and Known Issues

<%= vars.product_short %> v1.5.0 has the following known issues:

<p class="note breaking"><strong>Breaking change:</strong> Sink resources have changes to the resource name and API version that require updates to the applied configuration. All existing sinks are migrated and will not be affected, but new sinks will only accept the updated config. See the creating sink resources section of the docs for more details.</p>


#### <a name="security-group"></a>Azure Default Security Group Is Not Automatically Assigned to Cluster VMs

**Symptom**

You experience issues when configuring a load balancer for a multi-master Kubernetes cluster or creating a service of type `LoadBalancer`.
Additionally, in the Azure portal, the **VM** > **Networking** page does not display
any inbound and outbound traffic rules for your cluster VMs.

**Explanation**

As part of configuring the <%= vars.product_tile %> tile for Azure, you enter **Default Security Group** in the **Kubernetes Cloud Provider** pane.
When you create a Kubernetes cluster, <%= vars.product_short %> automatically assigns this security group to each VM in the cluster.
However, in <%= vars.product_short %> v1.4, the automatic assignment may not occur.

As a result, your inbound and outbound traffic rules defined in the security group are not applied to the cluster VMs.

**Workaround**

If you experience this issue, manually assign the default security group to each VM NIC in your cluster.

#### <a id='first-az'></a>Cluster Creation Fails When First AZ Runs out of Resources

**Symptom**

If the first availability zone (AZ) used by a plan with multiple AZs runs out of
resources, cluster creation fails with an error like the following:

<pre class="terminal">
L Error: CPI error 'Bosh::Clouds::CloudError' with message 'No valid placement found for requested memory: 4096
</pre>

**Explanation**

BOSH creates VMs for your <%= vars.product_short %> deployment using a round-robin
algorithm, creating the first VM in the first AZ that your plan uses.
If the AZ runs out of resources, cluster creation fails because BOSH cannot create
the cluster VM.

For example, if your three AZs each have enough resources for ten VMs, and you
create two clusters with four worker VMs each, BOSH creates VMs in the
following AZs:

<table>
  <tr>
    <th></th>
    <th>AZ1</th>
    <th>AZ2</th>
    <th>AZ3</th>
  </tr>
  <tr>
    <th>Cluster 1</th>
    <td>Worker VM 1</td>
    <td>Worker VM 2</td>
    <td>Worker VM 3</td>
  </tr>
  <tr>
    <td></td>
    <td>Worker VM 4</td>
    <td></td>
    <td></td>
  </tr>
  <tr>
    <th>Cluster 2</th>
    <td>Worker VM 1</td>
    <td>Worker VM 2</td>
    <td>Worker VM 3</td>
  </tr>
  <tr>
    <td></td>
    <td>Worker VM 4</td>
    <td></td>
    <td></td>
</table>

In this scenario, AZ1 has twice as many VMs as AZ2 or AZ3.

#### <a id='azure-worker-comm'></a>Azure Worker Node Communication Fails after Upgrade

**Symptom**

Outbound communication from a worker node VM fails after an upgrade to <%= vars.product_short %> v1.4.0.

**Explanation**

<%= vars.product_short %> 1.4.0 uses Azure Availability Sets to improve the uptime of workloads and worker nodes in the event of Azure platform failures. Worker node
VMs are distributed evenly across Availability Sets.

Azure Standard SKU Load Balancers are recommended for the Kubernetes control plane and Kubernetes ingress and egress. This load balancer type provides an IP address for outbound communication using SNAT.

During an upgrade, when BOSH rebuilds a given worker instance in an Availability Set,
Azure can time out while re-attaching the worker node network interface to the
back-end pool of the Standard SKU Load Balancer.

For more information, see [Outbound connections in Azure](https://docs.microsoft.com/en-us/azure/load-balancer/load-balancer-outbound-connections) in the Azure documentation.

**Workaround**

You can manually re-attach the worker instance to the back-end pool of the Azure Standard SKU Load Balancer in your Azure console.

#### <a name='no-password-om'></a> Passwords Not Supported for Ops Manager VM on vSphere

Starting in Ops Manager v2.6, you can only SSH onto the Ops Manager VM in a vSphere deployment with a private SSH key. You cannot SSH onto the Ops Manager VM with a password.

To avoid upgrade failure and errors when authenticating, add a public key to the **Customize Template** screen of the the OVF template for the Ops Manager VM. Then, use the private key to SSH onto the Ops Manager VM.

<p class="note warning"><strong>Warning</strong>: You cannot upgrade to Ops Manager v2.6 successfully without adding a public key. If you do not add a key, Ops Manager shuts down automatically because it cannot find a key and may enter a reboot loop.</p>

For more information about adding a public key to the OVF template, see [Deploy Ops Manager](https://docs.pivotal.io/pivotalcf/2-6/om/vsphere/deploy.html#deploy) in _Deploying Ops Manager on vSphere_.

#### <a name='oidc'></a> New OIDC Prefixes Break Existing Cluster Role Bindings

In <%= vars.product_short %> v1.5, operators can configure prefixes for OIDC usernames and groups.
If you add OIDC prefixes you must manually change any existing role bindings that bind to a username or group.
If you do not change your role bindings, developers cannot access Kubernetes clusters.
For instructions about creating a role binding, see [Managing Cluster Access and Permissions](./manage-cluster-permissions.html).

#### <a name='timeout'></a> Timeout Error During Individual Cluster Upgrades

**Symptom**

A timeout error occurs when you upgrade your Kubernetes clusters individually using the `pks upgrade-cluster` command.

**Explanation**

BOSH upgrades Kubernetes clusters in parallel with a limit of four concurrent cluster upgrades.
If you schedule more than four cluster upgrades,
<%= vars.product_short %> queues the upgrades and waits for BOSH to finish the last upgrade.
When BOSH finishes the last upgrade, it starts working on the next upgrade request.

If you submit too many cluster upgrades to BOSH, a timeout error may occur. The timeout is set to 168 hours.
However, BOSH does not remove the task from the queue or stop working on the upgrade if it has been picked up.

**Solution**

Do not submit more than four cluster upgrade requests at a time.
For instructions on upgrading Kubernetes clusters provisioned by <%= vars.product_short %>,
see [Upgrading Clusters](./upgrade-clusters.html).
