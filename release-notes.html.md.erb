---
title: PKS Release Notes
owner: PKS
---

<strong><%= modified_date %></strong>

This topic contains release notes for Pivotal Container Service (PKS) v1.3.x.

## <a id="v1.3.0"></a>v1.3.0

**Release Date**: January 16, 2019

### <a id="v1.3.0-snapshot"></a>Product Snapshot

<table class="nice">
    <th>Element</th>
    <th>Details</th>
    <tr>
        <td>Version</td>
        <td>v1.3.0</td>
    </tr>
    <tr>
        <td>Release date</td>
        <td>January 16, 2019</td>
    </tr>
    <tr>
        <td>Compatible Ops Manager versions</td>
        <td>v2.3.1+, v2.4.0+</td>
    </tr>
    <tr>
        <td>Stemcell version</td>
        <td>v170.15</td>
    </tr>
    <tr>
        <td>Kubernetes version</td>
        <td>v1.12.4</td>
    </tr>
    <tr>
        <td>On-Demand Broker version</td>
        <td>v0.24</td>
    </tr>
    <tr>
        <td>NSX-T versions <strong>&#42;</strong></td>
        <td>v2.2, v2.3.0.2, v2.3.1</td>
    </tr>
    <tr>
        <td>NCP version</td>
        <td>v2.3.1</td>
    </tr>
    <tr>
        <td>Docker version</td>
        <td>v18.06.1-ce<br><a href="https://github.com/cloudfoundry-incubator/docker-boshrelease/">CFCR</a></td>
    </tr>
</table>

<strong><sup>&#42;</sup></strong> PKS v1.3 supports NSX-T v2.2 and v2.3 with the following caveats:

<ul> 
    <li>To use the network profile features available in PKS v1.3, you must use NSX-T v2.3.</li>
    <li>NSX-T 2.3.0 has a known critical issue: <a href="https://kb.vmware.com/s/article/60293">ESX hosts lose network connectivity rendering the host inaccessible from network (60293)</a>. This issue is patched in NSX-T v2.3.0.2 and fixed in the NSX-T v2.3.1 release.</li>
</ul>

### <a id="v1.3.0-iaas"></a>Feature Support by IaaS

<table>
  <tr>
    <th></th>
    <th>AWS</th>
    <th>Azure</th>
    <th>GCP</th>
    <th>vSphere</th>
    <th>vSphere with NSX-T</th>
  </tr>

  <tr>
    <th>Automatic Kubernetes Cluster API load balancer</th>
    <td></td>
    <td></sup></td>
    <td></td>
    <td></td>
    <td>&check;</td>
  </tr>
  <tr>
    <th>HTTP proxy</th>
    <td></td>
    <td></sup></td>
    <td></td>
    <td>&check;</td>
    <td>&check;</td>
  </tr>
  <tr>
    <th>Multi-AZ storage</th>
    <td></td>
    <td></sup></td>
    <td></td>
    <td>&check;</td>
    <td>&check;</td>
  </tr>
  <tr>
    <th>Per-namespace subnets</th>
    <td></td>
    <td></sup></td>
    <td></td>
    <td></td>
    <td>&check;</td>
  </tr>
  <tr>
    <th>Service <code>type:LoadBalancer</code></th>
    <td>&check;<sup>&#42;</sup></td>
    <td>&check;</sup></td>
    <td>&check;</td>
    <td></td>
    <td>&check;</td>
  </tr>
</table>

<sup>&#42;</sup> For more information about configuring Service `type:LoadBalancer` on AWS, see the [Access Workloads Using an Internal AWS Load Balancer](deploy-workloads.html#internal-lb) section of _Deploying and Accessing Basic Workloads_.

### <a id="v1.3.0-upgrade"></a>Upgrade Path

The supported upgrade paths to PKS v1.3.0 are from PKS v1.2.5 and later.

For more information, see [Upgrading PKS](upgrade-pks.html) and [Upgrading PKS with NSX-T](upgrade-pks-nsxt.html).

<p class="note"><strong>Note</strong>: Upgrading from PKS v1.2.5+ to PKS v1.3.x causes all certificates to be automatically regenerated. The old certificate authority is still trusted, and has a validity of one year. But the new certificates are signed with a new certificate authority, which is valid for four years.</p>

### <a id="v1.3.0-whats-new"></a>What's New

PKS v1.3.0 adds the following:

- Support for PKS on Azure. For more information, see [Azure](azure-index.html).
- BOSH Backup and Restore (BBR) for single-master clusters. For more information, see [Backing up the Single Master Cluster](bbr-backup-cluster.html) and [Restoring the Single Master Cluster](bbr-restore-cluster.html).
- Routable pods on NSX-T. For more information, see [Routable Pod Networks](network-profiles-define.html#routable-pods) in <em>Defining Network Profiles</em>.
- Large size NSX-T load balancers with Bare Metal NSX-T edge nodes. For more information, see [Hardware Requirements for PKS on vSphere with NSX-T](vsphere-nsxt-rpd-mpd.html#pks-edge-cluster).
- HTTP proxy for NSX-T components. For more information, see [Using Proxies with PKS on NSX-T](proxies.html).
- Ability to specify the size of the Pods IP Block subnet using a network profile. For more information, see [Pod Subnet Prefix](./network-profiles-define.html#pod-prefix) in <em>Defining Network Profiles</em>.
- Support for bootstrap security groups, custom floating IPs, and edge router selection using network profiles. For more information, see [Bootstrap Security Group](network-profiles-define.html#ns-groups), [Custom Floating IP Pool](network-profiles-define.html#floating-ip), and [Edge Router Selection](network-profiles-define.html#multi-t0) in <em>Defining Network Profiles</em>.
- Support for sink resources in air-gapped environments.
- Support for creating sink resources with the PKS Command Line Interface (PKS CLI). For more information, see [Creating Sink Resources](create-sinks.html).
- Sink resources include both pod logs as well as events from the Kubernetes API. These events are combined in a shared format that provides operators with a robust set of filtering and monitoring options. For more information, see [Monitoring PKS with Sinks](monitor-sinks.html).
- Support for multiple NSX-T Tier-0 (T0) logical routers for use with PKS multi-tenant environments. For more information, see [Configuring Multiple Tier-0 Routers for Tenant Isolation](nsxt-multi-t0.html).
- Support for multiple PKS foundations on the same NSX-T. For more information, see [Implementing a Multi-Foundation PKS Deployment](nsxt-multi-pks.html). 
- Smoke tests errand that uses the PKS CLI to create a Kubernetes cluster and then delete it. If the creation or deletion fails, the errand fails and the installation of the PKS tile is aborted. For more information, see the <em>Errands</em> section of the <em>Installing PKS</em> topic for your IaaS, such as [Installing PKS on vSphere](installing-pks-vsphere.html#errands).
- Support for scaling down the number of worker nodes. For more information, see [Scaling Existing Clusters](scale-clusters.html).
- Support for defining the CIDR range for Kubernetes pods and services on Flannel networks. For more information, see the <em>Networking</em> section of the <em>Installing PKS</em> topic for your IaaS, such as [Installing PKS on vSphere](installing-pks-vsphere.html#networking).
- Kubernetes v1.12.4.  
- **Bug Fix**: The **No proxy** property for vSphere now accepts wildcard domains like
`*.example.com` and `example.com`. See [Networking](./installing-pks-vsphere.html#networking) in
_Installing PKS on vSphere_ for more information.
- **Bug Fix**: The issue with NSX-T where special characters in username and password doesn't work is resolved.
- **Security Fix**: [CVE 2018-18264](https://pivotal.io/security/cve-2018-18264): This CVE allows unauthenticated secret access to the Kubernetes Dashboard.
- **Security Fix**: [CVE-2018-15759](https://pivotal.io/security/cve-2018-15759): This CVE contains an insecure method of verifying credentials. A remote unauthenticated malicious user may make many requests to the service broker with a series of different credentials, allowing them to infer valid credentials and gain access to perform broker operations.

### <a id="v1.3.0-known-issues"></a>Breaking Changes and Known Issues

<p class="note breaking"><strong>Breaking Change:</strong> Heapster is deprecated in PKS v1.3, and
Kubernetes has retired Heapster. For more information, see the
<a href="https://github.com/kubernetes-retired/heapster">kubernetes-retired/heapster</a>
repository in GitHub.</p>

PKS v1.3.0 has the following known issues:


####<a id="duplicate-external-host"></a>Upgrades from 1.2.x can fail if customer has multiple clusters using the same external hostname
Customers who currently leverage the same external hostname across more than one Kubernetes cluster will be impacted when upgrading from 1.2.x to 1.3.0.  The external hostname is the value set via the -e or --external-hostname arguments when issuing `pks create-cluster -e [hostname]`.   1.3.0 introduced restrictions to prevent this scenario and the upgrade will fail if clusters with duplicate hostnames exist.   Customers who use duplicate external hostanems are recommended to NOT upgrade to 1.3.0 at this time and contact Support for more details.  This issue ONLY affects customers who have existing clusters with duplicate external hostnames.


####<a id="no-proxy"></a>Upgrades from 1.2.x can fail if customer is using a special character in thier NO_PROXY settings for PKS
Customers who leverage the HTTP Proxy feature for PKS network configuration may experience validation errors during the PKS 1.3.0 upgrade if they use the `-` character in their NO_PROXY settings.   Customers who experience this issue can contact Support for a hotfix that will also be applied in a future 1.3.x release. 


####<a id="flannel"></a> PKS Flannel Network Gets Out of Sync with Docker Bridge Network (cni0)

When VMs have been powered down for multiple days, turning them back on and issuing a `bosh recreate` to recreate the VMs causes the pods to get stuck in a `ContainerCreating` state.

**Workaround**: See [PKS Flannel network gets out of sync with docker bridge network (cni0)](https://community.pivotal.io/s/article/PKS-Flannel-network-gets-out-of-sync-with-docker-bridge-network-cni0) in the Pivotal Knowledge Base.

