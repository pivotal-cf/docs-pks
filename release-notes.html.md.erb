---
title: PKS Release Notes
owner: PKS
---

<strong><%= modified_date %></strong>

## <a id="v1.1.2"></a>v1.1.2

**Release Date**: July 17, 2018

### <a id="v1.1.2-security-fixes"></a>Security Fixes

This release includes the following security fix:

* High [CVE-2018-11047: UAA accepts refresh token as access token on admin endpoints](https://www.cloudfoundry.org/blog/cve-2018-11047/)

## <a id="v1.1.1"></a>v1.1.1

**Release Date**: July 16, 2018

### <a id="v1.1.1-upgrade"></a>Upgrade Procedure

The supported upgrade paths to PKS v1.1.1 are from PKS v1.0.2 and later.

To upgrade to PKS v1.1.1, follow the procedures in [Upgrade PKS](upgrade-pks.html).

### <a id="v1.1.1-what's-new"></a>What's New
- UAA and security enhancements
- NSX-T patches
- Telemetry patch
- Kubernetes 1.10.4

### <a id="v1.1.1-bug-fixes"></a>Bug Fixes

PKS v1.1.1 now supports Ops Manager v2.1.7 and later. For more information, see [Known Issues](#known-issues) with PKS v1.1.0.

## <a id="v1.1.0"></a>v1.1.0

**Release Date**: June 28, 2018

### <a id="v1.1.0-upgrade"></a>Upgrade Procedure

<p class="note"><strong>Note</strong>:
The only supported upgrade path for PKS v1.1.0 is from PKS v1.0.2 and later. Do not upgrade directly to PKS v1.1.0 from v1.0.0. Instead, first upgrade PKS v1.0.0 to v1.0.2; then upgrade PKS v1.0.2 to v1.1.0. Alternatively, do a clean install of PKS v1.1.0.
</p>

To upgrade to PKS v1.1.0, follow the procedures in [Upgrade PKS](upgrade-pks.html).

### <a id="v1.1.0-features"></a>Features

This section describes new features introduced in PKS v1.1.0.

#### General Features

* Adds support for Kubernetes 1.10.3.
* Adds support for backing up and restoring PKS using BOSH Backup and Restore (BBR). For more information, see [Backing Up and Restoring PKS](backup-and-restore.html).
* Adds support for granting PKS control plane access to clients and external LDAP groups. For more information, see the [Grant Cluster Access](manage-users.html#cluster-access) section of _Manage Users in UAA_.
* Adds support for allowing workers to be deployed across Availability Zones (AZs).
* Adds support for network automation and node network isolation.
* Adds support for NFS by enabling rpcbind on worker nodes.
* Adds support for kube-controller-manager to issue certificates.
* Adds support for configuring HTTP/HTTPS proxy to be used by the Kubernetes control plane.
* Adds support for configuring the SecurityContextDeny admission controller. For more information, see [Using Admission Controllers](https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/) in the Kubernetes documentation.
* Enables the MutatingAdmissionWebhook admission controller. For more information, see [Using Admission Controllers](https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/) in the Kubernetes documentation.
* Enables audit logging for the API server.
* Creates logs for delete-all-cluster errands in the /var/vcap/sys/log/delete-all-clusters folder on the PKS control plane VM.
* Adds BOSH instance IDs to worker node labels.
* Hardens security by removing the ABAC authorization option for clusters.
* Hardens security by using service account IDs instead of service account keys for GCP deployments.
* Hardens security for Kubernetes system components. For example, kube-dns now uses its own configuration instead of the kubelet configuration.

#### vSphere Features

* Adds support for NO-NAT deployment topologies for PKS installations on NSX-T. For more information, see [Installing and Configuring PKS with NSX-T Integration](installing-nsx-t.html).
* Adds support for PKS integration with [VMware Wavefront](https://www.wavefront.com) to capture metrics for clusters and pods. For more information, see the [(Optional) Logging](installing-pks.html#syslog) section of _Installing and Configuring PKS_.
* Adds support for node network access via HTTP proxy for vSphere deployments. For more information, see the [Networking](installing-pks.html#networking) section of _Installing and Configuring PKS_.
* Adds support for PKS integration with [VMware vRealize Log Insight (vRLI)](https://www.vmware.com/products/vrealize-log-insight.html) for tagged logging of the control plane, clusters, and pods. For more information, see the [(Optional) Monitoring](installing-pks.html#monitoring) section of _Installing and Configuring PKS_.
* Adds support for integration with [VMware Analytics Cloud (VAC)](https://codepen.io/didkobravo/project/live/AYdRpX) to capture telemetry information.
* Hardens security by removing VM change permissions from worker nodes for vSphere deployments.
* Hardens security by removing vCenter user credentials from worker nodes for vSphere deployments.
* Adds support for [Harbor Registry]( https://vmware.github.io/harbor/) integration enhancements: updated Harbor tile, ability to use NFS and Google Buckets as an image store, and HTTP/HTTPS proxy servers for Clair.

### <a id="v1.1.0-features"></a>Bug Fixes

* Prevents unnecessary route creation in the kube-controller-manager.
* Retains the original source IP when using Flannel.
* Disables the read-only port in the kubelet configuration.
* Disables cAdvisor in the kubelet configuration.
* For added security, the Kubernetes API server no longer tries to fix malformed requests.
* The Kubernetes API server now cleans up terminated pods more often to avoid running out of disk space.
* The Kubernetes API server now unmounts volumes of terminated pods for security reasons.
* Operators no longer have to manually delete NSX-T objects created during the life of the product. In PKS v1.1, running the `pks delete-cluster` command deletes all NSX objects.

### <a id="v1.1.0-beta"></a>Beta Components

* Adds support for deploying multiple Kubernetes master nodes across AZs. For information about configuring multiple masters, see the [Plans](installing-pks.html#plans) section of <em>Installing and Configuring PKS</em>.
  <%= partial 'beta-component' %>
    <p class="note warning"><strong>WARNING</strong>: You cannot change the number of master nodes for existing clusters. To use the multi-master feature, you must create a new plan that uses multiple master/etcd nodes and deploy a new cluster. If you are already using all three plan configurations in the PKS tile, you must delete a plan and all clusters you deployed using that plan before you can deploy a multi-master cluster.</p>

### <a id="v1.1.0-versions"></a>Component Versions

PKS v1.1.0 includes or supports the following component versions:

<p class="note warning"><strong>WARNING</strong>: PKS v1.1.0 does not support Ops Manager v2.1.7 and later.</p>

<table border="1" class="nice">
  <tbody>
    <tr>
    <th>Product Component</th>
    <th>Version Supported</th>
    <th>Notes</th>
   </tr>
   <tr>
   <td>Pivotal Cloud Foundry Operations Manager (Ops Manager)</td>
   <td>2.1.0-2.1.6</td>
   <td>Separate download available from Pivotal Network</td>
   </tr>
   <tr>
   <td>Stemcell</td>
   <td>3586.24</td>
   <td></td>
   </tr>
   <tr>
   <td>Kubernetes</td>
   <td>1.10.3</td>
   <td>Packaged in the PKS Tile (CFCR)</td>
   </tr>
   <tr>
   <td>CFCR (Kubo)</td>
   <td>0.17</td>
   <td>Packaged in the PKS Tile</td>
   </tr>
   <tr>
   <td>Golang</td>
   <td>1.9.7</td>
   <td>Packaged in the PKS Tile</td>
   </tr>   
   <tr>
   <td>NCP</td>
   <td>2.2</td>
   <td>Packaged in the PKS Tile</td>
   </tr>
   <tr>
   <td>Kubernetes CLI</td>
   <td>1.10.3</td>
   <td>Separate download available from the PKS section of Pivotal Network</td>
   </tr>
   <tr>
   <td>PKS CLI</td>
   <td>1.1</td>
   <td>Separate download available from the PKS section of Pivotal Network</td>
   </tr>
   <tr>
   <td>VMware vSphere</td>
   <td>6.5 U2, 6.5 U1, and 6.5. Editions: 
   <ul>
   <li>vSphere Enterprise Plus Edition</li>
   <li>vSphere with Operations Management Enterprise Plus</li>
   </ul>
   </td>
   <td>vSphere versions supported for Pivotal Container Service (PKS)</td>
   </tr>
   <td>VMware NSX-T</td>
   <td>2.1 - Advanced Edition</td>
   <td>NST-T versions supported for Pivotal Container Service (PKS)</td>
   </tr>
   <tr>
   <td>VMware Harbor Registry</td>
   <td>1.5.0</td>
   <td>Separate download available from Pivotal Network</td>
   </tr>
   <tr>
   <td>VMware vRealize Log Insight (for vSphere deployments)</td>
   <td>4.6</td>
   <td>Separate download available from Pivotal Network</td>
   </tr>
   </tbody>
   <tfoot>
   <tr>
    <td colspan="3"><em>* Components marked with an asterisk have been patched to resolve security vulnerabilities or fix component behavior.</em></td>
   </tr>
  </tfoot>
</table>

### <a id="known-issues"></a>Known Issues

This section includes known issues with PKS v1.1.0 and corresponding workarounds.

* PKS v1.1.0 does not support Ops Manager v2.1.7 and later.
For more information, see [Error: Duplicate Variable Name](troubleshoot-issues.html#duplicate-name) in the _Troubleshooting_ topic.
* If you use PKS CLI v1.0.x with PKS tile v1.1.x, you must log in every 600 
seconds to manually refresh the CLI token.
Pivotal recommends upgrading to PKS CLI v1.1.x to solve this issue.
* If you upgrade PKS from v1.0.x to v1.1, you must enable the **Upgrade All Clusters** errand in the PKS tile configuration. 
This ensures existing clusters can perform resize or delete actions after 
the upgrade.

#### <a id="compromised-cluster"></a>Cluster Security Recommendations

To reduce the risk of compromised clusters in your PKS deployment, the following policies are recommended:

* Ensure that only trusted operators and systems have access to clusters.
* Ensure that only trusted images are deployed to clusters.
* Maintain trusted images to consistently include current security fixes.
* Do not expose network ports to untrusted networks unless strictly required.

#### <a id="target-lb"></a>Reconfigure GCP Load Balancers After Master VM Recreation

If Kubernetes master node VMs are recreated for any reason, you must reconfigure your cluster load balancers to point to the new master VMs.
For example, after a stemcell upgrade, BOSH recreates the VMs in your deployment.

To reconfigure your GCP cluster load balancer to use the new master VM, follow the procedure in the [Reconfiguring a GCP Load Balancer](gcp-cluster-load-balancer.html#reconfigure) section of <em>Configuring a GCP Load Balancer for PKS Clusters</em>.

#### <a id="abac-clusters"></a>Existing ABAC Clusters

Attribute-based access control (ABAC) is no longer supported in v1.1.
Delete any ABAC clusters before upgrading to v1.1.

#### <a id="default-vm-type"></a>New Default VM Type

In the [Resource Config](installing-pks.html#resource-config) pane, the default **VM Type** is now **large**.
This is to ensure that PKS control plane VM has sufficient resources.

If the VMs in your PKS installation use the default VM type, your VMs will use the new **large** VM type after upgrading to PKS v1.1.0.

If the VMs in your PKS installation use a custom VM type, your configuration remains the same after upgrading to PKS v1.1.0.
