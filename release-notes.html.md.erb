---
title: Release Notes
owner: TKGI
topictype: releasenotes
---

This topic contains release notes for <%= vars.product_short %> (<%= vars.k8s_runtime_abbr %>) v1.10.

<p class="note warning"><strong>Warning:</strong> Before installing or upgrading to <%= vars.product_short %> v1.10,
review the <a href="#1-10-0-breaking-changes">Breaking Changes</a> below.
</p>

## <a id="1.10.0"></a><%= vars.k8s_runtime_abbr %> v1.10.0

**Release Date**: January 28, 2021

### <a id='1-10-0-snapshot'></a><a id='product-snapshot'></a>Product Snapshot

<table class="nice">
    <tr>
        <th>Release</th>
        <th>Details</th>
    </tr>
    <tr>
        <td>Version</td>
        <td>v1.10.0</td>
    </tr>
    <tr>
        <td>Release date</td>
        <td>January 28, 2021</td>
    </tr>
    <tr>
        <th>Component</th>
        <th>Version</th>
    </tr>
    <tr>
        <td>Kubernetes</td>
        <td>v1.19.6</td>
    </tr>
    <tr>
        <td>CoreDNS</td>
        <td>v1.7.0+vmware.5</td>
    </tr>
    <tr>
        <td>Docker</td>
        <td>
            Linux: v19.03.14<br>
            Windows: v19.03.14
        </td>
    </tr>
    <tr>
        <td>etcd</td>
        <td>v3.4.13</td>
    </tr>
    <tr>
        <td>Metrics Server</td>
        <td>v0.3.6</td>
    </tr>
    <tr>
        <td>NCP</td>
        <td>v3.1.0.1</td>
    </tr>
    <tr>
        <td>Percona XtraDB Cluster (PXC)</td>
        <td>v0.31.0</td>
    </tr>
    <tr>
        <td>UAA</td>
        <td>v74.5.21</td>
    </tr>
    <tr>
        <th>Compatibilities</th>
        <th>Versions</th>
    </tr>
    <tr>
        <td>Ops Manager</td>
        <td> Ops Manager 2.10.4 or later, or 2.9.15 or later.</td>
    </tr>
    <tr>
        <td>Xenial stemcells</td>
        <td>See <a href="https://network.pivotal.io/products/pivotal-container-service#/releases/820908"><%= vars.product_network %>.</a></td>
    </tr>
    <tr>
        <td>Windows stemcells</td>
        <td>v2019.29+</td>
    </tr>
    <%#not in interop matrix
    <tr>
        <td>Backup and Restore SDK</td>
        <td></td>
    </tr>
    %>
    <tr>
        <td>vSphere</td>
        <td>See <a href="https://www.vmware.com/resources/compatibility/sim/interop_matrix.php#interop&644=&175=&1=">VMware Product Interoperability Matrices.</a></td>
    </tr>
    <tr>
        <td>VMware Cloud Foundation (VCF)</td>
        <td>v4.2.0</td>
    </tr>
    <tr>
        <td>CNS for vSphere</td>
        <td>v1.0.2, v2.0.0</td>
    </tr>
    <tr>
        <td>NSX-T</td>
        <td>v3.0.1, v3.0.2, v3.1.0</td>
    </tr>
    <tr>
        <td>Harbor</td>
        <td>v2.1.1</td>
    </tr>
    <tr>
        <td>Velero</td>
        <td><a href="https://my.vmware.com/en/group/vmware/downloads/details?downloadGroup=VELERO-142&productId=1113&rPId=58884">v1.4.2</a></td>
    </tr>
</table>

### <a id='1-10-0-upgrade'></a>Upgrade Path <%#CONFIRM BEFORE GA%>

The supported upgrade paths to <%= vars.k8s_runtime_abbr %> v1.10.0 are from <%= vars.product_short %> v1.9.0 and later.

### <a id="1-10-0-features"></a>Features

This section describes new features and changes in <%= vars.product_full %> v1.10.0.

* (Beta) Supports high availability (HA) mode in <%= vars.product_short %>.
You can now scale the number of VM instances for the following <%= vars.product_short %> control plane jobs:

    * <%= vars.product_short %> API and UAA
    * <%= vars.product_short %> database

    To configure HA (beta) for new <%= vars.k8s_runtime_abbr %> v1.10 installations, see [Resource Config](installing-vsphere.html#resource-config) in the _Installing_ topic for your IaaS.
    To configure HA (beta) for <%= vars.product_short %> v1.10 upgrades from v1.9, see [Upgrading Enterprise PKS](upgrade.html) or [Upgrading Enterprise PKS with NSX-T](upgrade-nsxt.html)
    and follow the instructions in _(Optional) Scale to <%= vars.k8s_runtime_abbr %> High Availability Mode_.
    <p class="note warning"><strong>Warning:</strong> HA mode is a beta feature. Do not scale your <strong><%= vars.k8s_runtime_abbr %> API</strong> or <strong><%= vars.k8s_runtime_abbr %> Database</strong> to more than one instance in production environments.</p>
* Removes **Wavefront Alert Recipient**, **Create pre-defined Wavefront alerts errand**, and **Delete pre-defined Wavefront alerts errand** from the <%= vars.product_short %> tile. For more information, see [<%= vars.k8s_runtime_abbr %>-Defined Wavefront Alerts Removed from the Tile](#1-10-0-wavefront-integration) below.
* **[Enhancement]** Reduced the possibility of <%= vars.k8s_runtime_abbr %> CLI `get-credentials` returning the error “_od-broker is processing a request for the same instance... please try again later_” during periods of intermittent latency.
* **[Enhancement]** Swap is now disabled by default. For more information, see [Swap Is Disabled by Default ](#1-10-0-swap-disabled) in _Breaking Changes_ below.
* **[Enhancement]** Improves error messages for cluster creation, update and upgrade failures. 
For more information, see [Cluster Creation, Update and Upgrade Failure Error Messages No Longer Truncated](#1-10-0-cluster-errors) in _Breaking Changes_ below.
* **[Security Fix]** Passes additional CIS Kubernetes Benchmarks. See [<%= vars.k8s_runtime_abbr %> Cluster Benchmarks](./security.html#benchmarks) for details.
* **[Bug Fix]** Fixes [Your <%= vars.k8s_runtime_abbr %> Cluster Fails to Start After Changing Your Worker Node's Compute Profile AZ](https://docs.pivotal.io/tkgi/1-9/release-notes.html#1-9-2-compute-profile-az-worker).
* **[Bug Fix]** Fixes [<%= vars.k8s_runtime_abbr %> Upgrade or Install Fails with Error "x509: certificate relies on legacy Common Name field"](https://docs.pivotal.io/tkgi/1-9/release-notes.html#1-9-0-failed-upgrade). 
* **[Bug Fix]** Fixes [Cluster Creation Fails During the 'Creating Load Balancer' Step](https://docs-pcf-staging.cfapps.io/tkgi/1-9/release-notes.html#1-9-0-401-403). 
* **[Bug Fix]** Fixes [Network Profile Required with Compute Profile](https://docs.pivotal.io/tkgi/1-9/release-notes.html#1-9-0-az-network-profile).  
* **[Bug Fix]** Fixes [Kubernetes Worker Nodes Run Out of Space](https://docs.pivotal.io/tkgi/1-9/release-notes.html#1-9-0-k8s-race-condition).  

<%#
* **[Bug Fix]** Fixes [You Cannot Change a Master Node's AZ](https://docs.pivotal.io/tkgi/1-9/release-notes.html#1-9-0-compute-profile-az-master).  
%>

#### <a id="1-10-0-proxy"></a>Cluster-Specific Proxy Settings (NSX-T and AWS)

You can configure proxy settings specific to individual <%= vars.k8s_runtime_abbr %> clusters,
overriding the global settings in the <%= vars.k8s_runtime_abbr %> tile > **Networking** pane.
For more information, see [Configure Cluster Proxies](proxies-cluster.html).


#### <a id="1-10-0-antrea-cni"></a>Supports the Antrea CNI

<%= vars.product_short %> now provides the option to use the Antrea Container Network Interface (CNI) as
the CNI for new <%= vars.k8s_runtime_abbr %>-provisioned clusters. For more information about 
using Antrea as your CNI, see [About Upgrading from the Flannel CNI to the Antrea CNI](understanding-upgrades.html#cni) 
in _About <%= vars.product_short %> Upgrades_.

#### <a id="1-10-0-cert-rotate"></a>NSX-T Certificate Rotation

You can now rotate TKGI-provisioned Kubernetes cluster NSX-T TLS certificates using a TKGI CLI command. 
For more information, see [Certificate Rotation](nsxt-certs-rotate.html).

#### <a id="1-10-0-node-labels-and-taints"></a>Apply Persistent Node Labels and Node Taints Using Compute Profiles

On vSphere and vSphere with NSX-T, <%= vars.product_short %> supports applying persistent labels and taints to a Kubernetes node using Compute Profiles. 
For more information see [`node_pools` Block](compute-profiles-manage.html#worker-nodes) 
in _Creating and Managing Compute Profiles with the CLI (vSphere)_.

#### <a id="1-10-0-windows-ad-support"></a>Windows Worker Kubernetes Clusters Support Active Directory

Windows Server with Active Directory can now control access to <%= vars.k8s_runtime_abbr %> Windows worker-based Kubernetes clusters 
through integration with group Managed Service Account (gMSA). For more information, see [Authenticate Windows Clusters with Active Directory](gmsa.html).

#### <a id="1-10-0-wavefront-integration"></a> <%= vars.k8s_runtime_abbr %>-Defined Wavefront Alerts Removed from the Tile

<%= vars.k8s_runtime_abbr %> v1.10 removes the following configuration options from the Wavefront integration in the tile:

* **Create pre-defined Wavefront alerts errand**
* **Delete pre-defined Wavefront alerts errand**
* **Wavefront Alert Recipient**

If you want to enable pre-defined Wavefront alerts for <%= vars.k8s_runtime_abbr %> v1.10, configure your alert targets in Wavefront. For a list of available alerts, see [Predefined Alerts for the Integration](https://docs.wavefront.com/integrations_tkgi.html#predefined-alerts-for-the-integration).

If you enabled the **Create pre-defined Wavefront alerts errand** and **Wavefront Alert Recipient** in an earlier version of
<%= vars.k8s_runtime_abbr %> and you upgrade your environment to v1.10, you will continue to receive the
<%= vars.k8s_runtime_abbr %>-defined alerts.

#### <a id="1-10-0-component-updates"></a>Component Updates

The following components have been updated:

* Bumps Kubernetes to v1.19.4.
* Bumps Xenial stemcell to v621.94.
* Bumps NCP to v3.1.0.17170700.
* Bumps PXC to v0.31.0.
* Bumps UAA to v74.5.21.

### <a id="1-10-0-breaking-changes"></a> Breaking Changes
TKGI v1.10.0 has the following breaking changes. 

#### <a id='1-10-0-nsxt-3'></a> <%= vars.k8s_runtime_abbr %> v1.10 Is Not Compatible with NSX-T v2.5.2 or Earlier 

<%= vars.k8s_runtime_abbr %> v1.10 is not compatible with NSX-T v2.5.2 or earlier. 
If you are deploying <%= vars.k8s_runtime_abbr %> v1.10 to NSX-T, your NSX-T version must be 
NSX-T v3.0.1 or later. For more information about upgrading NSX-T and <%= vars.k8s_runtime_abbr %>, 
see [Upgrade Order for Tanzu Kubernetes Grid Integrated Edition Environments on vSphere](upgrade-scenarios.html) 
and [Upgrading Tanzu Kubernetes Grid Integrated Edition (NSX-T Networking)](upgrade-nsxt.html).  

#### <a id='1-10-0-swap-disabled'></a> Swap Is Disabled by Default  

Swap is now disabled on all worker nodes. 
In previous versions of <%= vars.product_short %>, Swap was enabled, but upstream Kubernetes does not support this setting. 
You cannot enable swap through the TKGI CLI, and manually configuring swap is not permitted. 

#### <a id='1-10-0-cluster-errors'></a> Cluster Creation, Update and Upgrade Failure Error Messages No Longer Truncated

<%= vars.product_short %> v1.10 includes improved error messages for cluster creation, update and upgrade failures. 
Previously, error messages greater than 128 bytes were truncated. In <%= vars.k8s_runtime_abbr %> v1.10 
logged cluster creation and upgrade failure error messages are no longer truncated. 


###<a id='1-10-0-known-issues'></a>Known Issues

<%= vars.k8s_runtime_abbr %> v1.10.0 has the following known issues:

#### <a id="1-10-0-nsxt-302-310"></a> Pods Stop After Upgrading From NSX-T v3.0.2 to v3.1.0 on vSphere 7.0 and 7.0.1

**Symptom**

Your TKGI-provisioned Pods stop after upgrading from NSX-T v3.0.2 to NSX-T v3.1.0 on vSphere 7.0 and 7.0.1.

**Explanation**

For information, see [Issue 2603550: Some VMs are vMotioned and lose network connectivity during UA nodes upgrade]
(https://docs.vmware.com/en/VMware-NSX-T-Data-Center/3.1/rn/VMware-NSX-T-Data-Center-311-Release-Notes.html#2603550) in the _VMware NSX-T Data Center 3.1.1 Release Notes_.

**Workaround**

To avoid the loss of network connectivity during UA node upgrade, ensure DRS is set to manual mode during your upgrade from NSX-T v3.0.2 to v3.1.0.

If you upgraded to NSX-T v3.1.0 with DRS in automation mode, run the following on the affected Pods' master VMs to restore Pod connectivity:

```

monit restart ncp

```

For more information on upgrading NSX-T v3.0.2 to NSX-T v3.1.0, see [Upgrade NSX-T Data Center to v3.0 or v3.1](upgrade-nsxt.html#upgrade-nsxt).  

#### <a id="1-10-0-azure-apply-changes"></a> Error: Could Not Execute "Apply-Changes" in Azure Environment

**Symptom**

After clicking **Apply Changes** on the <%= vars.k8s_runtime_abbr %> tile in an Azure environment, you experience
an error '_...could not execute "apply-changes"..._' with either of the following descriptions:

* _{"errors":{"base":["undefined method 'location' for nil:NilClass"]}}_
* _FailedError.new("Resource Groups in region '#{location}' do not support Availability Zones"))_

For example:

```
INFO | 2020-09-21 03:46:49 +0000 | Vessel::Workflows::Installer#run | Install product (apply changes)
2020/09/21 03:47:02 could not execute "apply-changes": installation failed to trigger: request failed: unexpected response from /api/v0/installations:
HTTP/1.1 500 Internal Server Error
Transfer-Encoding: chunked
Cache-Control: no-cache, no-store
Connection: keep-alive
Content-Type: application/json; charset=utf-8
Date: Mon, 21 Sep 2020 17:51:50 GMT
Expires: Fri, 01 Jan 1990 00:00:00 GMT
Pragma: no-cache
Referrer-Policy: strict-origin-when-cross-origin
Server: Ops Manager
Strict-Transport-Security: max-age=31536000; includeSubDomains
X-Content-Type-Options: nosniff
X-Download-Options: noopen
X-Frame-Options: SAMEORIGIN
X-Permitted-Cross-Domain-Policies: none
X-Request-Id: f5fc99c1-21a7-45c3-7f39
X-Runtime: 9.905591
X-Xss-Protection: 1; mode=block

44
{"errors":{"base":["undefined method `location' for nil:NilClass"]}}
0
```

**Explanation**

The Azure CPI endpoint used by Ops Manager has been changed and
your installed version of Ops Manager is not compatible with the new endpoint.

**Workaround**

Run the following Ops Manager CLI command:

```
om --skip-ssl-validation --username USERNAME --password PASSWORD --target https://OPSMAN-API curl --silent --path /api/v0/staged/director/verifiers/install_time/IaasConfigurationVerifier -x PUT -d '{ "enabled": false }'
```

Where:

* `USERNAME` is the account to use to run Ops Manager API commands.
* `PASSWORD` is the password for the account.
* `OPSMAN-API` is the IP address for the Ops Manager API


For more information, see [Error 'undefined method location' is received when running Apply Change on Azure]
(https://community.pivotal.io/s/article/undefined-method-location-when-running-Apply-Change-on-Azure?language=en_US)
in the VMware Tanzu Knowledge Base.

#### <a id="1-10-0-vrops-windows-clusters"></a> VMware vRealize Operations Does Not Support Windows Worker-Based Kubernetes Clusters

VMware vRealize Operations (vROPs) does not support Windows worker-based Kubernetes clusters and cannot be used to
manage <%= vars.k8s_runtime_abbr %>-provisioned Windows workers.


#### <a id='1-10-0-wavefront-no-win'></a><%= vars.k8s_runtime_abbr %> Wavefront Requires Manual Installation for Windows Workers

To monitor Windows-based worker node clusters with a Wavefront collector and proxy, you must first install Wavefront on the clusters maunually, using Helm.
For instructions, see the [Wavefront](windows-monitoring.html#wavefront) section of the _Monitoring Windows Worker Clusters and Nodes_ topic.

#### <a id='1-10-0-ping'></a>Pinging Windows Worker Kubernetes Clusters Does Not Work

<%= vars.k8s_runtime_abbr %>-provisioned Windows worker-based Kubernetes clusters inherit a Kubernetes limitation that prevents
outbound ICMP communication from workers.
As a result, pinging Windows workers does not work.

For information about this limitation, see [Limitations > Networking](https://kubernetes.io/docs/setup/production-environment/windows/intro-windows-in-kubernetes/#networking-1)
in the _Windows in Kubernetes_ documentation.

#### <a id="1-10-0-windows-velero-limitations"></a> Velero Does Not Support Backing Up Stateful Windows Workloads

You can use Velero to backup stateless <%= vars.k8s_runtime_abbr %>-provisioned Windows workers.
Velero can back up stateless Windows workloads only, and cannot be used to backup stateful Windows applications.
For more information, see [Velero on Windows](https://velero.io/docs/v1.4/basic-install/#velero-on-windows) in
_Basic Install_ in the Velero documentation.

#### <a id="1-10-0-tmc-on-gcp"></a>Tanzu Mission Control Integration Not Supported on GCP

<%= vars.k8s_runtime_abbr %> on Google Cloud Platform (GCP) does not support
Tanzu Mission Control (TMC) integration, which is configured in
the **<%= vars.product_tile %>** tile > the **Tanzu Mission Control (Experimental)** pane.

If you intend to run <%= vars.k8s_runtime_abbr %> v1.9 on GCP,
skip this pane when configuring the <%= vars.product_tile %> tile.

#### <a id='1-10-0-tmc-restic'></a>TMC Data Protection Feature Requires Privileged <%= vars.k8s_runtime_abbr %> Containers
TMC Data Protection feature supports privileged <%= vars.k8s_runtime_abbr %> containers only.
For more information, see [Plans](installing-vsphere.html#plans) in the _Installing TKGI_ topic for your IaaS.

#### <a id="1-10-0-profile-no-win-gmsa"></a>Windows Worker Kubernetes Clusters with Group Managed Service Account Do Not Support Compute Profiles 

Windows worker-based Kubernetes clusters integrated with group Managed Service Account (gMSA) cannot be managed using Compute Profiles.  

#### <a id="1-10-0-profile-no-win-flannel"></a> Windows Worker Kubernetes Clusters on Flannel Do Not Support Compute Profiles

On vSphere with NSX-T networking you can use compute profiles with both Linux and Windows worker&#8209;based Kubernetes clusters.
On vSphere with Flannel networking, you can apply compute profiles only to Linux clusters.

#### <a id="1-10-0-old-profile-no-cli"></a><%= vars.k8s_runtime_abbr %> Does Not Support Managing Pre-<%= vars.k8s_runtime_abbr %> v1.9 Compute Profiles

Compute profiles created in <%= vars.k8s_runtime_abbr %> v1.8 and earlier have a different format
from current compute profiles.

<%= vars.k8s_runtime_abbr %> v1.10 does not support resizing, updating, upgrading, or
managing compute profiles using `tkgi` CLI compute profile commands,
on a cluster that has a compute profile created in <%= vars.k8s_runtime_abbr %> v1.8 and earlier.

#### <a id="1-10-0-profile-resize-down"></a>TKGI CLI Does Not Prevent Reducing the Control Plane Node Count

TKGI CLI does not prevent accidentally reducing a cluster's control plane node count using a compute profile.

<p class="note warning"><strong>Warning:</strong>
    Reducing a cluster's control plane node count can destroy the cluster.
    Do not scale out or scale in existing master nodes by reconfiguring the TKGI tile or by using a compute profile.
    Reducing a cluster's number of control plane nodes may remove a master node and cause the cluster to become inactive.
</p>

#### <a id="1-10-0-profile-bosh-upgrade"></a>Compute Profile Dropped From Clusters During BOSH Upgrade

If a cluster created using a compute profile is upgraded using `tkgi upgrade-cluster`,
the cluster's compute profile will be dropped.

#### <a id="1-10-0-in-windows-notready-nodes"></a> Windows Cluster Nodes Not Deleted After VM Deleted

**Symptom**

After you delete a VM using your IAAS' management console you notice a Windows worker node
that had been on that VM is now in a `notReady` state.

**Solution**

1. To identify the leftover node:

    ```
    kubectl get no -o wide
    ```
1. Locate nodes on the returned list that are in a `notReady` state and have the same IP address as another node in the list.
1. To manually delete a `notReady` node:

    ```
    kubectl delete node NODE-NAME
    ```
    Where `NODE-NAME` is the name of the node in the `notReady` state.

#### <a id="1-10-0-oidc-response-header"></a>502 Bad Gateway After OIDC Login

**Symptom**

You experience a "502 Bad Gateway" error from the NSX load balancer after you log in to OIDC.

**Explanation**

A large response header has exceeded your NSX-T load balancer maximum
response header size. The default maximum response header size is 10,240 characters and should
be resized to 50,000.

**Workaround**

If you experience this issue, manually reconfigure your NSX-T `request_header_size`
and `response_header_size` to 50,000 characters.
For information about configuring NSX-T default header sizes,
see [OIDC Response Header Overflow](https://community.pivotal.io/s/article/OIDC-Response-Header-overflow) in the Knowledge Base.


#### <a id='1-10-0-eight-cores-error'></a> NSX-T Pre-Check Errand Fails Due to Edge Node Configuration

**Symptom**

You have configured your NSX-T Edge Node VM as `medium` size,
and the NSX-T Pre-Check Errand fails with the following error:
"_ERROR: NSX-T Precheck failed due to Edge Node ... no of cpu cores is less than 8_".

**Explanation**

The NSX-T Pre-Check Errand is erroneously returning the "_cpu cores is less than 8_" error.

**Solution**

You can safely configure your NSX-T Edge Node VMs as `medium` size and ignore the error.

#### <a id='1-10-0-windows-proxy'></a> Difficulty Changing Proxy for Windows Workers

You must configure a global proxy in the <%= vars.product_tile %> tile > **Networking** pane before you create any Windows workers that use the proxy.

You cannot change the proxy configuration for Windows workers in an existing cluster.

#### <a id='1-10-0-http-proxy-password'></a> Character Limitations in HTTP Proxy Password

For vSphere with NSX-T, the HTTP Proxy password field does not support the following special characters: `&` or `;`.

#### <a id='1-10-0-harbor-storage-config'></a> Error After Modifying Your Harbor Storage Configuration

**Symptom**

You receive the following error after modifying your existing Harbor installation's storage configuration:

```
Error response from daemon: manifest for ... not found: manifest unknown: manifest unknown
```


**Explanation**

Harbor does not support modifying an existing Harbor installation's storage configuration.

**Workaround**

To modify your Harbor storage configuration,
re-install Harbor. Before starting Harbor, configure the new Harbor installation with the desired configuration.

#### <a id='1-10-0-antrea-logging-interupt'></a> Unexplained Errors After Interrupting a Log Stream When Using Antrea Networking 

**Symptom**

While using Antrea networking, you observe unexplainable errors after you interrupt a log stream started using `kubectl logs -f POD-NAME`. 
The errors could include any of the following:  

* kubectl returns the error: "_Error from server (TooManyRequests): the server has received too many_".   
* `kube-apiserver` returns an http code `429`. 

**Explanation**

When using Antrea networking there is a chance that `konnectivity-agent` will become unstable after interrupting your kubectl log steam.

**Workaround**

To resolve the issue:  

1. Log in to the master VM:

    ```
    bosh -d DEPLOYMENT-NAME ssh master/0
    ````

1. Change to root:

    ```
    sudo -i
    ```

1. Restart `proxy-server`:

    ```
    monit restart proxy-server
    ```

1. Wait for `proxy-server` restart:

    ```
    monit summary
    ```


#### <a id='1-10-0-resizing-worker-nodes'></a> Ingress Controller Statefulset Fails to Start After Resizing Worker Nodes

**Symptom**  

Permissions are removed from your cluster’s files and processes after resizing the persistent disk 
during a cluster upgrade. The ingress controller statefulset fails to start.

**Explanation**  

When resizing a persistent disk, Bosh migrates the data from the old disk to the new disk but 
does not copy the files’ extended attributes.

**Workaround**  

To resolve the problem, complete the steps in 
[Ingress controller statefulset fails to start after resize of worker nodes with permission denied]
(https://community.pivotal.io/s/article/5000e00001nCJxT1603094435795?language=en_US)
in the VMware Tanzu Knowledge Base.


#### <a id='1-10-0-uuid-length'></a> One Plan ID Longer than Other Plan IDs

**Symptom**

One of your plan IDs is one character longer than your other plan IDs.

**Explanation**

In <%= vars.k8s_runtime_abbr %>, each plan has a unique plan ID.
A plan ID is normally a UUID consisting of 32 alphanumeric characters and 4 hyphens.
However, the **Plan 4** ID consists of 33 alphanumeric characters and 4 hyphens.

**Solution**

You can safely configure and use **Plan 4**.
The length of the **Plan 4** ID does not affect the functionality of **Plan 4** clusters.

If you require all plan IDs to have identical length, do not activate or use **Plan 4**.

## <a id="management-console-1.10.0"></a> <%= vars.k8s_runtime_abbr %> Management Console 1.10.0

**Release Date**: January 28, 2021

### <a id="management-console-1.10.0-features"></a>Features

<%= vars.product_short %> Management Console v1.10.0 updates include:  

* **[Bug Fix]** Fixes [TKGI Management Console regenerates certificates if the NSX Manager admin password changes](https://docs.pivotal.io/tkgi/1-9/release-notes.html#1-9-1-nsxt-cert).
* **[BETA]** Supports high availability (HA) mode in <%= vars.product_short %> Management Console.
You can now scale the number of VM instances for the following <%= vars.product_short %> control plane jobs:

    * <%= vars.product_short %> API and UAA
    * <%= vars.product_short %> database
* Adds support for Antrea CNI when deploying to vSphere without NSX-T networking.
* Adds support for a No-NAT with virtual switch (VSS/VDS) topology.
* Adds support for changing the compute profile after cluster creation.
* Adds support for adding labels and taints to nodes when creating compute profiles.
* Enforces the vSphere standard for passwords when creating local user accounts.

### <a id="management-console-1.10.0-snapshot"></a>Product Snapshot

<p class="note"><strong>Note</strong>: <%= vars.product_short %> Management Console provides an opinionated
    installation of <%= vars.k8s_runtime_abbr %>. The supported versions may differ from or be more limited than
    what is generally supported by <%= vars.k8s_runtime_abbr %>.</p>

<table class="nice">
    <th>Element</th>
    <th>Details</th>
    <tr>
      <td>Version</td>
      <td>v1.10.0</td>
    </tr>
    <tr>
      <td>Release date</td>
      <td>January 28, 2021</td>
    </tr>
    <tr>
      <td>Installed Tanzu Kubernetes Grid Integrated Edition version</td>
      <td>v1.10.0</td>
    </tr>
    <tr>
      <td>Installed Ops Manager version</td>
      <td>2.10.5</td>
    </tr>
    <tr>
      <td>Installed Kubernetes version</td>
        <td>1.19.6</td>
    </tr>
    <tr>
      <td>Compatible NSX-T versions</td>
      <td>v3.0.1.2, v3.0.2, v3.1</td>
    </tr>
    <tr>
      <td>Installed Harbor Registry version</td>
      <td>2.1.2</td>
    </tr>
        <tr>
        <td>Linux stemcell</td>
        <td>621.97</td>
    </tr>
    <tr>
        <td>Windows stemcells</td>
        <td>&gt;=2019.29</td>
    </tr>
</table>

### <a id='management-console-1-10-0-upgrade'></a>Upgrade Path <%#CONFIRM BEFORE GA%>

The supported upgrade path to <%= vars.product_short %> Management Console v1.10.0 is from
<%= vars.product_short %> v1.9.0 and later.

### <a id="management-console-1.10.0-known-issues"></a> Known Issues

The <%= vars.product_short %> Management Console v1.10.0 has the following known issues:

#### <a id="management-console-1.10.0-ui-customcerts"></a> Management console UI does not open if the management console uses custom certificates

**Symptom**

If you configure <%= vars.product_short %> Management console with custom certificates, the management console interface fails to open. This is caused by the failure of the script `/etc/vmware/pks-appliance-tls.sh` on the management console VM.

**Workaround**

1. Use SSH to log in to the management console VM.
1. Open `/etc/vmware/pks-appliance-tls.sh` in a text editor.
1. Replace line 32 with the following code: <pre>sed -i '/^$/d' $file</pre>
1. Reboot the management console VM.

#### <a id="management-console-1.10.0-vrli-https"></a> vRealize Log Insight Integration Does Not Support HTTPS Connections

**Symptom**

The <%= vars.product_short %> Management Console integration to vRealize Log Insight does not support connections to the HTTPS port on the vRealize Log Insight server.

**Workaround**

1. Use SSH to log in to the <%= vars.product_short %> Management Console appliance VM.
1. Open the file `/lib/systemd/system/pks-loginsight.service` in a text editor.
1. Add `-e LOG_SERVER_ENABLE_SSL_VERIFY=false`.
1. Set `-e LOG_SERVER_USE_SSL=true`.

    The resulting file should look like the following example:

    ```
    ExecStart=/bin/docker run --privileged --restart=always --network=pks
    -v /var/log/journal:/var/log/journal
    --name=pks-loginsight
    -e TYPE=gear2-vm
    -e LOG_SERVER_HOST=${LOGINSIGHT_HOST}
    -e LOG_SERVER_PORT=${LOGINSIGHT_PORT}
    -e LOG_SERVER_ENABLE_SSL_VERIFY=false
    -e LOG_SERVER_USE_SSL=true
    -e LOG_SERVER_AGENT_ID=${LOGINSIGHT_ID}
    pksoctopus/vrli-journald:v07092019
    ```

1. Save the file and run `systemctl daemon-reload`.
1. To restart the vRealize Log Insight service, run `systemctl restart pks-loginsight.service`.

<%= vars.product_short %> Management Console can now send logs to the HTTPS port on the vRealize Log Insight server.

#### <a id="management-console-1.10.0-vsphere-ha"></a> vSphere HA causes Management Console ovfenv Data Corruption

**Symptom**

If you enable vSphere HA on a cluster, if the TKGI Management Console appliance VM is running on a host in that cluster, and if the host reboots, vSphere HA recreates a new TKGI Management Console appliance VM on another host in the cluster. Due to an issue with vSphere HA, the `ovfenv` data for the newly created appliance VM is corrupted and the new appliance VM does not boot up with the correct network configuration.

**Workaround**

- In the vSphere Client, right-click the appliance VM and select **Power** > **Shut Down Guest OS**.
- Right-click the appliance again and select Edit Settings.
- Select **VM Options** and click **OK**.
- Verify under Recent Tasks that a `Reconfigure virtual machine` task has run on the appliance VM.
- Power on the appliance VM.

#### <a id="management-console-1.10.0-k8s-profile"></a> Base64 encoded file arguments are not decoded in Kubernetes profiles

**Symptom**

Some file arguments in Kubernetes profiles are base64 encoded. When the management console displays the Kubernetes profile,
some file arguments are not decoded.

**Workaround**

Run `echo "$content" | base64 --decode`

#### <a id="management-console-1.10.0-network-profile"></a> Network profiles not immediately selectable

**Symptom**

If you create network profiles and then try to apply them in the Create Cluster page, the new profiles
are not available for selection.

**Workaround**

Log out of the management console and log back in again.

#### <a id="management-console-1.10.0-cluster-summary"></a> Real-Time IP information not displayed for network profiles

**Symptom**

In the cluster summary page, only default IP pool, pod IP block, node IP block values are displayed,
rather than the real-time values from the associated network profile.

**Workaround**

None

#### <a id='management-console-1-10-0-harbor-storage-config'></a> Error After Modifying Your Harbor Storage Configuration

**Symptom**

You receive the following error after modifying your existing Harbor installation's storage configuration:

```
Error response from daemon: manifest for ... not found: manifest unknown: manifest unknown
```


**Explanation**

Harbor does not support modifying an existing Harbor installation's storage configuration.

**Workaround**

To modify your Harbor storage configuration,
re-install Harbor. Before starting Harbor, configure the new Harbor installation with the desired configuration.