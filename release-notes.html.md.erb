---
title: Release Notes
owner: TKGI
topictype: releasenotes
---

This topic contains release notes for <%= vars.product_short %> (<%= vars.k8s_runtime_abbr %>) v1.11.

<p class="note warning"><strong>Warning:</strong> Before installing or upgrading to <%= vars.product_short %> v1.11,
review the <a href="#1-11-0-breaking-changes">Breaking Changes</a> below.
</p>

## <a id="1-11-0"></a><%= vars.k8s_runtime_abbr %> v1.11.0

**Release Date**: May 27, 2021

### <a id='1-11-0-snapshot'></a><a id='product-snapshot'></a>Product Snapshot

<table class="nice">
    <tr>
        <th>Release</th>
        <th>Details</th>
    </tr>
    <tr>
        <td>Version</td>
        <td>v1.11.0</td>
    </tr>
    <tr>
        <td>Release date</td>
        <td>May 27, 2021</td>
    </tr>
    <tr>
        <th>Component</th>
        <th>Version</th>
    </tr>
    <tr>
        <td>Antrea</td>
        <td>v0.11.3</td>
    </tr>
    <tr>
        <td>cAdvisor</td>
        <td>v0.36.0</td>
    </tr> 
        <td>CSI Driver for vSphere</td>
        <td>v2.2.0</td>
    </tr>
    <tr>
        <td>CoreDNS</td>
        <td>v1.7.0+vmware.8</td>
    </tr>
    <tr>
        <td>Docker</td>
        <td>
            Linux: v19.03.14<br>
            Windows: v19.03.14
        </td>
    </tr>
    <tr>
        <td>etcd</td>
        <td>v3.4.13</td>
    </tr>
    <tr>
        <td>Harbor</td>
        <td>v2.2.1</td>
    </tr>
    <tr>
        <td>Kubernetes</td>
        <td>v1.20.5</td>
    </tr>
    <tr>
        <td>Metrics Server</td>
        <td>v0.3.6</td>
    </tr>
    <tr>
        <td>NCP</td>
        <td>v3.1.2</td>
    </tr>
    <tr>
        <td>Percona XtraDB Cluster (PXC)</td>
        <td>v0.33.0</td>
    </tr>
    <tr>
        <td>UAA</td>
        <td>v74.5.22</td>
    </tr>
    <tr>
        <td>Velero</td>
        <td><a href="https://my.vmware.com/group/vmware/searchresults?client=my_download&site=my_download&proxystylesheet=my_download_en&gsa_lang=en&c=ALL&q=Velero">v1.6.0</a></td>
    </tr>
    <tr>
        <td>VMware Cloud Foundation (VCF)</td>
        <td>v4.3 L1</td>
    </tr>
    <tr>
        <td>Wavefront</td>
        <td>Wavefront Collector: v1.3.4<br>Wavefront Proxy: v9.7</td>
    </tr>
<tr>
        <th>Compatibilities</th>
        <th>Versions</th>
    </tr>
    <tr>
        <td>Ops Manager</td>
        <td>v2.10.11 or later</td>
    </tr>
    <tr>
        <td>NSX-T</td>
        <td rowspan=2>See <a href="https://interopmatrix.vmware.com/#/Interoperability?isHideGenSupported=true&isHideTechSupported=true&isHideCompatible=false&isHideIncompatible=false&isHideNTCompatible=false&isHideNotSupported=true&isCollection=false&col=644&row=0,">VMware Product Interoperability Matrices.</a></td></td>
    </tr>
    <tr>
        <td>vSphere</td>
    </tr>
    <tr>
        <td>Windows stemcells</td>
        <td>v2019.34 or later</td>
    </tr>
    <tr>
        <td>Xenial stemcells</td>
        <td>See <a href="https://network.pivotal.io/products/pivotal-container-service/#/releases/888235"><%= vars.product_network %>.</a></td>
    </tr>
</table>

### <a id='1-11-0-upgrade'></a>Upgrade Path

The supported upgrade paths to <%= vars.k8s_runtime_abbr %> v1.11.0 are from <%= vars.product_short %> v1.10.0 and later.

### <a id="1-11-0-features"></a>Features

This section describes new features and changes in <%= vars.product_short %> v1.11.0.  

* **[Bug Fix]** Fixes [Workloads Using Dynamic PVs Must be Removed Before Deleting a Cluster](https://docs.pivotal.io/tkgi/1-10/release-notes.html#1-10-1-drain-dynamic-pvs).  
* **[Bug Fix]** Fixes [Certain Linux Nodes Are Unable to Complete the Drain Process during a TKGI Upgrade](https://docs.pivotal.io/tkgi/1-10/release-notes.html#bosh-stop-doesnt-drain).  
* **[Bug Fix]** Fixes [Windows Pods Are Unable to Complete the Drain Process during a TKGI Upgrade or Update](https://docs.pivotal.io/tkgi/1-10/release-notes.html#1-10-0-windows-drain-error).  
* **[Bug Fix]** Fixes [Network Profiles Does Not Support the `failover_mode` Parameter](https://docs.pivotal.io/tkgi/1-10/release-notes.html#1-10-0-failover-mode).  
* **[Bug Fix]** Fixes [<%= vars.k8s_runtime_abbr %> CLI `get-credentials` Returns an “od-broker" Error](https://docs.pivotal.io/tkgi/1-10/release-notes.html#1-10-0-od-broker).  
* **[Bug Fix]** Fixes [Your Cluster Returns the Error 'PodCIDR is Empty for Node'](https://docs.pivotal.io/tkgi/1-10/release-notes.html#1-10-0-stale-ip-problem).  
* **[Bug Fix]** Fixes [The TKGI CLI Resize and Update Cluster Commands Remove the Network Profile CNI Configuration from a Cluster]
(https://docs.pivotal.io/tkgi/1-10/release-notes.html#1-10-0-do-notuse-resize-cluster).  
* **[Bug Fix]** Fixes [The TKGI API Does Not Import the Current TKGI CA Certificates](https://docs.pivotal.io/tkgi/1-10/release-notes.html#1-10-0-import-single-cert).  
<%#
* **[Bug Fix]** Fixes [Database Cluster Stops After a DB Instance Is Stopped](https://docs.pivotal.io/tkgi/1-10/release-notes.html#1-10-0-db-down-after-vm-stopped).  
>#%>
* **[Enhancement]** Supports uppercase characters in <%= vars.k8s_runtime_abbr %> API command lines.
For more information, see [FQDNs in <%= vars.k8s_runtime_abbr %> API Commands Cannot Contain Uppercase Letters](https://docs.pivotal.io/tkgi/1-10/release-notes.html#1-10-0-uppercase-bosh-dns).  
* **[Enhancement]** Supports NSX-T v3.1 environments where the Transport Zone and Edge Transport Node switch names are not identical. 
For more information, see [On NSX-T v3.1 the Transport Zone and Edge Transport Node Switch Names Must be Identical](https://docs.pivotal.io/tkgi/1-10/release-notes.html#1-10-0-node-names-match).  
* **[Enhancement]** Returns an error if a network profile configuration changes a parameter that does not support 
modification.
* **[Enhancement]** Supports integration with Tanzu Mission Control. For more information about Tanzu Mission Control, see the [Tanzu Mission control](https://docs.vmware.com/en/VMware-Tanzu-Mission-Control/) documentation.

#### <a id="1-11-0-automatic-cns"></a>Support for Per-Cluster CA and Custom CA

<%= vars.k8s_runtime_abbr %> v1.11 updates the security of the platform by removing the shared cluster CA and introducing a per-cluster CA. By default a new or updated cluster is given its own unique CA that is used to sign the TLS certificates specific to that cluster. No action is needed to take advantage of the per-cluster CA; it is injected automatically for any Kubernetes cluster provisioned by <%= vars.k8s_runtime_abbr %> v1.11 and later. For more information, see [TKGI Certificates](certificate-concepts.html).

A byproduct of the introduction of the per-cluster CA is that you can now use a custom CA when you create a cluster. While this is an advanced use case, it may be appropriate for some customers with specific security requirements. See [Use a Custom CA for Kubernetes Clusters](custom-ca.html) and [Creating Clusters](create-cluster.html) for more information.

#### <a id="1-11-0-automatic-cns"></a>Automatic vSphere CSI Driver Integration

<%= vars.k8s_runtime_abbr %> v1.11.0 supports automatic vSphere CSI Driver installation on vSphere. 
Automatic vSphere CSI Driver integration provides CNS as a process, allowing CNS volumes to be deployed to clusters without requiring access to IaaS credentials. 
For more information, see [Deploying Cloud Native Storage (CNS) on vSphere](vsphere-cns.html).  


#### <a id="1-11-0-add-dns"></a>Add DNS Server IPs to Clusters Using Network Profiles 
Supports using Network Profiles to add additional DNS server IPs to existing clusters. 
For information about using a Network Profile to update a cluster with additional DNS server IPs, see [Confirm the Network Profile Property Supports Updates](network-profiles-define.html#supports-updates) 
in _Creating and Deleting Network Profiles (NSX-T Only)_.  

#### <a id="1-11-0-host-port"></a>Kubernetes Host Port Support

Supports the Kubernetes Host Port feature. 
The Kubernetes Host Port feature allows you to expose an application to be externally accessible through a single port. 
Host Port support requires a Host Port-compatible CNI. Host Port support does not require privileged mode. 
For more information, see [Creating and Deleting Network Profiles (NSX-T Only)](network-profiles-define.html).  

#### <a id="1-11-0-node-local-cache"></a>Supports Kubernetes NodeLocal DNSCache

Supports enabling the NCP 3.1 Kubernetes NodeLocal DNSCache feature. 
NodeLocal DNSCache improves cluster DNS performance by running a DNS caching agent on cluster nodes. The agent runs on the nodes as a DaemonSet. 
For more information, see [Creating and Deleting Network Profiles (NSX-T Only)](network-profiles-define.html).  

#### <a id="1-11-0-coredns-highly-available"></a>CoreDNS is Highly Available

CoreDNS in <%= vars.k8s_runtime_abbr %> v1.11.0 and later is highly available. 
CoreDNS functions in multi-node Pods are automatically distributed across Pods across your designated AZs. 
CoreDNS in an existing <%= vars.k8s_runtime_abbr %> multi-node Pod is automatically reconfigured for high availability when the cluster is upgraded 
to <%= vars.k8s_runtime_abbr %> v1.11.   

#### <a id="1-11-0-vm-extensions"></a>Support for BOSH VM&nbsp;Extensions

<%= vars.k8s_runtime_abbr %> v1.11.0 supports configuring new and existing Kubernetes clusters using BOSH VM&nbsp;Extensions. 
You can modify <%= vars.k8s_runtime_abbr %>-provisioned Kubernetes clusters using BOSH VM Extensions on AWS, Azure and vSphere. 
For more information, see [Using BOSH VM&nbsp;Extensions](bosh-vm-extensions.html).  

#### <a id="1-11-0-bitfusion-interoperability"></a>VMware vSphere Bitfusion Interoperability

VMware vSphere Bitfusion supports <%= vars.k8s_runtime_abbr %> v1.11.0.

Bitfusion decouples physical resources from servers within an environment. The Bitfusion platform
can share graphical processing units (GPUs) in a virtualized infrastructure, as a pool of
network-accessible resources, rather than isolated resources per server. Bitfusion works across
AI frameworks, clouds, networks, and in environments such as virtual machines and containers.

For more information about Bitfusion, see the
[VMware vSphere Bitfusion Documentation](https://docs.vmware.com/en/VMware-vSphere-Bitfusion/index.html).

#### <a id="1-11-0-component-updates"></a>Component Updates

The following components have been updated:

* Bumps Kubernetes to v1.20.5.  
* Bumps Antrea to v0.11.3.  
* Bumps cAdvisor to v0.36.0.  
* Bumps NCP to v3.1.2.  
* Bumps vSphere CSI Driver to v2.2.  
* Bumps Velero to v1.6.0.  
* Bumps Wavefront collector to v1.3.4.  
* Bumps Wavefront proxy to v9.7.  

### <a id="1-11-0-breaking-changes"></a> Breaking Changes

<%= vars.k8s_runtime_abbr %> v1.11.0 has the following breaking changes.

* The built-in Clair container image scanner is deprecated in favor of Trivy. If you have enabled Clair, do one of the following before upgrading to Harbor tile v2.2.1:
    * In the Harbor tile, select the Trivy scanner as the default in "Interrogation Service".  
    * Install the Clair scanner outside the Harbor tile VM and configure the Clair scanner as the default scanner in "Interrogation Service". For more information about installing Clair, see
[Getting Started With Clair](https://github.com/quay/clair/blob/main/Documentation/howto/getting_started.md)
in the Clair documentation. For information about upgrading TKGI with Clair enabled, see
[Update Default Scanner](checklist.html#update-default-scanner) in
_Upgrade Preparation Checklist for Tanzu Kubernetes Grid Integrated Edition v1.10_.

#### <a id="1-11-0-upcoming-deprecations"></a> Upcoming Deprecations

The following <%= vars.k8s_runtime_abbr %> features will be deprecated or removed in future versions of <%= vars.k8s_runtime_abbr %>:

* **Flannel Support**: Support for the Flannel Container Networking Interface (CNI) will be
deprecated in <%= vars.k8s_runtime_abbr %> v1.13. VMware does not recommend that you use the
Flannel CNI in <%= vars.k8s_runtime_abbr %> v1.13 or later production environments. 
For more information about Flannel CNI deprecation, see 
[About Upgrading from the Flannel CNI to the Antrea CNI](understanding-upgrades.html#cni) 
in _About Tanzu Kubernetes Grid Integrated Edition Upgrades_.   

* **Docker Support**: kubelet support for Docker images has been deprecated and Docker support will
be completely removed in Kubernetes v1.24. <%= vars.k8s_runtime_abbr %> support for containerd is
scheduled to begin in <%= vars.k8s_runtime_abbr %> v1.12.  

* **VCP Support**: VCP support has been deprecated and support will be completely removed in a
future <%= vars.k8s_runtime_abbr %> version. VMware does not recommend that you use VCP in
<%= vars.k8s_runtime_abbr %> v1.11 or later production environments.

<%#
* **Management API**: Management API will be deprecated and later replaced with Policy API in future <%= vars.k8s_runtime_abbr %
> versions. VMware does not recommend that you use Management API
in future <%= vars.k8s_runtime_abbr %
> versions.  
#%>

###<a id='1-11-0-known-issues'></a>Known Issues

<%= vars.k8s_runtime_abbr %> v1.11.0 has the following known issues: 


#### <a id="1-11-0-nsxt-302-310"></a> Pods Stop After Upgrading From NSX-T v3.0.2 to v3.1.0 on vSphere 7.0 and 7.0.1

**Symptom**

Your TKGI-provisioned Pods stop after upgrading from NSX-T v3.0.2 to NSX-T v3.1.0 on vSphere 7.0 and 7.0.1.

**Explanation**

For information, see [Issue 2603550: Some VMs are vMotioned and lose network connectivity during UA nodes upgrade]
(https://docs.vmware.com/en/VMware-NSX-T-Data-Center/3.1/rn/VMware-NSX-T-Data-Center-311-Release-Notes.html#2603550) in the _VMware NSX-T Data Center 3.1.1 Release Notes_.

**Workaround**

To avoid the loss of network connectivity during UA node upgrade, ensure DRS is set to manual mode during your upgrade from NSX-T v3.0.2 to v3.1.0.

If you upgraded to NSX-T v3.1.0 with DRS in automation mode, run the following on the affected Pods' master VMs to restore Pod connectivity:

```

monit restart ncp

```

For more information on upgrading NSX-T v3.0.2 to NSX-T v3.1.0, see [Upgrade NSX-T Data Center to v3.0 or v3.1](upgrade-nsxt.html#upgrade-nsxt).  

#### <a id="1-11-0-azure-apply-changes"></a> Error: Could Not Execute "Apply-Changes" in Azure Environment

**Symptom**

After clicking **Apply Changes** on the <%= vars.k8s_runtime_abbr %> tile in an Azure environment, you experience
an error '_...could not execute "apply-changes"..._' with either of the following descriptions:

* _{"errors":{"base":["undefined method 'location' for nil:NilClass"]}}_
* _FailedError.new("Resource Groups in region '#{location}' do not support Availability Zones"))_

For example:

```
INFO | 2020-09-21 03:46:49 +0000 | Vessel::Workflows::Installer#run | Install product (apply changes)
2020/09/21 03:47:02 could not execute "apply-changes": installation failed to trigger: request failed: unexpected response from /api/v0/installations:
HTTP/1.1 500 Internal Server Error
Transfer-Encoding: chunked
Cache-Control: no-cache, no-store
Connection: keep-alive
Content-Type: application/json; charset=utf-8
Date: Mon, 21 Sep 2020 17:51:50 GMT
Expires: Fri, 01 Jan 1990 00:00:00 GMT
Pragma: no-cache
Referrer-Policy: strict-origin-when-cross-origin
Server: Ops Manager
Strict-Transport-Security: max-age=31536000; includeSubDomains
X-Content-Type-Options: nosniff
X-Download-Options: noopen
X-Frame-Options: SAMEORIGIN
X-Permitted-Cross-Domain-Policies: none
X-Request-Id: f5fc99c1-21a7-45c3-7f39
X-Runtime: 9.905591
X-Xss-Protection: 1; mode=block

44
{"errors":{"base":["undefined method `location' for nil:NilClass"]}}
0
```

**Explanation**

The Azure CPI endpoint used by Ops Manager has been changed and
your installed version of Ops Manager is not compatible with the new endpoint.

**Workaround**

Run the following Ops Manager CLI command:

```
om --skip-ssl-validation --username USERNAME --password PASSWORD --target https://OPSMAN-API curl --silent --path /api/v0/staged/director/verifiers/install_time/IaasConfigurationVerifier -x PUT -d '{ "enabled": false }'
```

Where:

* `USERNAME` is the account to use to run Ops Manager API commands.
* `PASSWORD` is the password for the account.
* `OPSMAN-API` is the IP address for the Ops Manager API


For more information, see [Error 'undefined method location' is received when running Apply Change on Azure]
(https://community.pivotal.io/s/article/undefined-method-location-when-running-Apply-Change-on-Azure?language=en_US)
in the VMware Tanzu Knowledge Base.

#### <a id="1-11-0-vrops-windows-clusters"></a> VMware vRealize Operations Does Not Support Windows Worker-Based Kubernetes Clusters

VMware vRealize Operations (vROPs) does not support Windows worker-based Kubernetes clusters and cannot be used to
manage <%= vars.k8s_runtime_abbr %>-provisioned Windows workers.


#### <a id='1-11-0-wavefront-no-win'></a><%= vars.k8s_runtime_abbr %> Wavefront Requires Manual Installation for Windows Workers

To monitor Windows-based worker node clusters with a Wavefront collector and proxy, you must first install Wavefront on the clusters manually, using Helm.
For instructions, see the [Wavefront](windows-monitoring.html#wavefront) section of the _Monitoring Windows Worker Clusters and Nodes_ topic.

#### <a id='1-11-0-ping'></a>Pinging Windows Worker Kubernetes Clusters Does Not Work

<%= vars.k8s_runtime_abbr %>-provisioned Windows worker-based Kubernetes clusters inherit a Kubernetes limitation that prevents
outbound ICMP communication from workers.
As a result, pinging Windows workers does not work.

For information about this limitation, see [Limitations > Networking](https://kubernetes.io/docs/setup/production-environment/windows/intro-windows-in-kubernetes/#networking-1)
in the _Windows in Kubernetes_ documentation.

#### <a id="1-11-0-windows-velero-limitations"></a> Velero Does Not Support Backing Up Stateful Windows Workloads

You can use Velero to back up stateless <%= vars.k8s_runtime_abbr %>-provisioned Windows workers only.
You cannot use Velero to back up stateful Windows applications.
For more information, see [Velero on Windows](https://velero.io/docs/v1.4/basic-install/#velero-on-windows) in
_Basic Install_ in the Velero documentation.

#### <a id="1-11-0-tmc-on-gcp"></a>Tanzu Mission Control Integration Not Supported on GCP

<%= vars.k8s_runtime_abbr %> on Google Cloud Platform (GCP) does not support
Tanzu Mission Control (TMC) integration, which is configured in
the **<%= vars.product_tile %>** tile > the **Tanzu Mission Control** pane.

If you intend to run <%= vars.k8s_runtime_abbr %> on GCP,
skip this pane when configuring the <%= vars.product_tile %> tile.

#### <a id='1-11-0-tmc-restic'></a>TMC Data Protection Feature Requires Privileged <%= vars.k8s_runtime_abbr %> Containers
TMC Data Protection feature supports privileged <%= vars.k8s_runtime_abbr %> containers only.
For more information, see [Plans](installing-vsphere.html#plans) in the _Installing TKGI_ topic for your IaaS.

#### <a id="1-11-0-profile-no-win-gmsa"></a>Windows Worker Kubernetes Clusters with Group Managed Service Account Do Not Support Compute Profiles 

Windows worker-based Kubernetes clusters integrated with group Managed Service Account (gMSA) cannot be managed using compute profiles.  

#### <a id="1-11-0-profile-no-win-flannel"></a> Windows Worker Kubernetes Clusters on Flannel Do Not Support Compute Profiles

On vSphere with NSX-T networking you can use compute profiles with both Linux and Windows worker&#8209;based Kubernetes clusters.
On vSphere with Flannel networking, you can apply compute profiles only to Linux clusters.


#### <a id="1-11-0-profile-resize-down"></a>TKGI CLI Does Not Prevent Reducing the Control Plane Node Count

TKGI CLI does not prevent accidentally reducing a cluster's control plane node count using a compute profile.

<p class="note warning"><strong>Warning:</strong>
    Reducing a cluster's control plane node count can destroy the cluster.
    Do not scale out or scale in existing master nodes by reconfiguring the TKGI tile or by using a compute profile.
    Reducing a cluster's number of control plane nodes may remove a master node and cause the cluster to become inactive.
</p>

#### <a id="1-11-0-in-windows-notready-nodes"></a> Windows Cluster Nodes Not Deleted After VM Deleted

**Symptom**

After you delete a VM using the management console of your infrastructure provider, you notice a Windows worker node
that had been on that VM is now in a `notReady` state.

**Solution**

1. To identify the leftover node:

    ```
    kubectl get no -o wide
    ```
1. Locate nodes on the returned list that are in a `notReady` state and have the same IP address as another node in the list.
1. To manually delete a `notReady` node:

    ```
    kubectl delete node NODE-NAME
    ```
    Where `NODE-NAME` is the name of the node in the `notReady` state.

#### <a id="1-11-0-oidc-response-header"></a>502 Bad Gateway After OIDC Login

**Symptom**

You experience a "502 Bad Gateway" error from the NSX load balancer after you log in to OIDC.

**Explanation**

A large response header has exceeded your NSX-T load balancer maximum
response header size. The default maximum response header size is 10,240 characters and should
be resized to 50,000.

**Workaround**

If you experience this issue, manually reconfigure your NSX-T `request_header_size`
and `response_header_size` to 50,000 characters.
For information about configuring NSX-T default header sizes,
see [OIDC Response Header Overflow](https://community.pivotal.io/s/article/OIDC-Response-Header-overflow) in the Knowledge Base.


#### <a id='1-11-0-eight-cores-error'></a> NSX-T Pre-Check Errand Fails Due to Edge Node Configuration

**Symptom**

You have configured your NSX-T Edge Node VM as `medium` size,
and the NSX-T Pre-Check Errand fails with the following error:
"_ERROR: NSX-T Precheck failed due to Edge Node ... no of cpu cores is less than 8_".

**Explanation**

The NSX-T Pre-Check Errand is erroneously returning the "_cpu cores is less than 8_" error.

**Solution**

You can safely configure your NSX-T Edge Node VMs as `medium` size and ignore the error.

#### <a id='1-11-0-windows-proxy'></a> Difficulty Changing Proxy for Windows Workers

You must configure a global proxy in the <%= vars.product_tile %> tile > **Networking** pane before you create any Windows workers that use the proxy.

You cannot change the proxy configuration for Windows workers in an existing cluster.

#### <a id='1-11-0-http-proxy-password'></a> Character Limitations in HTTP Proxy Password

For vSphere with NSX-T, the HTTP Proxy password field does not support the following special characters: `&` or `;`.

#### <a id='1-11-0-harbor-storage-config'></a> Error After Modifying Your Harbor Storage Configuration

**Symptom**

You receive the following error after modifying your existing Harbor installation's storage configuration:

```
Error response from daemon: manifest for ... not found: manifest unknown: manifest unknown
```


**Explanation**

Harbor does not support modifying an existing Harbor installation's storage configuration.

**Workaround**

To modify your Harbor storage configuration,
re-install Harbor. Before starting Harbor, configure the new Harbor installation with the desired configuration.

#### <a id='1-11-0-antrea-logging-interupt'></a> Unexplained Errors After Interrupting a Log Stream When Using Antrea Networking 

**Symptom**

While using Antrea networking, you observe unexplainable errors after you interrupt a log stream started using `kubectl logs -f POD-NAME`. 
The errors could include any of the following:  

* kubectl returns the error: "_Error from server (TooManyRequests): the server has received too many_".   
* `kube-apiserver` returns an http code `429`. 

**Explanation**

When using Antrea networking there is a chance that `konnectivity-agent` will become unstable after interrupting your kubectl log steam.

**Workaround**

To resolve the issue:  

1. Log in to the master VM:

    ```
    bosh -d DEPLOYMENT-NAME ssh master/0
    ````

1. Change to root:

    ```
    sudo -i
    ```

1. Restart `proxy-server`:

    ```
    monit restart proxy-server
    ```

1. Wait for `proxy-server` restart:

    ```
    monit summary
    ```


#### <a id='1-11-0-resizing-worker-nodes'></a> Ingress Controller Statefulset Fails to Start After Resizing Worker Nodes

**Symptom**  

Permissions are removed from your cluster’s files and processes after resizing the persistent disk 
during a cluster upgrade. The ingress controller statefulset fails to start.

**Explanation**  

When resizing a persistent disk, Bosh migrates the data from the old disk to the new disk but 
does not copy the files’ extended attributes.

**Workaround**  

To resolve the problem, complete the steps in 
[Ingress controller statefulset fails to start after resize of worker nodes with permission denied]
(https://community.pivotal.io/s/article/5000e00001nCJxT1603094435795?language=en_US)
in the VMware Tanzu Knowledge Base.

#### <a id="security-group"></a>Azure Default Security Group Is Not Automatically Assigned to Cluster VMs

**Symptom**

You experience issues when configuring a load balancer for a multi-master Kubernetes cluster or creating a service of type `LoadBalancer`.
Additionally, in the Azure portal, the **VM** > **Networking** page does not display
any inbound and outbound traffic rules for your cluster VMs.

**Explanation**

As part of configuring the <%= vars.product_tile %> tile for Azure, you enter **Default Security Group** in the **Kubernetes Cloud Provider** pane.
When you create a Kubernetes cluster, <%= vars.product_short %> automatically assigns this security group to each VM in the cluster.
However, on Azure the automatic assignment may not occur.

As a result, your inbound and outbound traffic rules defined in the security group are not applied to the cluster VMs.

**Workaround**

If you experience this issue, manually assign the default security group to each VM NIC in your cluster.


#### <a id='1-11-0-uuid-length'></a> One Plan ID Longer than Other Plan IDs

**Symptom**

One of your plan IDs is one character longer than your other plan IDs.

**Explanation**

In <%= vars.k8s_runtime_abbr %>, each plan has a unique plan ID.
A plan ID is normally a UUID consisting of 32 alphanumeric characters and 4 hyphens.
However, the **Plan 4** ID consists of 33 alphanumeric characters and 4 hyphens.

**Solution**

You can safely configure and use **Plan 4**.
The length of the **Plan 4** ID does not affect the functionality of **Plan 4** clusters.

If you require all plan IDs to have identical length, do not activate or use **Plan 4**.


#### <a id='1-11-0-whitespace-pksapi'></a> The <%= vars.control_plane %> FQDN Must Not Include Trailing Whitespace

**Symptom**

Your <%= vars.k8s_runtime_abbr %> logs include the following error:

```
'uaa'. Errors are:- Error filling in template 'uaa.yml.erb' (line 59: Client redirect-uri is invalid: uaa.clients.pks_cli.redirect-uri Client redirect-uri is invalid: uaa.clients.pks_cluster_client.redirect-uri)
```

**Explanation**

The <%= vars.control_plane %> fully-qualified domain name (FQDN) for your cluster contains leading or trailing whitespace.  

**Workaround**

Do not include whitespace in the <%= vars.k8s_runtime_abbr %> tile **API Hostname (FQDN)** field.  


#### <a id='1-11-0-db-down-after-vm-stopped'></a> Database Cluster Stops After a DB Instance Is Stopped 

**Symptom**

After you stop one instance in a multiple-instance database cluster, the cluster stops, 
or communication between the remaining databases times out, and the entire cluster becomes unreachable.

The following might be in your UAA log:

```
WSREP has not yet prepared node for application use
```

**Explanation** 

The database cluster is unable to recover automatically because a member is no longer available to reconcile quorum. 

#### <a id='1-11-0-windows-dynamic-pvs'></a> Windows Workloads With Attached Dynamic PVs Must Be Removed before Deleting a Cluster

**Symptom**  

Your `tkgi delete-cluster` operation hangs while draining a Windows worker VM 
containing a Pod bound to one or more dynamic persistent volumes (PVs).  

**Workaround** 

Before deleting your Windows cluster, remove all workloads.  

#### <a id='1-11-0-velero-vsphere-pvs'></a> Velero Back Up Fails for vSphere PVs Attached to Clusters on Kubernetes v1.20 and Later

**Symptom**  

Backing up vSphere persistent volumes using Velero fails and your Velero backup log includes the following error:

```
rpc error: code = Unknown desc = Failed during IsObjectBlocked check: Could not translate selfLink to CRD name
```

**Explanation** 

This is a known issue when backing up clusters on Kubernetes v1.20 and later using the 
Velero Plugin for vSphere v1.1.0 or earlier.  


**Workaround**  

To resolve the problem, complete the steps in 
[Velero backups of vSphere persistent volumes fail on Kubernetes clusters version 1.20 or higher (83314)](https://kb.vmware.com/s/article/83314) 
in the VMware Tanzu Knowledge Base.   


## <a id="management-console-1-11-0"></a> <%= vars.k8s_runtime_abbr %> Management Console v1.11.0

**Release Date**: May 27, 2021

### <a id="management-console-1.11.0-snapshot"></a>Product Snapshot

<p class="note"><strong>Note</strong>: <%= vars.product_short %> Management Console provides an opinionated
    installation of <%= vars.k8s_runtime_abbr %>. The supported versions may differ from or be more limited than
    what is generally supported by <%= vars.k8s_runtime_abbr %>.
</p>

<table class="nice">
    <th>Element</th>
    <th>Details</th>
    <tr>
      <td>Version</td>
      <td>v1.11.0</td>
    </tr>
    <tr>
      <td>Release date</td>
      <td>May 27, 2021</td>
    </tr>
    <tr>
      <td>Installed Tanzu Kubernetes Grid Integrated Edition version</td>
      <td>v1.11.0</td>
    </tr>
    <tr>
      <td>Installed Ops Manager version</td>
      <td>v2.10.11</td>
    </tr>
    <tr>
      <td>Installed Kubernetes version</td>
        <td>v1.20.5</td>
    </tr>
    <tr>
      <td>Installed Harbor Registry version</td>
      <td>v2.2.1</td>
    </tr>
        <tr>
        <td>Linux stemcell</td>
        <td>v621.125</td>
    </tr>
    <tr>
        <td>Windows stemcells</td>
        <td>v2019.34 and later</td>
    </tr>
</table>

<br>
### <a id='management-console-1-11-0-upgrade'></a>Upgrade Path

The supported upgrade path to <%= vars.product_short %> Management Console v1.11.0 is from
<%= vars.product_short %> v1.10.0 and later.

### <a id="management-console-1-11-0-features"></a>Features

<%= vars.product_short %> Management Console v1.11.0 updates include:  

* **[Bug Fix]** Fixes [Management Console UI Does Not Open If the Management Console Uses Custom Certificates](https://docs.pivotal.io/tkgi/1-10/release-notes.html#management-console-1.10.0-ui-customcerts).  
* **[Bug Fix]** Fixes [Management Console Deletes Custom Workload Configurations](https://docs.pivotal.io/tkgi/1-10/release-notes.html#management-console-1-10-0-addons-spec).  
* **[Enhancement]** TKGI Management Console v1.11.0 can retrieve the transport zone of an NSX-T v3.1 edge node transport node. 
For more information, see [On NSX-T v3.1 the Transport Zone and Edge Transport Node Switch Names Must be Identical](https://docs.pivotal.io/tkgi/1-10/release-notes.html#1-10-0-node-names-match).  

### <a id="management-console-1-11-0-known-issues"></a> Known Issues

The <%= vars.product_short %> Management Console v1.11.0 has the following known issues:



#### <a id="management-console-1-11-0-vrli-https"></a> vRealize Log Insight Integration Does Not Support HTTPS Connections

**Symptom**

The <%= vars.product_short %> Management Console integration to vRealize Log Insight does not support connections to the HTTPS port on the vRealize Log Insight server.

**Workaround**

1. Use SSH to log in to the <%= vars.product_short %> Management Console appliance VM.
1. Open the file `/lib/systemd/system/pks-loginsight.service` in a text editor.
1. Add `-e LOG_SERVER_ENABLE_SSL_VERIFY=false`.
1. Set `-e LOG_SERVER_USE_SSL=true`.

    The resulting file should look like the following example:

    ```
    ExecStart=/bin/docker run --privileged --restart=always --network=pks
    -v /var/log/journal:/var/log/journal
    --name=pks-loginsight
    -e TYPE=gear2-vm
    -e LOG_SERVER_HOST=${LOGINSIGHT_HOST}
    -e LOG_SERVER_PORT=${LOGINSIGHT_PORT}
    -e LOG_SERVER_ENABLE_SSL_VERIFY=false
    -e LOG_SERVER_USE_SSL=true
    -e LOG_SERVER_AGENT_ID=${LOGINSIGHT_ID}
    pksoctopus/vrli-journald:v07092019
    ```

1. Save the file and run `systemctl daemon-reload`.
1. To restart the vRealize Log Insight service, run `systemctl restart pks-loginsight.service`.

<%= vars.product_short %> Management Console can now send logs to the HTTPS port on the vRealize Log Insight server.

#### <a id="management-console-1-11-0-vsphere-ha"></a> vSphere HA causes Management Console ovfenv Data Corruption

**Symptom**

If you enable vSphere HA on a cluster, if the TKGI Management Console appliance VM is running on a host in that cluster, and if the host reboots, vSphere HA recreates a new TKGI Management Console appliance VM on another host in the cluster. Due to an issue with vSphere HA, the `ovfenv` data for the newly created appliance VM is corrupted and the new appliance VM does not boot up with the correct network configuration.

**Workaround**

- In the vSphere Client, right-click the appliance VM and select **Power** > **Shut Down Guest OS**.
- Right-click the appliance again and select Edit Settings.
- Select **VM Options** and click **OK**.
- Verify under Recent Tasks that a `Reconfigure virtual machine` task has run on the appliance VM.
- Power on the appliance VM.

#### <a id="management-console-1-11-0-k8s-profile"></a> Base64 encoded file arguments are not decoded in Kubernetes profiles

**Symptom**

Some file arguments in Kubernetes profiles are base64 encoded. When the management console displays the Kubernetes profile,
some file arguments are not decoded.

**Workaround**

Run `echo "$content" | base64 --decode`

#### <a id="management-console-1-11-0-network-profile"></a> Network profiles not immediately selectable

**Symptom**

If you create network profiles and then try to apply them in the Create Cluster page, the new profiles
are not available for selection.

**Workaround**

Log out of the management console and log back in again.

#### <a id="management-console-1-11-0-cluster-summary"></a> Real-Time IP information not displayed for network profiles

**Symptom**

In the cluster summary page, only default IP pool, pod IP block, node IP block values are displayed,
rather than the real-time values from the associated network profile.

**Workaround**

None

#### <a id='management-console-1-11-0-harbor-storage-config'></a> Error After Modifying Your Harbor Storage Configuration

**Symptom**

You receive the following error after modifying your existing Harbor installation's storage configuration:

```
Error response from daemon: manifest for ... not found: manifest unknown: manifest unknown
```


**Explanation**

Harbor does not support modifying an existing Harbor installation's storage configuration.

**Workaround**

To modify your Harbor storage configuration,
re-install Harbor. Before starting Harbor, configure the new Harbor installation with the desired configuration.


#### <a id='management-console-1-11-0-re-import-windows-stemcell'></a> Windows Stemcells Must be Re-Imported After Upgrading Ops Manager

**Symptom**

After upgrading Ops Manager, your Management Console does not recognize a Windows stemcell imported when using the prior version of Ops Manager.

**Workaround**

If your Management Console does not recognize a Windows stemcell after upgrading Ops Manager:

1. Re-import your previously imported Windows stemcell.  
2. **Apply Changes** to <%= vars.k8s_runtime_abbr %> MC.  

#### <a id='management-console-1-11-0-cluster-group-name'></a> Your New Clusters Are Not Shown In Tanzu Mission Control

**Symptom**

After you create a cluster, Tanzu Mission Control does not include the cluster in cluster lists. 
You have a "_Resource not found_" error similar to the following in your BOSH logs:

```
Cluster Name in TMC: cluster-1
Cluster Name Prefix: tkgi-my-prefix-
Group Name in TMC: my-prefix-clusters
Cluster Description in TMC: VMware Enterprise PKS Attaching cluster ''tkgi-my-prefix-cluster-1'' to TMC 
Fetching token successful 
request POST:/v1alpha1/clusters, 
response 404 Not Found:{"error":"Resource not found - clustergroup(my-prefix-clusters) 
org id(d859dc9f-g622-426d-8c91-939a9f13dea9)",
"code":5,"message":"Resource not found - clustergroup(my-prefix-clusters)
```

**Explanation**

The cluster group you assign a cluster to must be defined in Tanzu Mission Control 
before you assign your cluster to the cluster group in the <%= vars.k8s_runtime_abbr %> Management Console.

**Workaround**  

To resolve the problem, complete the steps in 
[Attaching a Tanzu Kubernetes Grid Integrated (TKGI) cluster to Tanzu Mission Control (TMC) fails with "Resource not found - clustergroup(cluster-group-name)"](https://community.pivotal.io/s/article/tkgi-tmc-attach-fail) 
in the VMware Tanzu Knowledge Base.   
