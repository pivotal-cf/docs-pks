---
title: Release Notes
owner: TKGI
topictype: releasenotes
---

This topic contains release notes for <%= vars.product_short %> (<%= vars.k8s_runtime_abbr %>) v1.9.

## <a id="1.9.0"></a><%= vars.k8s_runtime_abbr %> v1.9.0

**Release Date**: September 29, 2020

### <a id='1-9-0-snapshot'></a><a id='product-snapshot'></a>Product Snapshot

<table class="nice">
    <tr>
        <th>Release</th>
        <th>Details</th>
    </tr>
    <tr>
        <td>Version</td>
        <td>v1.9.0</td>
    </tr>
    <tr>
        <td>Release date</td>
        <td>September 29, 2020</td>
    </tr>
    <tr>
        <th>Component</th>
        <th>Version</th>
    </tr>
    <tr>
        <td>Kubernetes</td>
        <td>v1.18.8</td>
    </tr>
    <tr>
        <td>CoreDNS</td>
        <td>v1.6.7+vmware.3</td>
    </tr>
    <tr>
        <td>Docker</td>
        <td>
            Linux: v19.03.5<br>
            Windows: v19.03.11
        </td>
    </tr>    
    <tr>
        <td>etcd</td>
        <td>v3.4.3</td>
    </tr>
    <tr>
        <td>Metrics Server</td>
        <td>v0.3.6</td>
    </tr>
    <tr>
        <td>NCP</td>
        <td>v3.0.2.1</td>
    </tr>
    <tr>
        <td>Percona XtraDB Cluster (PXC)</td>
        <td>v0.28.0</td>
    </tr>
    <tr>
        <td>UAA</td>
        <td>v74.5.18</td>
    </tr>
    <tr>
        <th>Compatibilities</th>
        <th>Versions</th>
    </tr>
    <tr>
        <td>Ops Manager</td>
        <td>Ops Manager v2.9.9 or later, or v2.10.1 or later.<br>
            Windows worker support on vSphere with NSX-T requires Ops Manager v2.10.1 or later
        </td>
    </tr>
    <tr>
        <td>Xenial stemcells<sup>&#42;</sup></td>
        <td>See <a href="https://network.pivotal.io/products/pivotal-container-service#/releases/748622"><%= vars.product_network %></a></td>
    </tr>     
    <tr>
        <td>Windows stemcells</td>
        <td>v2019.24+</td>
    </tr>
    <tr>
        <td>Backup and Restore SDK</td>
        <td>v1.18.0</td>
    </tr>   
    <tr>
        <td>vSphere</td>
        <td>v7.0, v6.7, v6.5</td>
    </tr>
    <tr>
        <td>VMware Cloud Foundation (VCF)</td>
        <td>v4.1, v4.0</td>
    </tr>
    <tr>
        <td>CNS for vSphere</td>
        <td>v1.0.2, v2.0</td>
    </tr>    
    <tr>
        <td>NSX-T</td>
        <td>v3.0.2, v2.5.2, v2.5.0</td>
    </tr>
    <tr>
        <td>Harbor</td>
        <td>v2.1, v2.0.1, v1.10.3</td>
    </tr>
    <tr>
        <td>Velero</td>
        <td>v1.4.2 and later</td>
    </tr>
</table>
<sup>&#42;</sup> See [Kubernetes Clusters With Xenial Stemcell v621.85 and Later Fail 
After Upgrading <%= vars.k8s_runtime_abbr %> on vSphere With NSX-T 
to <%= vars.k8s_runtime_abbr %> v1.9](#1-9-0-stemcell-compat-2).  

### <a id='1-9-0-upgrade'></a>Upgrade Path

The supported upgrade paths to <%= vars.k8s_runtime_abbr %> v1.9.0 are from <%= vars.product_short %> v1.8.0 and later.

### <a id="1-9-0-features"></a>Features

This section describes new features and changes in <%= vars.product_full %> v1.9.0.

#### <a id="1-9-0-windows"></a>Windows Workers on NSX-T

<%= vars.k8s_runtime_abbr %> v1.9.0 supports clusters with Windows-based worker nodes on vSphere with NSX-T networking only.
<%= vars.k8s_runtime_abbr %> v1.9.0 continues to support Windows workloads on vSphere with Flannel networking as a beta feature.
 
#### <a id="1-9-0-certs"></a>Cluster Certificate Rotation Support

For secure communication, <%= vars.k8s_runtime_abbr %> clusters use TLS certificates created unique for each cluster.
<%= vars.k8s_runtime_abbr %> v1.9.0 integrates with the [CredHub Maestro](https://docs.pivotal.io/platform/security/pcf-infrastructure/getting-started-with-maestro-cli.html) CLI to enable expiry checks and rotation for these cluster-specific certificates, including additional certificates that clusters use with NSX-T networking.

See [Rotating Cluster Certificates](./rotate-cluster-certificates.html) for how to check and rotate cluster-specific certificates,
and [<%= vars.k8s_runtime_abbr %> Certificates](./certificate-concepts.html) for managing all certificates used by <%= vars.k8s_runtime_abbr %>.

#### <a id="1-9-0-certs-local-bundle"></a>PKS CLI Supports Certificates Trusted by the Local System

The PKS CLI now trusts the certificates in a system CA store, such as the MacOS keychain. 
When logging in to the PKS CLI, you no longer need to specify the `--skip-ssl-validation` or `--ca-cert` command line arguments.  

#### <a id="1-9-0-compute-profiles"></a>Compute Profile CLI Support and Improvements (vSphere)

Compute profiles let developers customize cluster topology, node sizing, and other compute resource options, overriding configurations that a cluster inherits from its Plan.

<%= vars.k8s_runtime_abbr %> v1.9.0 redesigns and improves compute profile functionality, and adds TKG CLI options for creating, managing, and using compute profiles.

With NSX-T networking you can use compute profiles with Linux- and Windows-worker clusters.
With Flannel networking you can only apply compute profiles to Linux clusters.

For more information, see [Creating and Managing Compute Profiles with the CLI (vSphere)](./compute-profiles-manage.html).

#### <a id="1-9-0-velero"></a>Velero Support and Bundling for Backup and Restore

<%= vars.k8s_runtime_abbr %> v1.9.0 includes support for [Velero](https://velero.io/docs), an open source community standard tool for backing up and restoring Kubernetes workloads, including stateless and stateful using persistent volumes.
Velero is the preferred backup solution for workloads running on <%= vars.k8s_runtime_abbr %> clusters, 
and is downloadable from your <%= vars.k8s_runtime_abbr %> downloads page on https://my.vmware.com.  

For more information, see [Backing Up and Restoring <%= vars.product_short %>](backup-and-restore.html).  

#### <a id="1-9-0-resizable-pv"></a>Resizable Persistent Volume Support on vSphere 7.0 

<%= vars.k8s_runtime_abbr %> supports creating resizable persistent volumes on clusters created on 
vSphere 7.0 with CNS v2.0.

For more information, see [Cloud Native Storage (CNS) on vSphere](./vsphere-cns.html).

#### <a id="1-9-0-tagging"></a>Tagging Support on AWS and GCP 

<%= vars.k8s_runtime_abbr %> supports tagging from the CLI on Amazon Web Services (AWS) and Google Cloud Platform (GCP).
This enables the `--tags` option to `tkgi create-cluster` and other commands on all infrastructures.

For more information, see [Tagging Clusters](./tag-clusters.html).

#### <a id="1-9-0-telegraf"></a>New Telegraf Configuration Fields

<%= vars.k8s_runtime_abbr %> supports modifying the Telegraf Agent configuration. 
For more information, see [Configure Telegraf in the Tile](monitor-etcd.html#connect) 
in _Configuring Telegraf in <%= vars.k8s_runtime_abbr %>_.  

#### <a id="1-9-0-telemetry"></a>Telemetry Changes

* **Telemetry Enhanced Participation Level Change**: 
The <%= vars.k8s_runtime_abbr %> Customer Experience Improvement Program (CEIP) and Telemetry Program has been streamlined to bring the Enhanced participation level closer to the Standard participation level.
For descriptions of the participation levels, see [Telemetry](./telemetry.html).

* **Telemetry Database Removal**: 
The legacy Telemetry DB has been removed from the <%= vars.control_plane_db %>. 

#### <a id="1-9-0-component-updates"></a>Component Updates

The following components have been updated:

* Bumps Kubernetes to v1.18.8+vmware.1.
* Bumps CoreDNS to v1.6.7+vmware.3.
* Bumps NCP to v3.0.2.1.

###<a id='1-9-0-known-issues'></a>Known Issues

<%= vars.k8s_runtime_abbr %> v1.9.0 has the following known issues:

#### <a id='1-9-0-stemcell-compat-2'></a> Kubernetes Clusters With Xenial Stemcell v621.85 and Later Fail After Upgrading <%= vars.k8s_runtime_abbr %> on vSphere With NSX-T to <%= vars.k8s_runtime_abbr %> v1.9

**Symptom** 

After you upgrade to <%= vars.k8s_runtime_abbr %> on vSphere with NSX-T v1.9.0 and later, 
<%= vars.k8s_runtime_abbr %>-provisioned Kubernetes clusters with Xenial Stemcell v621.85 and later fail to start. 
The cluster start failure log includes the following:

```
Error: Action Failed get_task: Task ... result: Compiling package openvswitch: 
Running packaging script: Running packaging script: Command exited with 2;
```

Xenial Stemcell v621.85 is installed by default when installing Ops Manager v2.8.15.  

**Workaround**  

If you experience this issue, manually revert the stemcell to an earlier compatible version. 
For information about reverting stemcells, 
see [How to revert a stemcell in TKGI to prevent OVS compilation issues]
(https://pvtl.force.com/s/article/How-to-revert-stemcell-from-621-76-to-621-75-for-TKGI-1-8-0-to-prevent-ovs-compilation-issues?language=en_US) 
in the Knowledge Base. 

#### <a id="1.9.0-azure-apply-changes"></a> Error: Could Not Execute "Apply-Changes" in Azure Environment

**Symptom**  

After clicking **Apply Changes** on the <%= vars.k8s_runtime_abbr %> tile in an Azure environment, you experience 
an error '_...could not execute "apply-changes"..._' with either of the following descriptions:

* _{"errors":{"base":["undefined method 'location' for nil:NilClass"]}}_  
* _FailedError.new("Resource Groups in region '#{location}' do not support Availability Zones"))_

For example:  

```
INFO | 2020-09-21 03:46:49 +0000 | Vessel::Workflows::Installer#run | Install product (apply changes)
2020/09/21 03:47:02 could not execute "apply-changes": installation failed to trigger: request failed: unexpected response from /api/v0/installations:
HTTP/1.1 500 Internal Server Error
Transfer-Encoding: chunked
Cache-Control: no-cache, no-store
Connection: keep-alive
Content-Type: application/json; charset=utf-8
Date: Mon, 21 Sep 2020 17:51:50 GMT
Expires: Fri, 01 Jan 1990 00:00:00 GMT
Pragma: no-cache
Referrer-Policy: strict-origin-when-cross-origin
Server: Ops Manager
Strict-Transport-Security: max-age=31536000; includeSubDomains
X-Content-Type-Options: nosniff
X-Download-Options: noopen
X-Frame-Options: SAMEORIGIN
X-Permitted-Cross-Domain-Policies: none
X-Request-Id: f5fc99c1-21a7-45c3-7f39
X-Runtime: 9.905591
X-Xss-Protection: 1; mode=block

44
{"errors":{"base":["undefined method `location' for nil:NilClass"]}}
0
```


**Explanation**  

The Azure CPI endpoint used by Ops Manager has been changed and 
your installed version of Ops Manager is not compatible with the new endpoint.

**Workaround**  

Run the following Ops Manager CLI command:

```
om --skip-ssl-validation --username USERNAME --password PASSWORD --target https://OPSMAN-API curl --silent --path /api/v0/staged/director/verifiers/install_time/IaasConfigurationVerifier -x PUT -d '{ "enabled": false }'
```

Where:

* `USERNAME` is the account to use to run Ops Manager API commands.  
* `PASSWORD` is the password for the account.  
* `OPSMAN-API` is the IP address for the Ops Manager API


For more information, see [Error 'undefined method location' is received when running Apply Change on Azure]
(https://community.pivotal.io/s/article/undefined-method-location-when-running-Apply-Change-on-Azure?language=en_US) 
in the VMware Tanzu Knowledge Base.

#### <a id="1.9.0-vrops-windows-clusters"></a> VMware vRealize Operations Does Not Support Windows Worker-Based Kubernetes Clusters

VMware vRealize Operations (vROPs) does not support Windows worker-based Kubernetes clusters and cannot be used to 
manage <%= vars.k8s_runtime_abbr %>-provisioned Windows workers.  


#### <a id='1-9-0-wavefront-no-win'></a><%= vars.k8s_runtime_abbr %> Wavefront Does Not Work for Windows Workers

The Wavefront collector and proxy do not support monitoring of clusters with Windows-based worker nodes.
For alternative ways to set up in-cluster monitoring, see [Monitoring Workers and Workloads](./in-cluster-monitoring.html).

#### <a id='1-9-0-ping'></a>Pinging Windows Workers Does Not Work

<%= vars.k8s_runtime_abbr %>-provisioned Windows workers inherit a Kubernetes limitation that prevents
outbound ICMP communication from workers.
As a result, pinging Windows workers does not work.

For information about this limitation, see [Limitations > Networking](https://kubernetes.io/docs/setup/production-environment/windows/intro-windows-in-kubernetes/#networking-1)
in the _Windows in Kubernetes_ documentation.

#### <a id="1.9.0-windows-velero-limitations"></a> Velero Does Not Support Backing Up Stateful Windows Workloads

You can use Velero to backup stateless <%= vars.k8s_runtime_abbr %>-provisioned Windows workers. 
Velero can back up stateless Windows workloads only, and cannot be used to backup stateful Windows applications. 
For more information, see [Velero on Windows](https://velero.io/docs/v1.4/basic-install/#velero-on-windows) in 
_Basic Install_ in the Velero documentation.

#### <a id="1-9-0-tmc-on-gcp"></a>TMC Integration Not Supported on GCP

<%= vars.k8s_runtime_abbr %> on Google Cloud Platform (GCP) does not support
Tanzu Mission Control integration, which is configured in
the **<%= vars.product_tile %>** tile > the **Tanzu Mission Control (Experimental)** pane.

If you intend to run <%= vars.k8s_runtime_abbr %> v1.9 on GCP,
skip this pane when configuring the <%= vars.product_tile %> tile.

#### <a id="1-9-0-old-profile-no-cli"></a>Compute Profile CLI Commands Do Not Support Pre-v1.9 Profiles

Compute profiles created in <%= vars.k8s_runtime_abbr %> v1.8 and earlier have a different format from v1.9 compute profiles, as described in [Compute Profile CLI Support and Improvements (vSphere)](#1-9-0-compute-profiles).

You cannot use the v1.9 `tkgi` CLI compute profile commands to manage compute profiles created in prior versions of <%= vars.k8s_runtime_abbr %>.
To perform compute profile operations on pre-v1.9 profiles and the clusters that use them, use the `curl` commands and JSON structure described in [Using Compute Profiles](https://docs.pivotal.io/tkgi/1-8/compute-profiles.html) in the <%= vars.k8s_runtime_abbr %> v1.8 documentation.

Compute profiles created in prior versions of <%= vars.k8s_runtime_abbr %>, and the clusters that use them, continue to work in v1.9.

#### <a id="1-9-0-profile-bosh-upgrade"></a>Compute Profile Dropped From Clusters During Upgrade from <%= vars.k8s_runtime_abbr %> v1.8

If a <%= vars.k8s_runtime_abbr %> cluster configured with a pre-v1.9 compute profile is upgraded to v1.9 using `tkgi upgrade-cluster`, 
the clusterâ€™s compute profile will be dropped.

#### <a id="1-9-0-profile-1-9-upgrade"></a>You Cannot Upgrade Clusters Configured with a <%= vars.k8s_runtime_abbr %> v1.9 Compute Profile

`tkgi upgrade-cluster` does not upgrade clusters configured with a <%= vars.k8s_runtime_abbr %> v1.9 compute profile.  

<p class="note warning">
    <strong>Warning:</strong> VMware recommends configuring only non-production <%= vars.k8s_runtime_abbr %> clusters with v1.9 compute profiles.
</p> 

#### <a id="1-9-0-profile-no-win-flannel"></a>Compute Profiles Not Supported with for Windows-Worker Clusters on Flannel

On vSphere with Flannel networking, you can only apply compute profiles to Linux clusters.
On vSphere with NSX-T networking you can use compute profiles with both Linux- and Windows-worker clusters.

#### <a id="1-9-0-profile-resize-down"></a>TKGI CLI Does Not Prevent Reducing the Control Plane Node Count

TKGI CLI does not prevent accidentally reducing a cluster's control plane node count using a compute profile.  
 
<p class="note warning"><strong>Warning:</strong> 
    Reducing a cluster's control plane node count can destroy the cluster. 
    Do not attempt to use a compute profile to reduce a cluster's number of control plane nodes, <code>control_plane.instances</code>.
</p>

#### <a id="1-9-0-emptydir-volume"></a>Windows Nodes With Workloads and an emptyDir Volume are Unable to Drain 

Node draining will fail when scaling down a Windows cluster with a deployed Windows workload and an emptyDir volume.  

**Solution**

Run `kubectl drain NODENAME --delete-local-data`, then restart the cluster scale down.  

#### <a id="1-9-0-in-cluster-dns-lookup"></a>In-Cluster DNS Lookup Fails in Windows Clusters 

**Symptom**  

DNS lookup fails for Windows Pods that do not use a fully qualified domain name (FQDN) 
to lookup services and Pods within its namespace or cluster.  

**Explanation**  

DNS lookup fails because a Primary DNS suffix has not been configured on the Windows Pod. 
A Windows Pod configured without a Primary DNS suffix must use a fully qualified domain name (FQDN) 
to lookup addresses within its namespace and cluster.  

**Solution**

To configure a Primary DNS suffix for your Windows Pods, 
use a hook to dynamically inject a Primary DNS setting into your Windows Pods. 
For more information see [In-Cluster DNS lookup requires a Fully Qualified Domain Name (FQDN) in Windows Clusters]
(https://community.pivotal.io/s/article/In-Cluster-DNS-Lookup-Requires-a-Fully-Qualified-Domain-Name-in-Windows-Clusters) 
in the VMware Tanzu Community Knowledge Base.




#### <a id="1-9-0-in-windows-cluster-compute"></a> Windows Cluster Creation Fails for Certain Compute Profile Configurations

Windows cluster creation does not support using a compute profile with two or more worker instance groups.

#### <a id="1-9-0-in-windows-notready-nodes"></a> Windows Cluster Nodes Not Deleted After VM Deleted

**Symptom**  

After you delete a VM using your IAAS' management console you notice a Windows worker node 
that had been on that VM is now in a `notReady` state.   

**Solution**

1. To identify the leftover node:  

    ```
    kubectl get no -o wide
    ```
1. Locate nodes on the returned list that are in a `notReady` state and have the same IP address as another node in the list.  
1. To manually delete a `notReady` node:  

    ```
    kubectl delete node NODE-NAME
    ```
    Where `NODE-NAME` is the name of the node in the `notReady` state.  

#### <a id="1-9-0-oidc-response-header"></a>502 Bad Gateway After OIDC Login  

**Symptom**  

You experience a "502 Bad Gateway" error from the NSX load balancer after you log in to OIDC.  

**Explanation**  

A large response header has exceeded your NSX-T load balancer maximum 
response header size. The default maximum response header size is 10,240 characters and should 
be resized to 50,000.  

**Workaround**  

If you experience this issue, manually reconfigure your NSX-T `request_header_size` 
and `response_header_size` to 50,000 characters. 
For information about configuring NSX-T default header sizes, 
see [OIDC Response Header Overflow](https://community.pivotal.io/s/article/OIDC-Response-Header-overflow) in the Knowledge Base.  


#### <a id='1-9-0-eight-cores-error'></a> NSX-T Pre-Check Errand Fails Due to Edge Node Configuration  

**Symptom**  

You have configured your NSX-T Edge Node VM as `medium` size, 
and the NSX-T Pre-Check Errand fails with the following error: 
"_ERROR: NSX-T Precheck failed due to Edge Node ... no of cpu cores is less than 8_". 

**Explanation**  

The NSX-T Pre-Check Errand is erroneously returning the "_cpu cores is less than 8_" error.

**Solution**  

You can safely configure your NSX-T Edge Node VMs as `medium` size and ignore the error. 

#### <a id='1-9-0-windows-proxy'></a> Difficulty Changing Proxy for Windows Workers

You must configure a global proxy in the <%= vars.product_tile %> tile > **Networking** pane before you create any Windows workers that use the proxy.

You cannot change the proxy configuration for Windows workers in an existing cluster.

#### <a id='1-9-0-http-proxy-password'></a> Character Limitations in HTTP Proxy Password

For vSphere with NSX-T, the HTTP Proxy password field does not support the following special characters: `&` or `;`.  

#### <a id='1-9-0-uuid-length'></a> One Plan ID Longer than Other Plan IDs  

**Symptom**  

One of your plan IDs is one character longer than your other plan IDs.  

**Explanation**  

In <%= vars.k8s_runtime_abbr %>, each plan has a unique plan ID.
A plan ID is normally a UUID consisting of 32 alphanumeric characters and 4 hyphens. 
However, the **Plan 4** ID consists of 33 alphanumeric characters and 4 hyphens.  

**Solution**  

You can safely configure and use **Plan 4**. 
The length of the **Plan 4** ID does not affect the functionality of **Plan 4** clusters.  

If you require all plan IDs to have identical length, do not activate or use **Plan 4**.  

## <a id="management-console-1.9.0"></a> <%= vars.k8s_runtime_abbr %> Management Console 1.9.0

**Release Date**: September 29, 2020

### <a id="management-console-1.9.0-features"></a>Features

<%= vars.product_short %> Management Console v1.9.0 updates include:

- Support for vSphere 7
- Support for NSX-T 3.0, including running NSX-T on a virtual distributed switch.
- [Support for Windows worker nodes on NSX-T networking](console-deploy-wizard.html#plans)
- [Configure compute profiles in the management console](console-compute-profile.html)
- [Upgrade clusters in the management console](console-create-clusters.html#upgrade-clusters)

### <a id="management-console-1.9.0-snapshot"></a>Product Snapshot

<p class="note"><strong>Note</strong>: <%= vars.product_short %> Management Console provides an opinionated 
    installation of <%= vars.k8s_runtime_abbr %>. The supported versions may differ from or be more limited than
    what is generally supported by <%= vars.k8s_runtime_abbr %>.</p>

<table class="nice">
    <th>Element</th>
    <th>Details</th>
    <tr>
      <td>Version</td>
      <td>v1.9.0</td>
    </tr>
    <tr>
      <td>Release date</td>
      <td>September 29, 2020</td>
    </tr>
    <tr>
      <td>Installed Tanzu Kubernetes Grid Integrated Edition version</td>
      <td>v1.9.0</td>
    </tr>
    <tr>
      <td>Installed Ops Manager version</td>
      <td>v2.10.1</td>
    </tr>
    <tr>
      <td>Installed Kubernetes version</td>
        <td>v1.18.8+vmware.1</td>
    </tr>
    <tr>
      <td>Compatible NSX-T versions</td>
      <td>v3.0.2, v2.5.2, v2.5.0</td>
    </tr>
    <tr>
      <td>Installed Harbor Registry version</td>
      <td>v2.0.2</td>
    </tr>
    <tr>
        <td>Windows stemcells</td>
        <td>v2019.24+</td>
    </tr>
</table>

### <a id='management-console-1-9-0-upgrade'></a>Upgrade Path

The supported upgrade path to <%= vars.product_short %> Management Console v1.9.0 is from 
<%= vars.product_short %> v1.8.0 and later.

### <a id="management-console-1.9.0-known-issues"></a> Known Issues

The <%= vars.product_short %> Management Console v1.9.0 has the following known issues:

#### <a id="management-console-1.9.0-vrli-https"></a> vRealize Log Insight Integration Does Not Support HTTPS Connections

**Symptom**

The <%= vars.product_short %> Management Console integration to vRealize Log Insight does not support connections to the HTTPS port on the vRealize Log Insight server. 

**Workaround**

1. Use SSH to log in to the <%= vars.product_short %> Management Console appliance VM.
1. Open the file `/lib/systemd/system/pks-loginsight.service` in a text editor.
1. Add `-e LOG_SERVER_ENABLE_SSL_VERIFY=false`.  
1. Set `-e LOG_SERVER_USE_SSL=true`. 
   
    The resulting file should look like the following example:
       
    ```
    ExecStart=/bin/docker run --privileged --restart=always --network=pks 
    -v /var/log/journal:/var/log/journal 
    --name=pks-loginsight 
    -e TYPE=gear2-vm 
    -e LOG_SERVER_HOST=${LOGINSIGHT_HOST} 
    -e LOG_SERVER_PORT=${LOGINSIGHT_PORT} 
    -e LOG_SERVER_ENABLE_SSL_VERIFY=false 
    -e LOG_SERVER_USE_SSL=true 
    -e LOG_SERVER_AGENT_ID=${LOGINSIGHT_ID} 
    pksoctopus/vrli-journald:v07092019
    ```      
   
1. Save the file and run `systemctl daemon-reload`.
1. To restart the vRealize Log Insight service, run `systemctl restart pks-loginsight.service`.
 
<%= vars.product_short %> Management Console can now send logs to the HTTPS port on the vRealize Log Insight server.

#### <a id="management-console-1.9.0-vsphere-ha"></a> vSphere HA causes Management Console ovfenv Data Corruption

**Symptom**

If you enable vSphere HA on a cluster, if the TKGI Management Console appliance VM is running on a host in that cluster, and if the host reboots, vSphere HA recreates a new TKGI Management Console appliance VM on another host in the cluster. Due to an issue with vSphere HA, the `ovfenv` data for the newly created appliance VM is corrupted and the new appliance VM does not boot up with the correct network configuration. 

**Workaround**

- In the vSphere Client, right-click the appliance VM and select **Power** > **Shut Down Guest OS**.
- Right-click the appliance again and select Edit Settings.
- Select **VM Options** and click **OK**.
- Verify under Recent Tasks that a `Reconfigure virtual machine` task has run on the appliance VM.
- Power on the appliance VM.

#### <a id="management-console-1.9.0-k8s-profile"></a> Base64 encoded file arguments are not decoded in Kubernetes profiles

**Symptom**

Some file arguments in Kubernetes profiles are base64 encoded. When the management console displays the Kubernetes profile,
some file arguments are not decoded.

**Workaround**

Run `echo "$content" | base64 --decode` 

#### <a id="management-console-1.9.0-network-profile"></a> Network profiles not immediately selectable

**Symptom**

If you create network profiles and then try to apply them in the Create Cluster page, the new profiles 
are not available for selection. 

**Workaround**

Log out of the management console and log back in again.

#### <a id="management-console-1.9.0-cluster-summary"></a> Real-Time IP information not displayed for network profiles

**Symptom**

In the cluster summary page, only default IP pool, pod IP block, node IP block values are displayed, 
rather than the real-time values from the associated network profile. 

**Workaround**

None
