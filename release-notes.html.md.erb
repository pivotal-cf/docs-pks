---
title: Release Notes
owner: PKS
topictype: releasenotes
---

This topic contains release notes for <%= vars.product_full %> v1.6.0.

<p class="note warning"><strong>Warning:</strong> Before installing or upgrading to <%= vars.product_short %> v1.6.0, 
review the <a href="#1-6-0-breaking-changes">Breaking Changes</a>, below.
</p>

## <a id="1.6.0"></a> v1.6.0

**Release Date**: November 14, 2019

### <a id="1-6-0-features"></a>Features

This section describes new features and changes in this release.

#### <a id="pks-core"></a> PKS Control Plane and API

<%= vars.product_short %> v1.6.0 updates include:

* Enables operators to upgrade multiple Kubernetes clusters simultaneously and
to designate specific upgrade clusters as canary clusters.
For more information about multiple cluster upgrades, see
[Upgrade Clusters](upgrade-clusters.html#upgrade-clusters) in _Upgrading Clusters_.
* Adds a new UAA scope, `pks.clusters.admin.read`, for <%= vars.product_short %> users.
For information about UAA scopes,
see [UAA Scopes for <%= vars.product_short %> Users](uaa-scopes.html) and [Managing <%= vars.product_short %> Users with UAA](manage-users.html).
* Provides experimental integration with Tanzu Mission Control. For more information, see [Tanzu Mission Control Integration](./installing-nsx-t.html#tmc).
* Enables operators to limit the total number of clusters a user can provision in <%= vars.product_short %>.
For more information about quotas, see [Managing Resource Usage with Quotas](resource-usage.html) and [Viewing Usage Quotas](resource-review.html).
* Enables operators to configure a single Kubernetes cluster with a specific Docker
Registry CA certificate. For more information about configuring a cluster with a Docker
Registry CA certificate, see [Configuring <%= vars.product_short %> Clusters with Private Docker Registry CA Certificates (Beta)]
(docker-custom-ca-certs.html).
* Updates the `pks delete-cluster` PKS CLI command so that all cluster objects, including NSX-T networking objects, are deleted without the need to use the `bosh delete deployment` command to remove failed cluster deletions.

#### <a id="bosh-lifecycle"></a> Kubernetes Control Plane

<%= vars.product_short %> v1.6.0 updates include:

* Increases the **Worker VM Max in Flight** default value from `1` to `4` in the **PKS API** configuration pane, 
which accelerates cluster creation by allowing up to four new nodes to be provisioned simultaneously.
The updated default value is only applied during new <%= vars.product_short %> installation
and is not applied during an <%= vars.product_short %> upgrade.
If you are upgrading <%= vars.product_short %> from a previous version and want to accelerate multi-cluster provisioning, 
you can increase the value of **Worker VM Max in Flight** manually. 

#### <a id="logging-monitoring"></a> PKS Monitoring and Logging

<%= vars.product_short %> v1.6.0 updates include:

* Redesigns the **Logging** and **Monitoring** panes of the <%= vars.product_tile %> tile
and renames them to **Host Monitoring** and **In-Cluster Monitoring**. For information about configuring these panes, see
the _Installing <%= vars.product_short %>_ topic for your IaaS.
* Adds the **Max Message Size** field in the **Host Monitoring** pane. 
This allows you to configure the maximum number of characters of a log message that is forwarded to a syslog endpoint. 
This feature helps ensure that log messages are not truncated at the syslog endpoint. 
By default, the **Max Message Size** field is 10,000 characters. 
For more information, see [Host Monitoring](installing-pks-vsphere.html#syslog) in the _Installing <%= vars.product_short %>_ topic for your IaaS.  
* Adds the **Include kubelet metrics** setting. This enables operators to collect workload metrics across all Kubernetes clusters.
For more information, see [Host Monitoring](installing-pks-vsphere.html#syslog) in the _Installing <%= vars.product_short %>_ topic for your IaaS.
* Adds support for Fluent Bit output plugins to log sinks.
For information about configuring Fluent Bit output plugins, see [Create a ClusterLogSink or LogSink Resource with a Fluent Bit Output Plugin](create-sinks.html#fluentbit-output-plugin)
in _Creating Sink Resources_.
* Adds support for filtering logs and events from a `ClusterLogSink` or `LogSink` resource. For more information, see [Filter Sinks](create-sinks.html#filter-log-sinks) in _Creating Sink Resources_.  

####<a id="windows-features"></a> Windows on PKS

<%= vars.product_short %> v1.6.0 updates include:

* Adds support for floating Windows stemcells on vSphere.
For information about Kubernetes clusters with Windows workers in <%= vars.product_short %>,
see [Configuring Windows Worker-Based Kubernetes Clusters (Beta)](windows-pks-beta.html).
* Enables operators to configure the location of the Windows pause image.
For information about configuring **Kubelet customization - Windows pause image location**, see
[Plans](windows-pks-beta.html#plans) in _Configuring Windows Worker-Based Kubernetes Clusters (Beta)_.

####<a id="nsx-t-features"></a> PKS with NSX-T Networking

<%= vars.product_short %> v1.6.0 updates include:

* NSX Error CRD lets cluster managers and users view NSX errors in Kubernetes resource annotations, and use the command `kubectl get nsxerror` to view the health status of NSX-T cluster networking objects (NCP v2.5.0+). For more information,
see [Viewing the Health Status of Cluster Networking Objects (NSX-T only)](nsxt-health.html).
* DFW log control for dropped traffic lets cluster administrators define network profile to turn on logging and log any dropped or rejected packet by NSX-T distributed firewall rules (NCP v2.5.0+). For more information,
see [Defining Network Profiles for NCP Logging](network-profiles-ncp-logerr.html).
* Load balancer and ingress resource capacity observability using the NSXLoadBalancerMonitor CRD lets cluster managers and users use the command `kubectl get nsxLoadBalancerMonitors` to view a health score that reflects the current performance of the NSX-T load balancer service, including usage, traffic, and current status (NCP v2.5.1+). For more information,
see [Ingress Scaling (NSX-T only)](nsxt-ingress-scale.html).
* Ingress scale out using the LoadBalancer CRD lets cluster managers scale out the NSX-T load balancer for ingress routing (NCP v2.5.1+). For more information, see [Ingress Scaling (NSX-T only)](nsxt-ingress-scale.html).
* Support for Ingress URL Rewrite. For more information, see [Using Ingress URL Rewrite](nsxt-ingress-rewrite-url.html).
* Support for Activeâ€“Active Tier-0 router configuration when using a [Shared-Tier-1 topology](./network-profiles-shared-t1.html).
* Ability to place the load balancer and Tier-1 Active/Standby routers on different failure domains. See [Multisite Deployment of NSX-T Data Center](https://docs.vmware.com/en/VMware-NSX-T-Data-Center/2.5/administration/GUID-5D7E3D43-6497-4273-99C1-77613C36AD75.html) for more information.

####<a id="aws-features"></a> PKS on AWS Networking

<%= vars.product_short %> v1.6.0 updates include:

* Support for HTTP/HTTPS Proxy on AWS. For more information see, [Using Proxies with <%= vars.product_short %> on AWS](proxies-aws.html).  


#### <a id="telemetry"></a> Customer Experience Improvement Program

<%= vars.product_short %> v1.6.0 updates include:

* Administrators can name <%= vars.product_short %> installations so they are more easily recognizable in reports. For more information, see [Sample Reports](./telemetry.html#sample-reports).

#### <a id="component-updates"></a>Component Updates

<%= vars.product_short %> v1.6.0 updates include:

* Bumps Kubernetes to v1.15.5.
* Bumps UAA to v73.4.8.
* Bumps Jackson dependencies in the PKS API.

####<a id="bug-fixes"></a> Bug Fixes

<%= vars.product_short %> v1.6.0 includes the following bug fixes:

* Fixes an issue where enabling the Availability Sets mode at the BOSH Director > Azure Config resulted in the kubelet failing to start on provisioning of a Kubernetes cluster.
* Fixes an issue where persistent volume attachment failed on vSphere in a scenario where an AZ defined in Ops Manager does not contain a resource pool.
* Increases `network_profile` column size.
* Fixes a Telemetry event generation issue where the `upgrade_cluster_end` event is not sent for completed cluster upgrades.  
* Fixes an issue where networking changes did not propagate when upgrading from <%= vars.product_short %> v1.5 or later.  
* Fixes an issue where the Ingress IP address was excluded from the <%= vars.product_short %> floating IP pool.  
* Fixes an issue where the PKS OSB Proxy start was delayed by scanning all NSX-T firewall rules.
* Fixes an issue with the PKS clusters upgrade errand not pushing the latest NSX-T certificate to Kubernetes Master nodes.
* Fixes an issue with the PKS OSB Proxy taking a long time to start due to scanning all NSX-T firewall rules.
* Fixes an issue with PKS releasing floating IP addresses incompletely while deleting clusters under active/active mode.
* Fixes an issue with the DNS Lookup Feature: INGRESS IP not kept out of PKS Floating IP pool.
* Fixes an issue with the command `pks cluster details` does not display NS Group ID of master VMs.
* Checks the high availability mode of the Tier-0 router before creating PKS a cluster.  

### <a id='1-6-0-snapshot'></a>Product Snapshot

<table class="nice">
    <th>Element</th>
    <th>Details</th>
    <tr>
        <td>Version</td>
        <td>v1.6.0</td>
    </tr>
    <tr>
        <td>Release date</td>
        <td>November 14, 2019</td>
    </tr>
    <tr>
        <td>Compatible Ops Manager versions</td>
        <td><%= vars.ops_man_version_2_6 %> or <%= vars.ops_man_version_2_7 %></td>
    </tr>
    <tr>
        <td>Xenial stemcell version</td>
        <td>v456.51</td>
    </tr>
    <tr>
        <td>Windows stemcell version</td>
        <td>v2019.7</td>
    </tr>
    <tr>
        <td>Kubernetes version</td>
        <td>v1.15.5</td>
    </tr>
    <tr>
        <td>On-Demand Broker version</td>
        <td>v0.29.0</td>
    </tr>
    <tr>
        <td>NSX-T versions</td>
        <td>v2.5.0, v2.4.3</td>
    </tr>
    <tr>
        <td>NCP version</td>
        <td>v2.5.1</td>
    </tr>
    <tr>
        <td>Docker version</td>
        <td>v18.09.8</td>
    </tr>
    <tr>
        <td>Backup and Restore SDK version</td>
        <td>v1.17.0</td>
    </tr>
    <tr>
        <td>UAA</td>
        <td>v73.4.8</td>
    </tr>
   </tr>
</table>

### <a id='vsphere-reqs'></a> vSphere Version Requirements

For <%= vars.product_short %> installations on vSphere or on vSphere with NSX-T Data Center, refer to the <a href="https://www.vmware.com/resources/compatibility/sim/interop_matrix.php#interop&356=&175=&1=">VMware Product Interoperability Matrices</a>.</p>

### <a id='1-6-0-upgrade'></a>Upgrade Path

The supported upgrade paths to <%= vars.product_short %> v1.6.0 are from <%= vars.product_short %> v1.5.0 and later.

<p class="note"><strong>Note:</strong> Windows worker support is BETA. There is no upgrade support from PKS v1.5 to v1.6 for Windows worker nodes.</p>

### <a id='1-6-0-breaking-changes'></a>Breaking Changes

<%= vars.product_short %> v1.6.0 has the following breaking changes:

#### <a id='1-6-0-sink-cli-deprecation'></a> <%= vars.product_short %> Removes Sink Commands in the PKS CLI

<%= vars.product_short %> removes the following <%= vars.product_short %> Command Line Interface (PKS CLI) commands:

* `pks create-sink`
* `pks sinks`
* `pks delete-sink`

You can use the following Kubernetes CLI commands instead:

* `kubectl apply -f YOUR-SINK.yml`
* `kubectl get clusterlogsinks`
* `kubectl delete clusterlogsink YOUR-SINK`

For more information about defining and managing sink resources,
see [Creating Sink Resources](create-sinks.html).

#### <a id='api-endpoints'></a> Changes to PKS API Endpoints

This release moves the `clusters`, `compute-profiles`, `quotas`, and `usages` PKS API endpoints from `v1beta1` to `v1`.
`v1beta1` is no longer supported for these endpoints. You must use `v1`.
For example, instead of `https://YOUR-PKS-API-FQDN:9021/v1beta1/quotas`, use `https://YOUR-PKS-API-FQDN:9021/v1/quotas`.

###<a id='1-6-0-known-issues'></a> Known Issues

<%= vars.product_short %> v1.6.0 has the following known issues.

#### <a id="1-6-0-security-group"></a>Azure Default Security Group Is Not Automatically Assigned to Cluster VMs

**Symptom**

You experience issues when configuring a load balancer for a multi-master Kubernetes cluster or creating a service of type `LoadBalancer`.
Additionally, in the Azure portal, the **VM** > **Networking** page does not display
any inbound and outbound traffic rules for your cluster VMs.

**Explanation**

As part of configuring the <%= vars.product_tile %> tile for Azure, you enter **Default Security Group** in the **Kubernetes Cloud Provider** pane.
When you create a Kubernetes cluster, <%= vars.product_short %> automatically assigns this security group to each VM in the cluster.
However, on Azure the automatic assignment may not occur.

As a result, your inbound and outbound traffic rules defined in the security group are not applied to the cluster VMs.

**Workaround**

If you experience this issue, manually assign the default security group to each VM NIC in your cluster.

#### <a id='1-6-0-cluster-creation-fails'></a>Cluster Creation Fails When First AZ Runs Out of Resources

**Symptom**

If the first availability zone (AZ) used by a plan with multiple AZs runs out of
resources, cluster creation fails with an error like the following:

<pre class="terminal">
L Error: CPI error 'Bosh::Clouds::CloudError' with message 'No valid placement found for requested memory: 4096
</pre>

**Explanation**

BOSH creates VMs for your <%= vars.product_short %> deployment using a round-robin
algorithm, creating the first VM in the first AZ that your plan uses.
If the AZ runs out of resources, cluster creation fails because BOSH cannot create
the cluster VM.

For example, if you have three AZs and you
create two clusters with four worker VMs each, BOSH deploys VMs in the
following AZs:

<table>
  <tr>
    <th></th>
    <th>AZ1</th>
    <th>AZ2</th>
    <th>AZ3</th>
  </tr>
  <tr>
    <th>Cluster 1</th>
    <td>Worker VM 1</td>
    <td>Worker VM 2</td>
    <td>Worker VM 3</td>
  </tr>
  <tr>
    <td></td>
    <td>Worker VM 4</td>
    <td></td>
    <td></td>
  </tr>
  <tr>
    <th>Cluster 2</th>
    <td>Worker VM 1</td>
    <td>Worker VM 2</td>
    <td>Worker VM 3</td>
  </tr>
  <tr>
    <td></td>
    <td>Worker VM 4</td>
    <td></td>
    <td></td>
</table>

In this scenario, AZ1 has twice as many VMs as AZ2 or AZ3.

#### <a id='1-6-0-azure-worker-node'></a>Azure Worker Node Communication Fails after Upgrade

**Symptom**

Outbound communication from a worker node VM fails after upgrading <%= vars.product_short %>.

**Explanation**

<%= vars.product_short %> uses Azure Availability Sets to improve the uptime of workloads and worker nodes in the event of Azure platform failures. Worker node
VMs are distributed evenly across Availability Sets.

Azure Standard SKU Load Balancers are recommended for the Kubernetes control plane and Kubernetes ingress and egress. This load balancer type provides an IP address for outbound communication using SNAT.

During an upgrade, when BOSH rebuilds a given worker instance in an Availability Set,
Azure can time out while re-attaching the worker node network interface to the
back-end pool of the Standard SKU Load Balancer.

For more information, see [Outbound connections in Azure](https://docs.microsoft.com/en-us/azure/load-balancer/load-balancer-outbound-connections) in the Azure documentation.

**Workaround**

You can manually re-attach the worker instance to the back-end pool of the Azure Standard SKU Load Balancer in your Azure console.

#### <a id='1-6-0-timeout'></a> Error During Individual Cluster Upgrades

**Symptom**

While submitting a large number of cluster upgrade requests using the `pks upgrade-cluster` command, some of your Kubernetes clusters are marked as failed.

**Explanation**

BOSH upgrades Kubernetes clusters in parallel with a limit of up to four concurrent cluster upgrades by default.
If you schedule more than four cluster upgrades,
<%= vars.product_short %> queues the upgrades and waits for BOSH to finish the last upgrade.
When BOSH finishes the last upgrade, it starts working on the next upgrade request.

If you submit too many cluster upgrades to BOSH, an error may occur,
where some of your clusters are marked as `FAILED` because BOSH can start the upgrade only within the specified timeout.
The timeout is set to 168 hours by default.
However, BOSH does not remove the task from the queue or stop working on the upgrade if it has been picked up.

**Solution**

If you expect that upgrading all of your Kubernetes clusters takes more than 168 hours,
do not use a script that submits upgrade requests for all of your clusters at once.
For information about upgrading Kubernetes clusters provisioned by <%= vars.product_short %>,
see [Upgrading Clusters](./upgrade-clusters.html).

#### <a id="1-6-0-kubectl-azs"></a> Kubectl CLI Commands Do Not Work after Changing an Existing Plan to a Different AZ

**Symptom**

After you update the AZ of an existing plan, kubectl CLI commands do not work for your clusters associated with the plan.

**Explanation**

This issue occurs in IaaS environments that do not support attaching a disk across multiple AZs.

When the plan of an existing cluster changes to a different AZ,
BOSH migrates the cluster by creating VMs for the cluster in the new AZ and
removing your cluster VMs from the original AZ.

On an IaaS that does not support attaching VM disks across AZs,
the disks BOSH attaches to the new VMs do not have the original content.

**Workaround**

If you cannot run kubectl CLI commands after reconfiguring the AZ of an existing cluster, contact Support for assistance.

#### <a id='1-6-0-uuid-length'></a> One Plan ID Longer than Other Plan IDs

**Symptom**

One of your plan IDs is one character longer than your other plan IDs.

**Explanation**

In <%= vars.product_short %>, each plan has a unique plan ID.
A plan ID is normally a UUID consisting of 32 alphanumeric characters and 4 hyphens.
However, the **Plan 4** ID consists of 33 alphanumeric characters and 4 hyphens.

**Solution**

You can safely configure and use **Plan 4**.
The length of the **Plan 4** ID does not affect the functionality of **Plan 4** clusters.

If you require all plan IDs to have identical length, do not activate or use **Plan 4**.

#### <a id='1-6-0-tmc-names'></a> Kubernetes Cluster Name Limitation for Tanzu Mission Control Integration

Tanzu Mission Control integration cannot attach Tanzu Mission Control to Kubernetes clusters that have uppercase letters in their names.

**Symptom**

Clusters that you create with `pks create-cluster` do not appear in the Tanzu Mission Control, even though you configured Tanzu Mission Control integration as described in [Integrate Tanzu Mission Control](./installing-nsx-t.html#tmc).

**Explanation**

The regex pattern that parses cluster names in Tanzu Mission Control integration fails with names that contain uppercase letters.

**Solution**

When running `pks create-cluster` to create clusters that you want to track in Tanzu Mission Control, pass in names that contain only lowercase letters and numbers.

