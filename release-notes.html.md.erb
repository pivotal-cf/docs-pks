---
title: Release Notes
owner: TKGI
topictype: releasenotes
---

This topic contains release notes for <%= vars.product_short %> (<%= vars.k8s_runtime_abbr %>) v1.10.

<p class="note warning"><strong>Warning:</strong> Before installing or upgrading to <%= vars.product_short %> v1.10,
review the <a href="#1-10-0-breaking-changes">Breaking Changes</a> below.
</p>

## <a id="1.10.0"></a><%= vars.k8s_runtime_abbr %> v1.10.0

**Release Date**:

### <a id='1-10-0-snapshot'></a><a id='product-snapshot'></a>Product Snapshot

<table class="nice">
    <tr>
        <th>Release</th>
        <th>Details</th>
    </tr>
    <tr>
        <td>Version</td>
        <td>v1.10.0</td>
    </tr>
    <tr>
        <td>Release date</td>
        <td></td>
    </tr>
    <tr>
        <th>Component</th>
        <th>Version</th>
    </tr>
    <tr>
        <td>Kubernetes</td>
        <td>v1.19.4</td>
    </tr>
    <tr>
        <td>CoreDNS</td>
        <td></td>
    </tr>
    <tr>
        <td>Docker</td>
        <td>
            Linux: <br>
            Windows:
        </td>
    </tr>
    <tr>
        <td>etcd</td>
        <td></td>
    </tr>
    <tr>
        <td>Metrics Server</td>
        <td></td>
    </tr>
    <tr>
        <td>NCP</td>
        <td>v3.1.0.1</td>
    </tr>
    <tr>
        <td>Percona XtraDB Cluster (PXC)</td>
        <td>v0.31.0</td>
    </tr>
    <tr>
        <td>UAA</td>
        <td>v74.5.21</td>
    </tr>
    <tr>
        <th>Compatibilities</th>
        <th>Versions</th>
    </tr>
    <tr>
        <td>Ops Manager</td>
        <td>
        </td>
    </tr>
    <tr>
        <td>Xenial stemcells</td>
        <td>See <a href="https://network.pivotal.io/products/pivotal-container-service"><%= vars.product_network %>.</a></td>
    </tr>
    <tr>
        <td>Windows stemcells</td>
        <td>v2019.28+</td>
    </tr>
    <tr>
        <td>Backup and Restore SDK</td>
        <td></td>
    </tr>
    <tr>
        <td>vSphere</td>
        <td></td>
    </tr>
    <tr>
        <td>VMware Cloud Foundation (VCF)</td>
        <td></td>
    </tr>
    <tr>
        <td>CNS for vSphere</td>
        <td></td>
    </tr>
    <tr>
        <td>NSX-T</td>
        <td></td>
    </tr>
    <tr>
        <td>Harbor</td>
        <td></td>
    </tr>
    <tr>
        <td>Velero</td>
        <td></td>
    </tr>
</table>

### <a id='1-10-0-upgrade'></a>Upgrade Path <%#CONFIRM BEFORE GA%>

The supported upgrade paths to <%= vars.k8s_runtime_abbr %> v1.10.0 are from <%= vars.product_short %> v1.9.0 and later.

### <a id="1-10-0-features"></a>Features

This section describes new features and changes in <%= vars.product_full %> v1.10.0.

* (Beta) Supports high availability (HA) mode in <%= vars.product_short %>.
You can now scale the number of VM instances for the following <%= vars.product_short %> control plane jobs:

    * <%= vars.product_short %> API and UAA
    * <%= vars.product_short %> database

    To configure HA (beta) for new <%= vars.k8s_runtime_abbr %> v1.10 installations, see [Resource Config](installing-vsphere.html#resource-config) in the _Installing_ topic for your IaaS.
    To configure HA (beta) for <%= vars.product_short %> v1.10 upgrades from v1.9, see [Upgrading Enterprise PKS](upgrade.html) or [Upgrading Enterprise PKS with NSX-T](upgrade-nsxt.html)
    and follow the instructions in _(Optional) Scale to <%= vars.k8s_runtime_abbr %> High Availability Mode_.
    <p class="note warning"><strong>Warning:</strong> HA mode is a beta feature. Do not scale your <strong><%= vars.k8s_runtime_abbr %> API</strong> or <strong><%= vars.k8s_runtime_abbr %> Database</strong> to more than one instance in production environments.</p>
* Removes **Wavefront Alert Recipient**, **Create pre-defined Wavefront alerts errand**, and **Delete pre-defined Wavefront alerts errand** from the <%= vars.product_short %> tile. For more information, see [<%= vars.k8s_runtime_abbr %>-Defined Wavefront Alerts Removed from the Tile](#1-10-0-wavefront-integration) below.
* **[Enhancement]** Reduced the possibility of <%= vars.k8s_runtime_abbr %> CLI `get-credentials` returning the error “_od-broker is processing a request for the same instance... please try again later_” during periods of intermittent latency.
* **[Enhancement]** Swap is now disabled by default. For more information, see [Swap Is Disabled by Default ](#1-10-0-swap-disabled) in _Breaking Changes_ below.
* **[Enhancement]** Improves error messages for cluster creation and upgrade failures. 
For more information, see [Cluster Creation and Upgrade Failure Error Messages No Longer Truncated](#1-10-0-cluster-errors) in _Breaking Changes_ below.
* **[Security Fix]** Passes additional CIS Kubernetes Benchmarks. See [<%= vars.k8s_runtime_abbr %> Cluster Benchmarks](./security.html#benchmarks) for details.
* **[Bug Fix]** Fixes [Your <%= vars.k8s_runtime_abbr %> Cluster Fails to Start After Changing Your Worker Node's Compute Profile AZ](https://docs.pivotal.io/tkgi/1-9/release-notes.html#1-9-2-compute-profile-az-worker).
* **[Bug Fix]** Fixes [<%= vars.k8s_runtime_abbr %> Upgrade or Install Fails with Error "x509: certificate relies on legacy Common Name field"](https://docs.pivotal.io/tkgi/1-9/release-notes.html#1-9-0-failed-upgrade). 

#### <a id="1-10-0-proxy"></a>Cluster-Specific Proxy Settings (NSX-T and AWS)

You can configure proxy settings specific to individual <%= vars.k8s_runtime_abbr %> clusters,
overriding the global settings in the <%= vars.k8s_runtime_abbr %> tile > **Networking** pane.
For more information, see [Configure Cluster Proxies](proxies-cluster.html).


#### <a id="1-10-0-antrea-cni"></a>Supports the Antrea CNI

<%= vars.product_short %> now provides the option to use the Antrea Container Network Interface (CNI) as
the CNI for new <%= vars.k8s_runtime_abbr %>-provisioned clusters. For more information about 
using Antrea as your CNI, see [About Upgrading from the Flannel CNI to the Antrea CNI](understanding-upgrades.html#cni) 
in _About <%= vars.product_short %> Upgrades_.


#### <a id="1-10-0-node-labels-and-taints"></a>Apply Persistent Node Labels and Node Taints Using Compute Profiles

On vSphere and vSphere with NSX-T, <%= vars.product_short %> supports applying persistent labels and taints to a Kubernetes node using Compute Profiles. 
For more information see [`node_pools` Block](compute-profiles-manage.html#worker-nodes) 
in _Creating and Managing Compute Profiles with the CLI (vSphere)_.

#### <a id="1-10-0-wavefront-integration"></a> <%= vars.k8s_runtime_abbr %>-Defined Wavefront Alerts Removed from the Tile

<%= vars.k8s_runtime_abbr %> v1.10 removes the following configuration options from the tile:

* **Create pre-defined Wavefront alerts errand**
* **Delete pre-defined Wavefront alerts errand**
* **Wavefront Alert Recipient**

Previously, the **Create pre-defined Wavefront alerts errand** created a pre-defined set of alerts for
<%= vars.k8s_runtime_abbr %> in Wavefront. If you enabled these options in an earlier version of
<%= vars.k8s_runtime_abbr %> and you upgrade it to v1.10, you will continue to receive the
<%= vars.k8s_runtime_abbr %>-defined alerts, in addition to the default alerts that are provided by the Wavefront VMware Tanzu Kubernetes Grid Integrated Edition integration.

For more information about the default alerts, see [Predefined Alerts for the Integration](https://docs.wavefront.com/integrations_tkgi.html#predefined-alerts-for-the-integration).

#### <a id="1-10-0-component-updates"></a>Component Updates

The following components have been updated:

* Bumps Kubernetes to v1.19.4.
* Bumps Xenial stemcell to v621.94.
* Bumps NCP to v3.1.0.17170700.
* Bumps PXC to v0.31.0.
* Bumps UAA to v74.5.21.

### <a id="1-10-0-breaking-changes"></a> Breaking Changes
TKGI v1.10.0 has the following breaking changes. 

#### <a id='1-10-0-swap-disabled'></a> Swap Is Disabled by Default  

Swap is now disabled on all worker nodes. 
In previous versions of <%= vars.product_short %>, Swap was enabled, but upstream Kubernetes does not support this setting. 
You cannot enable swap through the TKGI CLI, and manually configuring swap is not permitted. 

#### <a id='1-10-0-cluster-errors'></a> Cluster Creation and Upgrade Failure Error Messages No Longer Truncated

<%= vars.product_short %> v1.10 includes improved error messages for cluster creation and upgrade failures. 
Previously, error messages greater than 128 bytes were truncated. In <%= vars.k8s_runtime_abbr %> v1.10 
logged cluster creation and upgrade failure error messages are no longer truncated. 


###<a id='1-10-0-known-issues'></a>Known Issues

<%= vars.k8s_runtime_abbr %> v1.10.0 has the following known issues:

#### <a id="1-9-0-azure-apply-changes"></a> Error: Could Not Execute "Apply-Changes" in Azure Environment

**Symptom**

After clicking **Apply Changes** on the <%= vars.k8s_runtime_abbr %> tile in an Azure environment, you experience
an error '_...could not execute "apply-changes"..._' with either of the following descriptions:

* _{"errors":{"base":["undefined method 'location' for nil:NilClass"]}}_
* _FailedError.new("Resource Groups in region '#{location}' do not support Availability Zones"))_

For example:

```
INFO | 2020-09-21 03:46:49 +0000 | Vessel::Workflows::Installer#run | Install product (apply changes)
2020/09/21 03:47:02 could not execute "apply-changes": installation failed to trigger: request failed: unexpected response from /api/v0/installations:
HTTP/1.1 500 Internal Server Error
Transfer-Encoding: chunked
Cache-Control: no-cache, no-store
Connection: keep-alive
Content-Type: application/json; charset=utf-8
Date: Mon, 21 Sep 2020 17:51:50 GMT
Expires: Fri, 01 Jan 1990 00:00:00 GMT
Pragma: no-cache
Referrer-Policy: strict-origin-when-cross-origin
Server: Ops Manager
Strict-Transport-Security: max-age=31536000; includeSubDomains
X-Content-Type-Options: nosniff
X-Download-Options: noopen
X-Frame-Options: SAMEORIGIN
X-Permitted-Cross-Domain-Policies: none
X-Request-Id: f5fc99c1-21a7-45c3-7f39
X-Runtime: 9.905591
X-Xss-Protection: 1; mode=block

44
{"errors":{"base":["undefined method `location' for nil:NilClass"]}}
0
```

**Explanation**

The Azure CPI endpoint used by Ops Manager has been changed and
your installed version of Ops Manager is not compatible with the new endpoint.

**Workaround**

Run the following Ops Manager CLI command:

```
om --skip-ssl-validation --username USERNAME --password PASSWORD --target https://OPSMAN-API curl --silent --path /api/v0/staged/director/verifiers/install_time/IaasConfigurationVerifier -x PUT -d '{ "enabled": false }'
```

Where:

* `USERNAME` is the account to use to run Ops Manager API commands.
* `PASSWORD` is the password for the account.
* `OPSMAN-API` is the IP address for the Ops Manager API


For more information, see [Error 'undefined method location' is received when running Apply Change on Azure]
(https://community.pivotal.io/s/article/undefined-method-location-when-running-Apply-Change-on-Azure?language=en_US)
in the VMware Tanzu Knowledge Base.

#### <a id="1-9-0-vrops-windows-clusters"></a> VMware vRealize Operations Does Not Support Windows Worker-Based Kubernetes Clusters

VMware vRealize Operations (vROPs) does not support Windows worker-based Kubernetes clusters and cannot be used to
manage <%= vars.k8s_runtime_abbr %>-provisioned Windows workers.


#### <a id='1-9-0-wavefront-no-win'></a><%= vars.k8s_runtime_abbr %> Wavefront Does Not Work for Windows Workers

The Wavefront collector and proxy do not support monitoring of clusters with Windows-based worker nodes.
For alternative ways to set up in-cluster monitoring, see [Monitoring Workers and Workloads](./in-cluster-monitoring.html).

#### <a id='1-9-0-ping'></a>Pinging Windows Workers Does Not Work

<%= vars.k8s_runtime_abbr %>-provisioned Windows workers inherit a Kubernetes limitation that prevents
outbound ICMP communication from workers.
As a result, pinging Windows workers does not work.

For information about this limitation, see [Limitations > Networking](https://kubernetes.io/docs/setup/production-environment/windows/intro-windows-in-kubernetes/#networking-1)
in the _Windows in Kubernetes_ documentation.

#### <a id="1-9-0-windows-velero-limitations"></a> Velero Does Not Support Backing Up Stateful Windows Workloads

You can use Velero to backup stateless <%= vars.k8s_runtime_abbr %>-provisioned Windows workers.
Velero can back up stateless Windows workloads only, and cannot be used to backup stateful Windows applications.
For more information, see [Velero on Windows](https://velero.io/docs/v1.4/basic-install/#velero-on-windows) in
_Basic Install_ in the Velero documentation.

#### <a id="1-9-0-tmc-on-gcp"></a>TMC Integration Not Supported on GCP

<%= vars.k8s_runtime_abbr %> on Google Cloud Platform (GCP) does not support
Tanzu Mission Control integration, which is configured in
the **<%= vars.product_tile %>** tile > the **Tanzu Mission Control (Experimental)** pane.

If you intend to run <%= vars.k8s_runtime_abbr %> v1.9 on GCP,
skip this pane when configuring the <%= vars.product_tile %> tile.

#### <a id="1-9-0-old-profile-no-cli"></a>Compute Profile CLI Commands Do Not Support Pre-v1.9 Profiles

Compute profiles created in <%= vars.k8s_runtime_abbr %> v1.8 and earlier have a different format from v1.9 compute profiles, as described in [Compute Profile CLI Support and Improvements (vSphere)](#1-9-0-compute-profiles).

You cannot use the v1.9 `tkgi` CLI compute profile commands to manage compute profiles created in prior versions of <%= vars.k8s_runtime_abbr %>.
To perform compute profile operations on pre-v1.9 profiles and the clusters that use them, use the `curl` commands and JSON structure described in [Using Compute Profiles](https://docs.pivotal.io/tkgi/1-8/compute-profiles.html) in the <%= vars.k8s_runtime_abbr %> v1.8 documentation.

Compute profiles created in prior versions of <%= vars.k8s_runtime_abbr %>, and the clusters that use them, continue to work in v1.9.

#### <a id="1-9-0-profile-no-win-flannel"></a>Compute Profiles Not Supported with for Windows-Worker Clusters on Flannel

On vSphere with Flannel networking, you can only apply compute profiles to Linux clusters.
On vSphere with NSX-T networking you can use compute profiles with both Linux- and Windows-worker clusters.

#### <a id="1-9-0-profile-resize-down"></a>TKGI CLI Does Not Prevent Reducing the Control Plane Node Count

TKGI CLI does not prevent accidentally reducing a cluster's control plane node count using a compute profile.

<p class="note warning"><strong>Warning:</strong>
    Reducing a cluster's control plane node count can destroy the cluster.
    Do not scale out or scale in existing master nodes by reconfiguring the TKGI tile or by using a compute profile.
    Reducing a cluster's number of control plane nodes may remove a master node and cause the cluster to become inactive.
</p>

#### <a id="1-9-0-profile-bosh-upgrade"></a>Compute Profile Dropped From Clusters During BOSH Upgrade

If a cluster created using a compute profile is upgraded using `tkgi upgrade-cluster`,
the cluster's compute profile will be dropped.

#### <a id="1-9-0-in-windows-notready-nodes"></a> Windows Cluster Nodes Not Deleted After VM Deleted

**Symptom**

After you delete a VM using your IAAS' management console you notice a Windows worker node
that had been on that VM is now in a `notReady` state.

**Solution**

1. To identify the leftover node:

    ```
    kubectl get no -o wide
    ```
1. Locate nodes on the returned list that are in a `notReady` state and have the same IP address as another node in the list.
1. To manually delete a `notReady` node:

    ```
    kubectl delete node NODE-NAME
    ```
    Where `NODE-NAME` is the name of the node in the `notReady` state.

#### <a id="1-9-0-oidc-response-header"></a>502 Bad Gateway After OIDC Login

**Symptom**

You experience a "502 Bad Gateway" error from the NSX load balancer after you log in to OIDC.

**Explanation**

A large response header has exceeded your NSX-T load balancer maximum
response header size. The default maximum response header size is 10,240 characters and should
be resized to 50,000.

**Workaround**

If you experience this issue, manually reconfigure your NSX-T `request_header_size`
and `response_header_size` to 50,000 characters.
For information about configuring NSX-T default header sizes,
see [OIDC Response Header Overflow](https://community.pivotal.io/s/article/OIDC-Response-Header-overflow) in the Knowledge Base.


#### <a id='1-9-0-eight-cores-error'></a> NSX-T Pre-Check Errand Fails Due to Edge Node Configuration

**Symptom**

You have configured your NSX-T Edge Node VM as `medium` size,
and the NSX-T Pre-Check Errand fails with the following error:
"_ERROR: NSX-T Precheck failed due to Edge Node ... no of cpu cores is less than 8_".

**Explanation**

The NSX-T Pre-Check Errand is erroneously returning the "_cpu cores is less than 8_" error.

**Solution**

You can safely configure your NSX-T Edge Node VMs as `medium` size and ignore the error.

#### <a id='1-9-0-windows-proxy'></a> Difficulty Changing Proxy for Windows Workers

You must configure a global proxy in the <%= vars.product_tile %> tile > **Networking** pane before you create any Windows workers that use the proxy.

You cannot change the proxy configuration for Windows workers in an existing cluster.

#### <a id='1-9-0-http-proxy-password'></a> Character Limitations in HTTP Proxy Password

For vSphere with NSX-T, the HTTP Proxy password field does not support the following special characters: `&` or `;`.

#### <a id='1-9-0-harbor-storage-config'></a> Error After Modifying Your Harbor Storage Configuration

**Symptom**

You receive the following error after modifying your existing Harbor installation's storage configuration:

```
Error response from daemon: manifest for ... not found: manifest unknown: manifest unknown
```


**Explanation**

Harbor does not support modifying an existing Harbor installation's storage configuration.

**Workaround**

To modify your Harbor storage configuration,
re-install Harbor. Before starting Harbor, configure the new Harbor installation with the desired configuration.

#### <a id='1-9-0-uuid-length'></a> One Plan ID Longer than Other Plan IDs

**Symptom**

One of your plan IDs is one character longer than your other plan IDs.

**Explanation**

In <%= vars.k8s_runtime_abbr %>, each plan has a unique plan ID.
A plan ID is normally a UUID consisting of 32 alphanumeric characters and 4 hyphens.
However, the **Plan 4** ID consists of 33 alphanumeric characters and 4 hyphens.

**Solution**

You can safely configure and use **Plan 4**.
The length of the **Plan 4** ID does not affect the functionality of **Plan 4** clusters.

If you require all plan IDs to have identical length, do not activate or use **Plan 4**.

## <a id="management-console-1.10.0"></a> <%= vars.k8s_runtime_abbr %> Management Console 1.10.0

**Release Date**:

### <a id="management-console-1.10.0-features"></a>Features

<%= vars.product_short %> Management Console v1.10.0 updates include:  

* **[Bug Fix]** Fixes [TKGI Management Console regenerates certificates if the NSX Manager admin password changes](https://docs.pivotal.io/tkgi/1-9/release-notes.html#1-9-1-nsxt-cert).

### <a id="management-console-1.10.0-snapshot"></a>Product Snapshot

<p class="note"><strong>Note</strong>: <%= vars.product_short %> Management Console provides an opinionated
    installation of <%= vars.k8s_runtime_abbr %>. The supported versions may differ from or be more limited than
    what is generally supported by <%= vars.k8s_runtime_abbr %>.</p>

<table class="nice">
    <th>Element</th>
    <th>Details</th>
    <tr>
      <td>Version</td>
      <td>v1.10.0</td>
    </tr>
    <tr>
      <td>Release date</td>
      <td></td>
    </tr>
    <tr>
      <td>Installed Tanzu Kubernetes Grid Integrated Edition version</td>
      <td>v1.10.0</td>
    </tr>
    <tr>
      <td>Installed Ops Manager version</td>
      <td>2.10.5</td>
    </tr>
    <tr>
      <td>Installed Kubernetes version</td>
        <td>1.19.4</td>
    </tr>
    <tr>
      <td>Compatible NSX-T versions</td>
      <td></td>
    </tr>
    <tr>
      <td>Installed Harbor Registry version</td>
      <td>2.1.2</td>
    </tr>
    <tr>
        <td>Windows stemcells</td>
        <td></td>
    </tr>
</table>

### <a id="management-console-1-10-0-features"></a>Features

This section describes new features and changes in <%= vars.product_full %> Management Console v1.10.0.

* Supports high availability (HA) mode in <%= vars.product_short %> Management Console.
You can now scale the number of VM instances for the following <%= vars.product_short %> control plane jobs:

    * <%= vars.product_short %> API and UAA
    * <%= vars.product_short %> database
* Adds support for Antrea CNI when deploying to vSphere without NSX-T networking.
* Adds support for a No-NAT with virtual switch (VSS/VDS) topology.
* Adds support for changing the compute profile after cluster creation.
* Adds support for adding labels and taints to nodes when creating compute profiles.
* Enforces the vSphere standard for passwords when creating local user accounts.

### <a id='management-console-1-10-0-upgrade'></a>Upgrade Path <%#CONFIRM BEFORE GA%>

The supported upgrade path to <%= vars.product_short %> Management Console v1.10.0 is from
<%= vars.product_short %> v1.9.0 and later.

### <a id="management-console-1.10.0-known-issues"></a> Known Issues

The <%= vars.product_short %> Management Console v1.10.0 has the following known issues:

#### <a id="management-console-1.10.0-vrli-https"></a> vRealize Log Insight Integration Does Not Support HTTPS Connections

**Symptom**

The <%= vars.product_short %> Management Console integration to vRealize Log Insight does not support connections to the HTTPS port on the vRealize Log Insight server.

**Workaround**

1. Use SSH to log in to the <%= vars.product_short %> Management Console appliance VM.
1. Open the file `/lib/systemd/system/pks-loginsight.service` in a text editor.
1. Add `-e LOG_SERVER_ENABLE_SSL_VERIFY=false`.
1. Set `-e LOG_SERVER_USE_SSL=true`.

    The resulting file should look like the following example:

    ```
    ExecStart=/bin/docker run --privileged --restart=always --network=pks
    -v /var/log/journal:/var/log/journal
    --name=pks-loginsight
    -e TYPE=gear2-vm
    -e LOG_SERVER_HOST=${LOGINSIGHT_HOST}
    -e LOG_SERVER_PORT=${LOGINSIGHT_PORT}
    -e LOG_SERVER_ENABLE_SSL_VERIFY=false
    -e LOG_SERVER_USE_SSL=true
    -e LOG_SERVER_AGENT_ID=${LOGINSIGHT_ID}
    pksoctopus/vrli-journald:v07092019
    ```

1. Save the file and run `systemctl daemon-reload`.
1. To restart the vRealize Log Insight service, run `systemctl restart pks-loginsight.service`.

<%= vars.product_short %> Management Console can now send logs to the HTTPS port on the vRealize Log Insight server.

#### <a id="management-console-1.10.0-vsphere-ha"></a> vSphere HA causes Management Console ovfenv Data Corruption

**Symptom**

If you enable vSphere HA on a cluster, if the TKGI Management Console appliance VM is running on a host in that cluster, and if the host reboots, vSphere HA recreates a new TKGI Management Console appliance VM on another host in the cluster. Due to an issue with vSphere HA, the `ovfenv` data for the newly created appliance VM is corrupted and the new appliance VM does not boot up with the correct network configuration.

**Workaround**

- In the vSphere Client, right-click the appliance VM and select **Power** > **Shut Down Guest OS**.
- Right-click the appliance again and select Edit Settings.
- Select **VM Options** and click **OK**.
- Verify under Recent Tasks that a `Reconfigure virtual machine` task has run on the appliance VM.
- Power on the appliance VM.

#### <a id="management-console-1.10.0-k8s-profile"></a> Base64 encoded file arguments are not decoded in Kubernetes profiles

**Symptom**

Some file arguments in Kubernetes profiles are base64 encoded. When the management console displays the Kubernetes profile,
some file arguments are not decoded.

**Workaround**

Run `echo "$content" | base64 --decode`

#### <a id="management-console-1.10.0-network-profile"></a> Network profiles not immediately selectable

**Symptom**

If you create network profiles and then try to apply them in the Create Cluster page, the new profiles
are not available for selection.

**Workaround**

Log out of the management console and log back in again.

#### <a id="management-console-1.10.0-cluster-summary"></a> Real-Time IP information not displayed for network profiles

**Symptom**

In the cluster summary page, only default IP pool, pod IP block, node IP block values are displayed,
rather than the real-time values from the associated network profile.

**Workaround**

None

#### <a id='management-console-1-10-0-harbor-storage-config'></a> Error After Modifying Your Harbor Storage Configuration

**Symptom**

You receive the following error after modifying your existing Harbor installation's storage configuration:

```
Error response from daemon: manifest for ... not found: manifest unknown: manifest unknown
```


**Explanation**

Harbor does not support modifying an existing Harbor installation's storage configuration.

**Workaround**

To modify your Harbor storage configuration,
re-install Harbor. Before starting Harbor, configure the new Harbor installation with the desired configuration.
