---
title: Release Notes
owner: PKS
topictype: releasenotes
---

This topic contains release notes for <%= vars.product_full %> v1.6.

<p class="note warning"><strong>Warning:</strong> Before installing or upgrading to <%= vars.product_short %> v1.6,
review the <a href="#1-6-0-breaking-changes">Breaking Changes</a> below.
</p>

## <a id="1.6.3"></a> v1.6.3

**Release Date**: July 28, 2020

### <a id="1-6-3-features"></a>Features

New features and changes in this release:

* Bumps Kubernetes to v1.15.12.
* Bumps UAA to v73.4.21.
* Bumps the Windows stemcell to v2019.15.

### <a id='1-6-3-snapshot'></a>Product Snapshot

<table class="nice">
    <tr>
        <th>Release</th>
        <th>Details</th>
    </tr>
    <tr>
        <td>Version</td>
        <td>v1.6.3</td>
    </tr>
    <tr>
        <td>Release date</td>
        <td>July 28, 2020</td>
    </tr>
    <tr>
        <th>Component</th>
        <th>Version</th>
    </tr>
    <tr>
        <td>Kubernetes</td>
        <td>v1.15.12</td>
    </tr>
    <tr>
        <td>CoreDNS</td>
        <td>v1.3.1</td>
    </tr>
    <tr>
        <td>Docker</td>
        <td>v18.09.9</td>
    </tr>
    <tr>
        <td>etcd</td>
        <td>v3.3.12</td>
    </tr>
    <tr>
        <td>Metrics Server</td>
        <td>v0.3.3</td>
    </tr>
    <tr>
        <td>NCP</td>
        <td>v2.5.1</td>
    </tr>
    <tr>
        <td>On-Demand Broker</td>
        <td>v0.38.0</td>
    </tr>
    <tr>
        <td>Percona XtraDB Cluster (PXC)</td>
        <td>v0.22.0</td>
    </tr>
    <tr>
        <td>UAA</td>
        <td>v73.4.21</td>
    </tr>
    <tr>
        <th>Compatibilities</th>
        <th>Versions</th>
    </tr>
    <tr>
        <td>Ops Manager</td>
        <td>See <a href="https://network.pivotal.io/products/pivotal-container-service">Pivotal Network</a></td>
    </tr>
    <tr>
        <td>Xenial stemcells</td>
        <td>See <a href="https://network.pivotal.io/products/pivotal-container-service">Pivotal Network</a></td>
    </tr>
    <tr>
        <td>Windows stemcells</td>
        <td>v2019.15</td>
    </tr>
    <tr>
        <td>Backup and Restore SDK</td>
        <td>v1.17.0</td>
    </tr>
    <tr>
        <td>vSphere</td>
        <td>See <a href="https://www.vmware.com/resources/compatibility/sim/interop_matrix.php">VMware Product Interoperability Matrices</a></td>
    </tr>
    <tr>
        <td>NSX-T</td>
        <td>v2.5.1, v2.5.0<sup>&#42;</sup>, v2.4.3</td>
    </tr>
</table>

<sup>&#42;</sup> <%= vars.recommended_by %> recommends NSX-T v2.5.1 or later for NSX-T v2.5 integration. 

### <a id='1-6-3-upgrade'></a>Upgrade Path

The supported upgrade paths to Enterprise PKS v1.6.3 are from Enterprise PKS v1.5.0 and later.

### <a id='1-6-3-breaking-changes'></a>Breaking Changes

All breaking changes in Enterprise PKS v1.6.3 are also in Enterprise PKS v1.6.0.
See [Breaking Changes in Enterprise PKS v1.6.0](#1-6-0-breaking-changes).

###<a id='1-6-3-known-issues'></a> Known Issues

All known issues in Enterprise PKS v1.6.3 are also in Enterprise PKS v1.6.0.
See [Known Issues in Enterprise PKS v1.6.0](#1-6-0-known-issues).

## <a id="1.6.2"></a> v1.6.2

**Release Date**: April 29, 2020

### <a id="1-6-2-features"></a>Features

New features and changes in this release:

* Bumps Kubernetes to v1.15.10.
* Bumps UAA to v73.4.20.
* Bumps Percona XtraDB Cluster (PXC) to v0.22.
* Bumps Windows Stemcell to v2019.15.
* Bumps ODB to v0.38.0.
* Bumps Apache Tomcat (in PKS API) to v9.0.31.
* [**Security Fix**] UAA bump fixes blind SCIM injection vulnerability, CVE-2019-11282.
* [**Security Fix**] UAA bump fixes CSRF attack vulnerability.
* [**Security Fix**] PXC bump fixes cURL/libcURL buffer overflow vulnerability, CVE-2019-3822.
* [**Bug Fix**] Improves the behavior of the `pks get-kubeconfig` and `pks get-credentials` commands during cluster updates and upgrades. You can now run the `pks get-kubeconfig` command during single- and multi-master cluster updates. Additionally, you can run the `pks get-credentials` command during multi-master cluster upgrades.
* [**Bug Fix**] New UAA version includes Apache Tomcat bump that fixes SAML login issues.

### <a id='1-6-2-snapshot'></a>Product Snapshot

<table class="nice">
    <tr>
        <th>Release</th>
        <th>Details</th>
    </tr>
    <tr>
        <td>Version</td>
        <td>v1.6.2</td>
    </tr>
    <tr>
        <td>Release date</td>
        <td>April 29, 2020</td>
    </tr>
    <tr>
        <th>Component</th>
        <th>Version</th>
    </tr>
    <tr>
        <td>Kubernetes</td>
        <td>v1.15.10</td>
    </tr>
    <tr>
        <td>CoreDNS</td>
        <td>v1.3.1</td>
    </tr>
    <tr>
        <td>Docker</td>
        <td>v18.09.9</td>
    </tr>
    <tr>
        <td>etcd</td>
        <td>v3.3.12</td>
    </tr>
    <tr>
        <td>Metrics Server</td>
        <td>v0.3.3</td>
    </tr>
    <tr>
        <td>NCP</td>
        <td>v2.5.1</td>
    </tr>
    <tr>
        <td>On-Demand Broker</td>
        <td>v0.38.0</td>
    </tr>
    <tr>
        <td>UAA</td>
        <td>v73.4.20</td>
    </tr>
    <tr>
        <th>Compatibilities</th>
        <th>Versions</th>
    </tr>
    <tr>
        <td>Ops Manager</td>
        <td>See <a href="https://network.pivotal.io/products/pivotal-container-service#/releases/631561">Pivotal Network</a></td>
    </tr>
    <tr>
        <td>Xenial stemcells</td>
        <td>See <a href="https://network.pivotal.io/products/pivotal-container-service#/releases/631561">Pivotal Network</a></td>
    </tr>
    <tr>
        <td>Windows stemcells</td>
        <td>v2019.15</td>
    </tr>
    <tr>
        <td>Backup and Restore SDK</td>
        <td>v1.17.0</td>
    </tr>
    <tr>
        <td>vSphere</td>
        <td>See <a href="https://www.vmware.com/resources/compatibility/sim/interop_matrix.php">VMware Product Interoperability Matrices</a></td>
    </tr>
    <tr>
        <td>NSX-T</td>
        <td>v2.5.1, v2.5.0, v2.4.3</td>
    </tr>
</table>

### <a id='1-6-2-upgrade'></a>Upgrade Path

The supported upgrade paths to Enterprise PKS v1.6.2 are from Enterprise PKS v1.5.0 and later.

### <a id='1-6-2-breaking-changes'></a>Breaking Changes

All breaking changes in Enterprise PKS v1.6.2 are also in Enterprise PKS v1.6.0.
See [Breaking Changes in Enterprise PKS v1.6.0](#1-6-0-breaking-changes).

###<a id='1-6-2-known-issues'></a> Known Issues

All known issues in Enterprise PKS v1.6.2 are also in Enterprise PKS v1.6.0.
See [Known Issues in Enterprise PKS v1.6.0](#1-6-0-known-issues).

## <a id="1.6.1"></a> v1.6.1

**Release Date**: Jan 13, 2020

### <a id="1-6-1-features"></a>Features

New features and changes in this release:

* [**Security Fix**] Secures traffic into Kubernetes clusters with up-to-date TLS (v1.2+) and approved cipher suites.
* [**Security Fix**] Bumps UAA to v73.4.16. This update prevents logging of secure information and enables the PKS UAA to start with  the `env.no_proxy` property set.
* [**Bug Fix**] Resolves an issue where if you are using Ops Manager v2.7 and PKS v1.6 as a fresh install, enabling Plans 11, 12, or 13 does not enable Windows worker-based clusters. It creates Linux-based clusters only. For more information,
see [<%= vars.product_short %> Creates a Linux Cluster When You Expect a Windows Cluster](#1-6-0-win-plans).
* [**Bug Fix**] Resolves an issue where applying changes to <%= vars.product_short %> fails
if Plan 8 is enabled in the <%= vars.product_tile %> tile. For more information,
see [Applying Changes Fails If Plan 8 Is Enabled](#1-6-0-plan-8).
* [**Bug Fix**] Resolves an issue where the `pks update-cluster --network-profile` command
sets `subnet_prefix` to 0 in the ncp.ini file if the network profile does not have `pod_subnet_prefix`. For more information,
see [Network Profile for "pks update-cluster" Does Not Use the Defaults from the Original Cluster Manifest](#1-6-0-network-profile).
* [**Bug Fix**] Resolves an issue where trying to create a cluster with a long network profile causes an error `Data too long for column 'nsxt_network_profile'`.
* Updates the supported NCP version to NCP v2.5.1. Refer to the [NCP Release Notes](https://docs.vmware.com/en/VMware-NSX-T-Data-Center/2.5/rn/NSX-Container-Plugin-251-Release-Notes.html) for more information.
* Support for NSX-T v2.5.1.

### <a id='1-6-1-snapshot'></a>Product Snapshot

<table class="nice">
    <tr>
        <th>Release</th>
        <th>Details</th>
    </tr>
    <tr>
        <td>Version</td>
        <td>v1.6.1</td>
    </tr>
    <tr>
        <td>Release date</td>
        <td>January 13, 2020</td>
    </tr>
    <tr>
        <th>Component</th>
        <th>Version</th>
    </tr>
    <tr>
        <td>Kubernetes</td>
        <td>v1.15.5</td>
    </tr>
    <tr>
        <td>CoreDNS</td>
        <td>v1.3.1</td>
    </tr>
    <tr>
        <td>Docker</td>
        <td>v18.09.9</td>
    </tr>
    <tr>
        <td>etcd</td>
        <td>v3.3.12</td>
    </tr>
    <tr>
        <td>Metrics Server</td>
        <td>v0.3.3</td>
    </tr>
    <tr>
        <td>NCP</td>
        <td>v2.5.1</td>
    </tr>
    <tr>
        <td>On-Demand Broker</td>
        <td>v0.29.0</td>
    </tr>
    <tr>
        <td>UAA</td>
        <td>v73.4.16</td>
    </tr>
    <tr>
        <th>Compatibilities</th>
        <th>Versions</th>
    </tr>
    <tr>
        <td>Ops Manager</td>
        <td>See <a href="https://network.pivotal.io/products/pivotal-container-service#/releases/551663">Pivotal Network</a></td>
    </tr>
    <tr>
        <td>Xenial stemcells</td>
        <td>See <a href="https://network.pivotal.io/products/pivotal-container-service#/releases/551663">Pivotal Network</a></td>
    </tr>
    <tr>
        <td>Windows stemcells</td>
        <td>v2019.7</td>
    </tr>
    <tr>
        <td>Backup and Restore SDK</td>
        <td>v1.17.0</td>
    </tr>
    <tr>
        <td>vSphere</td>
        <td>See <a href="https://www.vmware.com/resources/compatibility/sim/interop_matrix.php">VMware Product Interoperability Matrices</a></td>
    </tr>
    <tr>
        <td>NSX-T</td>
        <td>v2.5.1, v2.5.0, v2.4.3</td>
    </tr>
</table>

### <a id='1-6-1-upgrade'></a>Upgrade Path

The supported upgrade paths to Enterprise PKS v1.6.1 are from Enterprise PKS v1.5.0 and later.

### <a id='1-6-1-breaking-changes'></a>Breaking Changes

All breaking changes in Enterprise PKS v1.6.1 are also in Enterprise PKS v1.6.0.
See [Breaking Changes in Enterprise PKS v1.6.0](#1-6-0-breaking-changes).

###<a id='1-6-1-known-issues'></a> Known Issues

All known issues in Enterprise PKS v1.6.1 are also in Enterprise PKS v1.6.0.
See [Known Issues in Enterprise PKS v1.6.0](#1-6-0-known-issues).

## <a id="1.6.0"></a> v1.6.0

**Release Date**: November 14, 2019

### <a id="1-6-0-features"></a>Features

This section describes new features and changes in this release.

#### <a id="pks-core"></a> PKS Control Plane and API

<%= vars.product_short %> v1.6.0 updates include:

* Enables operators to upgrade multiple Kubernetes clusters simultaneously and
to designate specific upgrade clusters as canary clusters.
For more information about multiple cluster upgrades, see
[Upgrade Clusters](upgrade-clusters.html#upgrade-clusters) in _Upgrading Clusters_.
* Adds a new UAA scope, `pks.clusters.admin.read`, for <%= vars.product_short %> users.
For information about UAA scopes,
see [UAA Scopes for <%= vars.product_short %> Users](uaa-scopes.html) and [Managing <%= vars.product_short %> Users with UAA](manage-users.html).
* Provides experimental integration with Tanzu Mission Control. For more information, see [Tanzu Mission Control Integration](./installing-nsx-t.html#tmc).
* Enables operators to limit the total number of clusters a user can provision in <%= vars.product_short %>.
For more information about quotas, see [Managing Resource Usage with Quotas](resource-usage.html) and [Viewing Usage Quotas](resource-review.html).
* Enables operators to configure a single Kubernetes cluster with a specific Docker
Registry CA certificate. For more information about configuring a cluster with a Docker
Registry CA certificate, see [Configuring <%= vars.product_short %> Clusters with Private Docker Registry CA Certificates (Beta)]
(docker-custom-ca-certs.html).
* Updates the `pks delete-cluster` PKS CLI command so that all cluster objects, including NSX-T networking objects, are deleted without the need to use the `bosh delete deployment` command to remove failed cluster deletions.

#### <a id="bosh-lifecycle"></a> Kubernetes Control Plane

<%= vars.product_short %> v1.6.0 updates include:

* Increases the **Worker VM Max in Flight** default value from `1` to `4` in the **PKS API** configuration pane, 
which accelerates cluster creation by allowing up to four new nodes to be provisioned simultaneously.
The updated default value is only applied during new <%= vars.product_short %> installation
and is not applied during an <%= vars.product_short %> upgrade.
If you are upgrading <%= vars.product_short %> from a previous version and want to accelerate multi-cluster provisioning, 
you can increase the value of **Worker VM Max in Flight** manually. 

#### <a id="logging-monitoring"></a> PKS Monitoring and Logging

<%= vars.product_short %> v1.6.0 updates include:

* Redesigns the **Logging** and **Monitoring** panes of the <%= vars.product_tile %> tile
and renames them to **Host Monitoring** and **In-Cluster Monitoring**. For information about configuring these panes, see
the _Installing <%= vars.product_short %>_ topic for your IaaS.
* Adds the **Max Message Size** field in the **Host Monitoring** pane. 
This allows you to configure the maximum number of characters of a log message that is forwarded to a syslog endpoint. 
This feature helps ensure that log messages are not truncated at the syslog endpoint. 
By default, the **Max Message Size** field is 10,000 characters. 
For more information, see [Host Monitoring](installing-pks-vsphere.html#syslog) in the _Installing <%= vars.product_short %>_ topic for your IaaS.  
* Adds the **Include kubelet metrics** setting. This enables operators to collect workload metrics across all Kubernetes clusters.
For more information, see [Host Monitoring](installing-pks-vsphere.html#syslog) in the _Installing <%= vars.product_short %>_ topic for your IaaS.
* Adds support for Fluent Bit output plugins to log sinks.
For information about configuring Fluent Bit output plugins, see [Create a ClusterLogSink or LogSink Resource with a Fluent Bit Output Plugin](create-sinks.html#fluentbit-output-plugin)
in _Creating and Managing Sink Resources_.
* Adds support for filtering logs and events from a `ClusterLogSink` or `LogSink` resource. For more information, see [Filter Sinks](create-sinks.html#filter-log-sinks) in _Creating and Managing Sink Resources_.  

####<a id="windows-features"></a> Windows on PKS

<%= vars.product_short %> v1.6.0 updates include:

* Adds support for floating Windows stemcells on vSphere.
For information about Kubernetes clusters with Windows workers in <%= vars.product_short %>,
see [Configuring Windows Worker-Based Kubernetes Clusters (Beta)](windows-pks-beta.html).
* Enables operators to configure the location of the Windows pause image.
For information about configuring **Kubelet customization - Windows pause image location**, see
[Plans](windows-pks-beta.html#plans) in _Configuring Windows Worker-Based Kubernetes Clusters (Beta)_.

####<a id="nsx-t-features"></a> PKS with NSX-T Networking

<%= vars.product_short %> v1.6.0 updates include:

* NSX Error CRD lets cluster managers and users view NSX errors in Kubernetes resource annotations, and use the command `kubectl get nsxerror` to view the health status of NSX-T cluster networking objects (NCP v2.5.0+). For more information,
see [Viewing the Health Status of Cluster Networking Objects (NSX-T only)](nsxt-health.html).
* DFW log control for dropped traffic lets cluster administrators define network profile to turn on logging and log any dropped or rejected packet by NSX-T distributed firewall rules (NCP v2.5.0+). For more information,
see [Defining Network Profiles for NCP Logging](network-profiles-ncp-logerr.html).
* Load balancer and ingress resource capacity observability using the NSXLoadBalancerMonitor CRD lets cluster managers and users use the command `kubectl get nsxLoadBalancerMonitors` to view a health score that reflects the current performance of the NSX-T load balancer service, including usage, traffic, and current status (NCP v2.5.1+). For more information,
see [Ingress Scaling (NSX-T only)](nsxt-ingress-scale.html).
* Ingress scale out using the LoadBalancer CRD lets cluster managers scale out the NSX-T load balancer for ingress routing (NCP v2.5.1+). For more information, see [Ingress Scaling (NSX-T only)](nsxt-ingress-scale.html).
* Support for Ingress URL Rewrite. For more information, see [Using Ingress URL Rewrite](nsxt-ingress-rewrite-url.html).
* Support for Active–Active Tier-0 router configuration when using a [Shared-Tier-1 topology](./network-profiles-shared-t1.html).
* Ability to place the load balancer and Tier-1 Active/Standby routers on different failure domains. See [Multisite Deployment of NSX-T Data Center](https://docs.vmware.com/en/VMware-NSX-T-Data-Center/2.5/administration/GUID-5D7E3D43-6497-4273-99C1-77613C36AD75.html) for more information.

####<a id="aws-features"></a> PKS on AWS Networking

<%= vars.product_short %> v1.6.0 updates include:

* Support for HTTP/HTTPS Proxy on AWS. For more information see, [Using Proxies with <%= vars.product_short %> on AWS](proxies-aws.html).  


#### <a id="telemetry"></a> Customer Experience Improvement Program

<%= vars.product_short %> v1.6.0 updates include:

* Administrators can name <%= vars.product_short %> installations so they are more easily recognizable in reports. For more information, see [Sample Reports](./telemetry.html#sample-reports).

#### <a id="component-updates"></a>Component Updates

<%= vars.product_short %> v1.6.0 updates include:

* Bumps Kubernetes to v1.15.5.
* Bumps UAA to v73.4.8.
* Bumps Jackson dependencies in the PKS API.

####<a id="bug-fixes"></a> Bug Fixes

<%= vars.product_short %> v1.6.0 includes the following bug fixes:

* Fixes an issue where enabling the Availability Sets mode at the BOSH Director > Azure Config resulted in the kubelet failing to start on provisioning of a Kubernetes cluster.
* Fixes an issue where persistent volume attachment failed on vSphere in a scenario where an AZ defined in Ops Manager does not contain a resource pool.
* Increases `network_profile` column size.
* Fixes a Telemetry event generation issue where the `upgrade_cluster_end` event is not sent for completed cluster upgrades.  
* Fixes an issue where networking changes did not propagate when upgrading from <%= vars.product_short %> v1.5 or later.  
* Fixes an issue where the Ingress IP address was excluded from the <%= vars.product_short %> floating IP pool.  
* Fixes an issue where the PKS OSB Proxy start was delayed by scanning all NSX-T firewall rules.
* Fixes an issue with the PKS clusters upgrade errand not pushing the latest NSX-T certificate to Kubernetes Master nodes.
* Fixes an issue with the PKS OSB Proxy taking a long time to start due to scanning all NSX-T firewall rules.
* Fixes an issue with PKS releasing floating IP addresses incompletely while deleting clusters under active/active mode.
* Fixes an issue with the DNS Lookup Feature: INGRESS IP not kept out of PKS Floating IP pool.
* Fixes an issue with the command `pks cluster details` does not display NS Group ID of master VMs.
* Checks the high availability mode of the Tier-0 router before creating PKS a cluster.  

### <a id='1-6-0-snapshot'></a>Product Snapshot

<table class="nice">
    <tr>
        <th>Release</th>
        <th>Details</th>
    </tr>
    <tr>
        <td>Version</td>
        <td>v1.6.0</td>
    </tr>
    <tr>
        <td>Release date</td>
        <td>November 14, 2019</td>
    </tr>
    <tr>
        <th>Component</th>
        <th>Version</th>
    </tr>
    <tr>
        <td>Kubernetes</td>
        <td>v1.15.5</td>
    </tr>
    <tr>
        <td>CoreDNS</td>
        <td>v1.3.1</td>
    </tr>
    <tr>
        <td>Docker</td>
        <td>v18.09.9</td>
    </tr>
    <tr>
        <td>etcd</td>
        <td>v3.3.12</td>
    </tr>
    <tr>
        <td>Metrics Server</td>
        <td>v0.3.3</td>
    </tr>
    <tr>
        <td>NCP</td>
        <td>v2.5.1</td>
    </tr>
    <tr>
        <td>On-Demand Broker</td>
        <td>v0.29.0</td>
    </tr>
    <tr>
        <td>UAA</td>
        <td>v73.4.8</td>
    </tr>
    <tr>
        <th>Compatibilities</th>
        <th>Versions</th>
    </tr>
    <tr>
        <td>Ops Manager</td>
        <td>See <a href="https://network.pivotal.io/products/pivotal-container-service#/releases/501833">Pivotal Network</a></td>
    </tr>
    <tr>
        <td>Xenial stemcells</td>
        <td>See <a href="https://network.pivotal.io/products/pivotal-container-service#/releases/501833">Pivotal Network</td>
    </tr>
    <tr>
        <td>Windows stemcells</td>
        <td>v2019.7</td>
    </tr>
    <tr>
        <td>Backup and Restore SDK</td>
        <td>v1.17.0</td>
    </tr>
    <tr>
        <td>vSphere</td>
        <td>See <a href="https://www.vmware.com/resources/compatibility/sim/interop_matrix.php">VMware Product Interoperability Matrices</a></td>
    </tr>
    <tr>
        <td>NSX-T</td>
        <td>v2.5.0, v2.4.3</td>
    </tr>
</table>

### <a id='1-6-0-upgrade'></a>Upgrade Path

The supported upgrade paths to <%= vars.product_short %> v1.6.0 are from <%= vars.product_short %> v1.5.0 and later.

### <a id='1-6-0-breaking-changes'></a>Breaking Changes

<%= vars.product_short %> v1.6.0 has the following breaking changes:

#### <a id='1-6-0-volume-data-loss'></a> Persistent Volume Data Loss with Worker Reboot

With old versions of Ops Manager, PKS worker nodes with persistent disk volumes may get stuck in a startup state and lose data when they are rebooted manually from the dashboard or automatically by vSphere HA.

This issue is fixed in the following Ops Manager versions:

- v2.8.0+
- v2.7.6+
- v2.6.16+

For all PKS installations that host workers using persistent volumes, Pivotal recommends upgrading to one of the Ops Manager versions above.

#### <a id='1-6-0-sink-cli-deprecation'></a> <%= vars.product_short %> Removes Sink Commands in the PKS CLI

<%= vars.product_short %> removes the following <%= vars.product_short %> Command Line Interface (PKS CLI) commands:

* `pks create-sink`
* `pks sinks`
* `pks delete-sink`

You can use the following Kubernetes CLI commands instead:

* `kubectl apply -f YOUR-SINK.yml`
* `kubectl get clusterlogsinks`
* `kubectl delete clusterlogsink YOUR-SINK`

For more information about defining and managing sink resources,
see [Creating and Managing Sink Resources](create-sinks.html).

#### <a id='api-endpoints'></a> Changes to PKS API Endpoints

This release moves the `clusters`, `compute-profiles`, `quotas`, and `usages` PKS API endpoints from `v1beta1` to `v1`.
`v1beta1` is no longer supported for these endpoints. You must use `v1`.
For example, instead of `https://YOUR-PKS-API-FQDN:9021/v1beta1/quotas`, use `https://YOUR-PKS-API-FQDN:9021/v1/quotas`.

###<a id='1-6-0-known-issues'></a> Known Issues

<%= vars.product_short %> v1.6.0 has the following known issues.

#### <a id='1-6-0-tls-kube-cert'></a> Your Kubernetes API Server CA Certificate Expires Unless You Regenerate It

**Symptom**

Your Kubernetes API server's `tls-kubernetes-2018` certificate is a one-year certificate
instead of a four-year certificate.

**Explanation**

When you upgraded from PKS v1.2.7 to PKS v1.3.1, the upgrade process extended the lifespan of all PKS CA certificates to four years, except for the Kubernetes API server's `tls-kubernetes-2018` certificate.  The `tls-kubernetes-2018` certificate remained a one-year certificate.

Unless you regenerate the `tls-kubernetes-2018` certificate it retains its one-year lifespan, even through subsequent <%= vars.product_short %> upgrades.

**Workaround**

If you have not already done so, you should replace the Kubernetes API server's one-year `tls-kubernetes-2018` certificate before it expires.
For information about generating and applying a new four-year `tls-kubernetes-2018` certificate, see
[How to regenerate tls-kubernetes-2018 certificate when it is not regenerated in the upgrade to PKS v1.3.x]
(https://community.pivotal.io/s/article/How-to-regenerate-tls-kubernetes-2018-certificate-when-it-was-not-regenerated-in-the-upgrade-to-PKS-v1-3-x) in the Pivotal Knowledge Base.

#### <a id="1-6-0-win-k8s-version"></a> Cluster Upgrade Does Not Upgrade Kubernetes Version on Windows Workers

When PKS clusters are upgraded, Windows worker nodes in the cluster do not upgrade their Kubernetes version.  The master and Linux worker nodes in the cluster do upgrade their Kubernetes version as expected.

When the Kubernetes version of a Windows worker does not exactly match the version of the master node, the cluster still functions. [`kube-apiserver`](https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/) has no restriction on lagging patch bumps.

PKS clusters upgrade manually with the `pks upgrade-cluster` command, or automatically with PKS upgrades when the **Upgrade all clusters** errand is set to **Default (On)** in the PKS tile **Errands** pane.

#### <a id="1-6-0-network-profile"></a>Network Profile for "pks update-cluster" Does Not Use the Defaults from the Original Cluster Manifest

<p class="note"><strong>Note:</strong> This issue is resolved in <%= vars.product_short %> v1.6.1.</p>

**Symptom**

The Network profile for `pks update-cluster` uses contents that are being updated and 
not using the defaults from the original cluster manifest.

**Explanation**

The `pks update-cluster` operation sets the `subnet_prefix` to 0 in the ncp.ini file when 
the network-profile has `pod_ip_block_ids` set but it does not have `pod_subnet_prefix`.

**Workaround**

When creating the network profile to be used for update, include all the below fields. 
Then update-cluster with the network profile should work.

```
{
  	"name": "np",
	"parameters": {
		"t0_router_id": "c501f114-870b-4eda-99ac-966adf464452",
		"fip_pool_ids": ["b7acbda8-46de-4195-add2-5fb11ca46cbf"],
		"pod_ip_block_ids": ["b03bff60-854b-4ccb-9b2b-016867b319c9","234c3652-69e7-4365-9627-8e0d8d4a6b86"],
		"pod_subnet_prefix": 24,
		"single_tier_topology": false
	}
}
```

#### <a id="1-6-0-security-group"></a>Azure Default Security Group Is Not Automatically Assigned to Cluster VMs

**Symptom**

You experience issues when configuring a load balancer for a multi-master Kubernetes cluster or creating a service of type `LoadBalancer`.
Additionally, in the Azure portal, the **VM** > **Networking** page does not display
any inbound and outbound traffic rules for your cluster VMs.

**Explanation**

As part of configuring the <%= vars.product_tile %> tile for Azure, you enter **Default Security Group** in the **Kubernetes Cloud Provider** pane.
When you create a Kubernetes cluster, <%= vars.product_short %> automatically assigns this security group to each VM in the cluster.
However, on Azure the automatic assignment may not occur.

As a result, your inbound and outbound traffic rules defined in the security group are not applied to the cluster VMs.

**Workaround**

If you experience this issue, manually assign the default security group to each VM NIC in your cluster.

#### <a id='1-6-0-cluster-creation-fails'></a>Cluster Creation Fails When First AZ Runs Out of Resources

**Symptom**

If the first availability zone (AZ) used by a plan with multiple AZs runs out of
resources, cluster creation fails with an error like the following:

<pre class="terminal">
L Error: CPI error 'Bosh::Clouds::CloudError' with message 'No valid placement found for requested memory: 4096
</pre>

**Explanation**

BOSH creates VMs for your <%= vars.product_short %> deployment using a round-robin
algorithm, creating the first VM in the first AZ that your plan uses.
If the AZ runs out of resources, cluster creation fails because BOSH cannot create
the cluster VM.

For example, if you have three AZs and you
create two clusters with four worker VMs each, BOSH deploys VMs in the
following AZs:

<table>
  <tr>
    <th></th>
    <th>AZ1</th>
    <th>AZ2</th>
    <th>AZ3</th>
  </tr>
  <tr>
    <th>Cluster 1</th>
    <td>Worker VM 1</td>
    <td>Worker VM 2</td>
    <td>Worker VM 3</td>
  </tr>
  <tr>
    <td></td>
    <td>Worker VM 4</td>
    <td></td>
    <td></td>
  </tr>
  <tr>
    <th>Cluster 2</th>
    <td>Worker VM 1</td>
    <td>Worker VM 2</td>
    <td>Worker VM 3</td>
  </tr>
  <tr>
    <td></td>
    <td>Worker VM 4</td>
    <td></td>
    <td></td>
  </tr>
</table>

In this scenario, AZ1 has twice as many VMs as AZ2 or AZ3.

#### <a id='1-6-0-azure-worker-node'></a>Cluster Creation Fails with Long Network Profile

<p class="note"><strong>Note:</strong> This issue is resolved in <%= vars.product_short %> v1.6.1.</p>

Creating a cluster with a long network profile, such as with multiple `pod_ip_block_ids` values, causes an error `Data too long for column 'nsxt_network_profile'`.

#### <a id='1-6-0-azure-worker-node'></a>Azure Worker Node Communication Fails after Upgrade

**Symptom**

Outbound communication from a worker node VM fails after upgrading <%= vars.product_short %>.

**Explanation**

<%= vars.product_short %> uses Azure Availability Sets to improve the uptime of workloads and worker nodes in the event of Azure platform failures. Worker node
VMs are distributed evenly across Availability Sets.

Azure Standard SKU Load Balancers are recommended for the Kubernetes control plane and Kubernetes ingress and egress. This load balancer type provides an IP address for outbound communication using SNAT.

During an upgrade, when BOSH rebuilds a given worker instance in an Availability Set,
Azure can time out while re-attaching the worker node network interface to the
back-end pool of the Standard SKU Load Balancer.

For more information, see [Outbound connections in Azure](https://docs.microsoft.com/en-us/azure/load-balancer/load-balancer-outbound-connections) in the Azure documentation.

**Workaround**

You can manually re-attach the worker instance to the back-end pool of the Azure Standard SKU Load Balancer in your Azure console.

#### <a id='1-6-0-timeout'></a> Error During Individual Cluster Upgrades

**Symptom**

While submitting a large number of cluster upgrade requests using the `pks upgrade-cluster` command, some of your Kubernetes clusters are marked as failed.

**Explanation**

BOSH upgrades Kubernetes clusters in parallel with a limit of up to four concurrent cluster upgrades by default.
If you schedule more than four cluster upgrades,
<%= vars.product_short %> queues the upgrades and waits for BOSH to finish the last upgrade.
When BOSH finishes the last upgrade, it starts working on the next upgrade request.

If you submit too many cluster upgrades to BOSH, an error may occur,
where some of your clusters are marked as `FAILED` because BOSH can start the upgrade only within the specified timeout.
The timeout is set to 168 hours by default.
However, BOSH does not remove the task from the queue or stop working on the upgrade if it has been picked up.

**Solution**

If you expect that upgrading all of your Kubernetes clusters takes more than 168 hours,
do not use a script that submits upgrade requests for all of your clusters at once.
For information about upgrading Kubernetes clusters provisioned by <%= vars.product_short %>,
see [Upgrading Clusters](./upgrade-clusters.html).

#### <a id="1-6-0-kubectl-azs"></a> Kubectl CLI Commands Do Not Work after Changing an Existing Plan to a Different AZ

**Symptom**

After you update the AZ of an existing plan, kubectl CLI commands do not work for your clusters associated with the plan.

**Explanation**

This issue occurs in IaaS environments that do not support attaching a disk across multiple AZs.

When the plan of an existing cluster changes to a different AZ,
BOSH migrates the cluster by creating VMs for the cluster in the new AZ and
removing your cluster VMs from the original AZ.

On an IaaS that does not support attaching VM disks across AZs,
the disks BOSH attaches to the new VMs do not have the original content.

**Workaround**

If you cannot run kubectl CLI commands after reconfiguring the AZ of an existing cluster, contact Support for assistance.

#### <a id='1-6-0-plan-8'></a> Applying Changes Fails If Plan 8 Is Enabled

<p class="note"><strong>Note:</strong> This issue is resolved in <%= vars.product_short %> v1.6.1.</p>

**Symptom**

After you click **Apply Changes** on the Ops Manager Installation Dashboard,
the following error occurs: `Cannot generate manifest for product Enterprise PKS`.

**Explanation**

This error occurs if Plan 8 is enabled in your <%= vars.product_tile %> v1.6.0 tile.

**Workaround**

Disable Plan 8 in the <%= vars.product_tile %> tile and
move your plan settings to a plan that is available for configuration, for example, Plan 9 or 10.

To disable Plan 8:

1. In **Plan 8**, select **Plan > Inactive**.
1. Click **Save**.

#### <a id='1-6-0-uuid-length'></a> One Plan ID Longer than Other Plan IDs

**Symptom**

One of your plan IDs is one character longer than your other plan IDs.

**Explanation**

In <%= vars.product_short %>, each plan has a unique plan ID.
A plan ID is normally a UUID consisting of 32 alphanumeric characters and 4 hyphens.
However, the **Plan 4** ID consists of 33 alphanumeric characters and 4 hyphens.

**Solution**

You can safely configure and use **Plan 4**.
The length of the **Plan 4** ID does not affect the functionality of **Plan 4** clusters.

If you require all plan IDs to have identical length, do not activate or use **Plan 4**.

#### <a id='1-6-0-tmc-names'></a> Kubernetes Cluster Name Limitation for Tanzu Mission Control Integration

Tanzu Mission Control integration cannot attach Tanzu Mission Control to Kubernetes clusters that have uppercase letters in their names.

**Symptom**

Clusters that you create with `pks create-cluster` do not appear in the Tanzu Mission Control, even though you configured Tanzu Mission Control integration as described in [Integrate Tanzu Mission Control](./installing-nsx-t.html#tmc).

**Explanation**

The regex pattern that parses cluster names in Tanzu Mission Control integration fails with names that contain uppercase letters.

**Solution**

When running `pks create-cluster` to create clusters that you want to track in Tanzu Mission Control, pass in names that contain only lowercase letters and numbers.

#### <a id="1-6-0-win-plans"></a> <%= vars.product_short %> Creates a Linux Cluster When You Expect a Windows Cluster

<p class="note"><strong>Note:</strong> This issue is resolved in <%= vars.product_short %> v1.6.1.</p>

**Symptom**

When you create an <%= vars.product_short %> cluster using either Plan 11, 12 or 13 the cluster is created as a Linux cluster instead of a Windows cluster.

**Explanation**  

When you create an <%= vars.product_short %> cluster using either Plan 11, 12 or 13 a Windows cluster should be created. 
If you are using <%= vars.product_short %> v1.6 with Operations Manager v2.7 a Linux cluster is created instead.

#### <a id="1-6-0-ldap-verifier"></a> Saving UAA Tab Settings Fails With Error: 'InvalidURIError bad URI'

**Symptom**

When you save your UAA tab with **LDAP Server** selected and multiple LDAP servers specified, 
you receive the error: `URI::InvalidURIError bad URI(is not URI?):LDAP URLs`.

**Explanation** 

When you configure the UAA tab with multiple LDAP servers your settings will fail to validate 
when using the following Ops Manager releases:  
 
<table>
  <tr>
    <th width=25%>Ops Manager Version</th>
    <th>Affected Releases</th>
  </tr>
  <tr>
    <th>Ops Manager v2.6</th>
    <td>Ops Manager v2.6.18 and earlier patch releases.</td>
  </tr>
  <tr>
    <th>Ops Manager v2.7</th>
    <td>All patch releases.</td>
  </tr>
  <tr>
    <th>Ops Manager v2.8</th>
    <td>All patch releases.</td>
  </tr>
</table>
<br>

**Workaround**

To resolve this issue see the following:  

<table>
  <tr>
    <th width=25%>Ops Manager Version</th>
    <th>Workaround</th>
  </tr>
  <tr>
    <th>Ops Manager v2.6</th>
    <td>Perform one of the following:  
        <ul>
            <li>Upgrade to Ops Manager v2.6.19 or later v2.6 patch release.</li>
            <li>Complete the procedures in <a href="https://kb.vmware.com/s/article/76495">
        UAA authentication tab in PKS 1.6 fails to save with error “URI::InvalidURIError bad 
       URI(is not URI?):LDAP URLs” (76495)</a> in the Pivotal Support Knowledge Base.</li>
        </ul>
    </td>
  </tr>
  <tr>
    <th>Ops Manager v2.7</th>
    <td>Complete the procedures in <a href="https://kb.vmware.com/s/article/76495">
        UAA authentication tab in PKS 1.6 fails to save with error “URI::InvalidURIError bad 
       URI(is not URI?):LDAP URLs” (76495)</a> in the Pivotal Support Knowledge Base.
   </td>
  </tr>
  <tr>
    <th>Ops Manager v2.8</th>
    <td>Complete the procedures in <a href="https://kb.vmware.com/s/article/76495">
        UAA authentication tab in PKS 1.6 fails to save with error “URI::InvalidURIError bad 
       URI(is not URI?):LDAP URLs” (76495)</a> in the Pivotal Support Knowledge Base.
   </td>
  </tr>
</table>

#### <a id="1-6-0-windows-cluster-upgrade-failure"></a> Windows Worker Clusters Fail to Upgrade to v1.6 

**Symptoms**

During your upgrade from <%= vars.product_short %> v1.5 to <%= vars.product_short %> v1.6 
a Windows worker VM fails to upgrade, as evidenced by:

- The command line outputs an error `Failed jobs: docker-windows`.
- The Windows worker VM disappears from the output of `kubectl get nodes`.
- The command line shows the status `failed` and the action `UPGRADE` for the cluster that contains the worker.
- The log shows an entry `\docker\dockerd.exe: Access is denied`.

**Explanation** 

Between PKS v1.5 and v1.6, the name of the Docker service changed from `docker` to `docker-windows`, 
but your environment continues to use the old Docker service name and paths. 
The incompatible service name and pathing causes a Windows worker upgrade to fail.

If your cluster has multiple Windows workers, this issue does not incur downtime .
Before BOSH attempts to upgrade a Windows worker, it moves the worker's apps to other Windows workers in the cluster.
When the upgrade fails, BOSH stops the cluster upgrade process and the other Windows workers continue running at the earlier version.

**Workaround**

After upgrading to <%= vars.product_short %> v1.6 and your Windows worker clusters have failed to upgrade, 
complete the following steps:

1. Upload a vSphere stemcell v2019.8 or later for Windows Server version 2019 to your <%= vars.product_tile %> tile.  
1. To upgrade your Windows worker clusters, perform one of the following:
    * Enable the **Upgrade all clusters errand** setting and deploy the PKS tile. 
    For more information about configuring the **Upgrade all clusters errand** and deploying the <%= vars.product_tile %> tile, see 
    [Modify Errand Configuration in the <%= vars.product_tile %> Tile](upgrade-clusters.html#disable-errand) 
    in _Upgrading Clusters_.
    * Run `pks upgrade-cluster` or `pks upgrade-clusters` on your failed Windows worker cluster(s). 
    For more information about upgrading specific <%= vars.product_short %> clusters, see [Upgrade Clusters](upgrade-clusters.html#upgrade-clusters) 
    in _Upgrading Clusters_.

#### <a id="1-6-0-oidc-response-header"></a>502 Bad Gateway After OIDC Login

**Symptom**

You experience a "502 Bad Gateway" error from the NSX load balancer after you log in to OIDC. 

**Explanation**

A large response header has exceeded your NSX-T load balancer maximum 
response header size. The default maximum response header size is 10,240 characters and should 
be resized to 50,000. 

**Workaround**

If you experience this issue, manually reconfigure your NSX-T `request_header_size` 
and `response_header_size` to 50,000 characters. 
For information about configuring NSX-T default header sizes, 
see [OIDC Response Header Overflow](https://community.pivotal.io/s/article/OIDC-Response-Header-overflow) in the Pivotal Knowledge Base.

#### <a id='1-6-0-eight-cores-error'></a> NSX-T Pre-Check Errand Fails Due to Edge Node Configuration  

**Symptom**  

You have configured your NSX-T Edge Node VM as `medium` size, 
and the NSX-T Pre-Check Errand fails with the following error: 
"_ERROR: NSX-T Precheck failed due to Edge Node ... no of cpu cores is less than 8_". 

**Explanation**  

The NSX-T Pre-Check Errand is erroneously returning the "_cpu cores is less than 8_" error.

**Solution**  

You can safely configure your NSX-T Edge Node VMs as `medium` size and ignore the error. 

#### <a id='1-6-0-http-proxy-password'></a> Character Limitations in HTTP Proxy Password

For vSphere with NSX-T, the HTTP Proxy password field does not support the following special characters: `&` or `;`.

## <a id="management-console-1.6.3"></a> <%= vars.product_short %> Management Console 1.6.3

**Release Date**: July 28, 2020

### <a id="management-console-1.6.3-features"></a>Features

Other than support for <%= vars.product_short %> v1.6.3, <%= vars.product_short %> Management Console 1.6.3 has no new features.

### <a id="management-console-1.6.3-bug-fixes"></a>Bug Fixes

<%= vars.product_short %> Management Console 1.6.3 includes no bug fixes.

### <a id="management-console-1.6.3-snapshot"></a>Product Snapshot

<p class="note"><strong>Note</strong>: <%= vars.product_short %> Management Console provides an opinionated installation of <%= vars.product_short %>. The supported versions may differ from or be more limited than what is generally supported by <%= vars.product_short %>.</p>

<table class="nice">
    <th>Element</th>
    <th>Details</th>
    <tr>
      <td>Version</td>
      <td>v1.6.3</td>
    </tr>
    <tr>
      <td>Release date</td>
      <td>July 28, 2020</td>
    </tr>
    <tr>
      <td>Installed Enterprise PKS version</td>
      <td>v1.6.3</td>
    </tr>
    <tr>
      <td>Installed Ops Manager version</td>
      <td>v2.8.10</td>
    </tr>
    <tr>
      <td>Installed Kubernetes version</td>
        <td>v1.15.12</td>
    </tr>
    <tr>
      <td>Compatible NSX-T versions</td>
      <td>v2.5.1, v2.5.0<sup>&#42;</sup>, v2.4.3</td>
    </tr>
    <tr>
      <td>Installed Harbor Registry version</td>
      <td>v1.9.4</td>
    </tr>
</table>

<sup>&#42;</sup> <%= vars.recommended_by %> recommends NSX-T v2.5.1 or later for NSX-T v2.5 integration. 

### <a id="management-console-1.6.3-known-issues"></a> Known Issues

The <%= vars.product_short %> Management Console v1.6.3 appliance and user interface have the same [known issues](#management-console-1.6.2-known-issues) as v1.6.2.

## <a id="management-console-1.6.2"></a> <%= vars.product_short %> Management Console 1.6.2

**Release Date**: April 29, 2020

### <a id="management-console-1.6.2-features"></a>Features

Other than support for <%= vars.product_short %> v1.6.2, <%= vars.product_short %> Management Console 1.6.2 has no new features.

### <a id="management-console-1.6.2-bug-fixes"></a>Bug Fixes

<%= vars.product_short %> Management Console 1.6.2 includes no bug fixes.

### <a id="management-console-1.6.2-snapshot"></a>Product Snapshot

<p class="note"><strong>Note</strong>: <%= vars.product_short %> Management Console provides an opinionated installation of <%= vars.product_short %>. The supported versions may differ from or be more limited than what is generally supported by <%= vars.product_short %>.</p>

<table class="nice">
    <th>Element</th>
    <th>Details</th>
    <tr>
      <td>Version</td>
      <td>v1.6.2</td>
    </tr>
    <tr>
      <td>Release date</td>
      <td>April 29, 2020</td>
    </tr>
    <tr>
      <td>Installed Enterprise PKS version</td>
      <td>v1.6.2</td>
    </tr>
    <tr>
      <td>Installed Ops Manager version</td>
      <td>v2.8.5</td>
    </tr>
    <tr>
      <td>Installed Kubernetes version</td>
        <td>v1.15.10</td>
    </tr>
    <tr>
      <td>Compatible NSX-T versions</td>
      <td>v2.5.0, v2.4.3</td>
    </tr>
    <tr>
      <td>Installed Harbor Registry version</td>
      <td>v1.9.4</td>
    </tr>
</table>

### <a id="management-console-1.6.2-known-issues"></a> Known Issues

The <%= vars.product_short %> Management Console v1.6.2 appliance and user interface have the same [known issues](#management-console-1.6.1-known-issues) as v1.6.1.

## <a id="management-console-1.6.1"></a> <%= vars.product_short %> Management Console 1.6.1

**Release Date**: January 23, 2020

### <a id="management-console-1.6.1-features"></a>Features

Other than support for <%= vars.product_short %> v1.6.1, <%= vars.product_short %> Management Console 1.6.1 has no new features.

### <a id="management-console-1.6.1-bug-fixes"></a>Bug Fixes

<%= vars.product_short %> Management Console 1.6.1 includes no bug fixes.

### <a id="management-console-1.6.1-snapshot"></a>Product Snapshot

<p class="note"><strong>Note</strong>: <%= vars.product_short %> Management Console provides an opinionated installation of <%= vars.product_short %>. The supported versions may differ from or be more limited than what is generally supported by <%= vars.product_short %>.</p>

<table class="nice">
    <th>Element</th>
    <th>Details</th>
    <tr>
      <td>Version</td>
      <td>v1.6.1</td>
    </tr>
    <tr>
      <td>Release date</td>
      <td>January 23, 2020</td>
    </tr>
    <tr>
      <td>Installed Enterprise PKS version</td>
      <td>v1.6.1</td>
    </tr>
    <tr>
      <td>Installed Ops Manager version</td>
      <td>v2.8.0</td>
    </tr>
    <tr>
      <td>Installed Kubernetes version</td>
        <td>v1.15.5</td>
    </tr>
    <tr>
      <td>Compatible NSX-T versions</td>
      <td>v2.5.0, v2.4.3</td>
    </tr>
    <tr>
      <td>Installed Harbor Registry version</td>
      <td>v1.9.3</td>
    </tr>
</table>

### <a id="management-console-1.6.1-known-issues"></a> Known Issues

The <%= vars.product_short %> Management Console v1.6.1 appliance and user interface have the same [known issues](#management-console-1.6.0-rev2-known-issues) as v1.6.0-rev.3 and v1.6.0-rev.2.
    
## <a id="management-console-1.6.0-rev3"></a> <%= vars.product_short %> Management Console 1.6.0-rev.3

**Release Date**: December 19, 2019

<p class="note warning"><strong> IMPORTANT:</strong> The <%= vars.product_short %> Management Console 1.6.0-rev.3 offline patch can only be applied in an air-gapped environment. It can only be applied to 1.6.0-rev.2 and not to any other version. For information about how to apply the patch, see <a href="./console/patch-console-components.html">Patch Enterprise PKS Management Console Components</a>.</p>

### <a id="management-console-1.6.0-rev3-features"></a>Features

<%= vars.product_short %> Management Console 1.6.0-rev.3 has no new features.

### <a id="management-console-1.6.0-rev3-bug-fixes"></a>Bug Fixes

<%= vars.product_short %> Management Console 1.6.0-rev.3 includes the following bug fixes:

- Fixes UI failure caused by multiple datacenters being present in vCenter Server.
- Adds support for both FQDN and IP addresses in LDAP/LDAPS configuration for identity management. 
- Fixes UI freezing after entering unconventionally formatted URLs for SAML provider metadata.
- Adds support for UAA role `pks.clusters.admin.read` in identity Management configuration.
- Adds validation for Harbor FQDN in lower case.
- Fixes misconfigured Wavefront HTTP Proxy when field is left empty.

### <a id="management-console-1.6.0-rev3-snapshot"></a>Product Snapshot

<p class="note"><strong>Note</strong>: <%= vars.product_short %> Management Console provides an opinionated installation of <%= vars.product_short %>. The supported versions may differ from or be more limited than what is generally supported by <%= vars.product_short %>.</p>

<table class="nice">
    <th>Element</th>
    <th>Details</th>
    <tr>
      <td>Version</td>
      <td>v1.6.0-rev.3</td>
    </tr>
    <tr>
      <td>Release date</td>
      <td>December 19, 2019</td>
    </tr>
    <tr>
      <td>Installed Enterprise PKS version</td>
      <td>v1.6.0</td>
    </tr>
    <tr>
      <td>Installed Ops Manager version</td>
      <td>v2.7.3</td>
    </tr>
    <tr>
      <td>Installed Kubernetes version</td>
        <td>v1.15.5</td>
    </tr>
    <tr>
      <td>Compatible NSX-T versions</td>
      <td>v2.5.0, v2.4.3</td>
    </tr>
    <tr>
      <td>Installed Harbor Registry version</td>
      <td>v1.9.3</td>
    </tr>
</table>

### <a id="management-console-1.6.0-rev3-known-issues"></a> Known Issues

With the exception of the [Bug Fixes](#management-console-1.6.0-rev3-bug-fixes) listed above, the <%= vars.product_short %> Management Console v1.6.0-rev.3 appliance and user interface have the same [known issues](#management-console-1.6.0-rev2-known-issues) as v1.6.0-rev.2.

## <a id="management-console-1.6.0-rev2"></a> <%= vars.product_short %> Management Console v1.6.0-rev.2

**Release Date**: November 26, 2019

### <a id="management-console-1.6.0-rev2-features"></a>Features

<%= vars.product_short %> Management Console v1.6.0-rev.2 updates include:

* Provides experimental integration with VMware Tanzu Mission Control. For more information, see [Tanzu Mission Control Integration](./console/deploy-ent-pks-wizard.html#integrations-tanzumc).
* Provides experimental support for plans that use Windows worker nodes. For information, see [Configure Plans](./console/deploy-ent-pks-wizard.html#plans).
* Deploys Harbor registry v1.9. For information, see [Configure Harbor](./console/deploy-ent-pks-wizard.html#harbor).
* Adds support for active-active mode on the tier 0 router in automated-NAT deployments and No-NAT configurations in Bring Your Own Topology deployments.  For information, see [Configure Networking](./console/deploy-ent-pks-wizard.html#networking).
* Adds the ability to configure proxies for the integration with Wavefront. For information, see [Configure a Connection to Wavefront](./console/deploy-ent-pks-wizard.html#integrations-wavefront).
* Adds the ability to configure the size of the PKS API VM. For information, see [Configure Resources and Storage](./console/deploy-ent-pks-wizard.html#storage).
* Allows you to use the management console to upgrade to v1.6.0-rev.2. For information, see [Upgrade <%= vars.product_short %> Management Console](./console/upgrade-console.html).

### <a id="management-console-1.6.0-rev2-snapshot"></a>Product Snapshot

<p class="note"><strong>Note</strong>: Enterprise PKS Management Console provides an opinionated installation of Enterprise PKS. The supported versions may differ from or be more limited than what is generally supported by Enterprise PKS.</p>

<table class="nice">
    <th>Element</th>
    <th>Details</th>
    <tr>
      <td>Version</td>
      <td>v1.6.0-rev.2</td>
    </tr>
    <tr>
      <td>Release date</td>
      <td>November 26, 2019</td>
    </tr>
    <tr>
      <td>Installed Enterprise PKS version</td>
      <td>v1.6.0</td>
    </tr>
    <tr>
      <td>Installed Ops Manager version</td>
      <td>v2.7.3</td>
    </tr>
    <tr>
      <td>Installed Kubernetes version</td>
        <td>v1.15.5</td>
    </tr>
    <tr>
      <td>Compatible NSX-T versions</td>
      <td>v2.5.0, v2.4.3</td>
    </tr>
    <tr>
      <td>Installed Harbor Registry version</td>
      <td>v1.9.3</td>
    </tr>
</table>

### <a id="management-console-1.6.0-rev2-known-issues"></a>Known Issues

The following known issues are specific to the <%= vars.product_short %> Management Console v1.6.0-rev.2 appliance and user interface.

#### <a id="management-console-1.6.0-rev2-yaml-validation"></a>YAML Validation Errors Not Cleared

**Symptom**

If you attempt to upload a YAML configuration file and the deployment fails because of an invalid manifest, <%= vars.product_short %> Management Console displays an error notification with the validation error. If subsequent attempts also fail because of validation issues, the validation errors are appended to each other. 

**Explanation**

The validation errors are not cleared when you resubmit the YAML configuration file.

**Workaround**

None

#### <a id="management-console-1.6.0-rev2-notifications"></a><%= vars.product_short %> Management Console Notifications Persist

**Symptom**

In the **Enterprise PKS** view of <%= vars.product_short %> Management Console, error notifications sometimes persist in memory on the  **Clusters** and **Nodes** pages after you clear those notifications.

**Explanation**

After clicking the **X** button to clear a notification it is removed, but when you navigate back to those pages the notification might show again.

**Workaround**

Use shift+refresh to reload the page.

#### <a id="management-console-1.6.0-rev2-delete-deployment"></a>Cannot Delete <%= vars.product_short %> Deployment from Management Console

**Symptom**

In the **Enterprise PKS** view of <%= vars.product_short %> Management Console, you cannot use the **Delete Enterprise PKS Deployment** option even after you have removed all clusters.

**Explanation**

The option to delete the deployment is only activated in the management console a short period after the clusters are deleted.

**Workaround**

After removing clusters, wait for a few minutes before attempting to use the **Delete Enterprise PKS Deployment** option again.

#### <a id="management-console-1.6.0-rev2-vli-port"></a>Configuring <%= vars.product_short %> Management Console Integration with VMware vRealize Log Insight

**Symptom**

Enterprise PKS Management Console appliance sends logs to VMware vRealize Log Insight over HTTP, not HTTPS.

**Explanation**

When you deploy the Enterprise PKS Management Console appliance from the OVA, if you require log forwarding to vRealize Log Insight, you must provide the port on the vRealize Log Insight server on which it listens for HTTP traffic. Do not provide the HTTPS port.

**Workaround**

Set the vRealize Log Insight port to the HTTP port. This is typically port `9000`.

#### <a id="management-console-1.6.0-rev2-nsxt-flannel-error"></a>Deploying <%= vars.product_short %> to an Unprepared NSX-T Data Center Environment Results in Flannel Error

**Symptom**

When using the management console to deploy <%= vars.product_short %> in **NSX-T Data Center (Not prepared for PKS)** mode, if an error occurs during the network configuration, the message `Unable to set flannel environment` is displayed in the deployment progress page.

**Explanation**

The network configuration has failed, but the error message is incorrect.

**Workaround**

To see the correct reason for the failure, see the server logs. For instructions about how to obtain the server logs, see [Troubleshooting Enterprise PKS Management Console](./console/console-troubleshooting.html).

#### <a id="management-console-1.6.0-rev2-bosh-cli"></a>Using BOSH CLI from Operations Manager VM

**Symptom**

The BOSH CLI client bash command that you obtain from the **Deployment Metadata** view does not work when logged in to the Operations Manager VM.

**Explanation**

The BOSH CLI client bash command from the **Deployment Metadata** view is intended to be used from within the <%= vars.product_short %> Management Console appliance.

**Workaround**

To use the BOSH CLI from within the Operations Manager VM, see [Connect to Operations Manager](./console/console-login-opsmanager.html).

From the Ops Manager VM, use the BOSH CLI client bash command from the **Deployment Metadata** page, with the following modifications:

  * Remove the clause `BOSH_ALL_PROXY=xxx`
  * Replace the `BOSH_CA_CERT` section with `BOSH_CA_CERT=/var/tempest/workspaces/default/root_ca_certificate` 

#### <a id="management-console-1.6.0-rev2-pks-cli"></a>Run `pks` Commands against the PKS API Server

**Explanation**

The PKS CLI is available in the <%= vars.product_short %> Management Console appliance. 

**Workaround**

To be able to run `pks` commands against the PKS API Server, you must first log to PKS using the following command syntax `pks login -a fqdn_of_pks ...`.

To do this, you must ensure either of the following:

  * The FQDN configured for the PKS Server is resolvable by the DNS server configured for the Enterprise PKS Management Console appliance, or
  * An entry that maps the Floating IP assigned to the PKS Server to the FQDN exists on /etc/hosts in the appliance. For example: `192.168.160.102 api.pks.local`.  
