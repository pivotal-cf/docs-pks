---
title: Release Notes
owner: TKGI
topictype: releasenotes
---

This topic contains release notes for <%= vars.product_short %> (<%= vars.k8s_runtime_abbr %>) v1.8.

## <a id="1.8.1"></a><%= vars.k8s_runtime_abbr %> v1.8.1

**Release Date**: July 17, 2020

### <a id='1-8-1-snapshot'></a>Product Snapshot

<table class="nice">
    <tr>
        <th>Release</th>
        <th>Details</th>
    </tr>
    <tr>
        <td>Version</td>
        <td>v1.8.1</td>
    </tr>
    <tr>
        <td>Release date</td>
        <td>July 17, 2020</td>
    </tr>
    <tr>
        <th>Component</th>
        <th>Version</th>
    </tr>
    <tr>
        <td>Kubernetes</td>
        <td>v1.17.8+vmware.1</td>
    </tr>
    <tr>
        <td>Docker</td>
        <td>v19.03.5</td>
    </tr>
    <tr>
        <td>On-Demand Broker</td>
        <td>v0.38.0</td>
    </tr>
    <tr>
        <td>CoreDNS</td>
        <td>v1.6.2</td>
    </tr>
    <tr>
        <td>NCP</td>
        <td>v3.0.1.2</td>
    </tr>
    <tr>
        <td>UAA</td>
        <td>v74.5.15</td>
    </tr>
    <tr>
        <td>Percona XtraDB Cluster (PXC)</td>
        <td>v0.22.0</td>
    </tr>
    <tr>
        <td>Metrics Server</td>
        <td>v0.3.6</td>
    </tr>
    <tr>
        <td>etcd</td>
        <td>v3.4.3</td>
    </tr>
    <tr>
        <th>Compatibilities</th>
        <th>Versions</th>
    <tr>
        <td>Ops Manager</td>
        <td><strong>AWS</strong>, <strong>Azure</strong>, <strong>GCP</strong>: See <a href="https://network.pivotal.io/products/pivotal-container-service">VMware Tanzu Network</a><br />
        <strong>vSphere v7.0</strong>: Ops Manager v2.9.3+ (v2.9.5+ with VDS<sup>&#42;</sup>)<br />
        <strong>vSphere v6.7</strong> or <strong>v6.5</strong>: Ops Manager v2.9.3+, v2.8.2+, v2.7.15+</td>
    </tr>
    <tr>
        <td>vSphere</td>
        <td>See <a href="https://www.vmware.com/resources/compatibility/sim/interop_matrix.php#interop&356=&175=&1=">VMware Product Interoperability Matrices</a></td>
    </tr>
    <tr>
        <td>NSX-T</td>
        <td>v3.0.1.1 (v3.0.1 EP1), v2.5.1, v2.5.0</td>
    </tr>
        <tr>
        <td>Xenial stemcells</td>
        <td>See <a href="https://network.pivotal.io/products/pivotal-container-service"><%= vars.product_network %></a></td>
    </tr>
    <tr>
        <td>Windows stemcells</td>
        <td>
        <strong>vSphere v7.0</strong>: v2019.15<br />
        <strong>vSphere v6.7</strong> or <strong>v6.5</strong>: v2019.15 and later</td>
    </tr>
    <tr>
        <td>Harbor</td>
        <td>v2.0, v1.10.3</td>
    </tr>
    <tr>
        <td>CNS for vSphere</td>
        <td>v1.0.2</td>
    </tr>
    <tr>
        <td>Backup and Restore SDK</td>
        <td>v1.18.0</td>
    </tr>
</table>

<sup>&#42;</sup> Under certain conditions; see [VDS v7 Beta Support in <%= vars.k8s_runtime_abbr %> v1.8.1](#1-8-1-cvds).

### <a id='1-8-1-upgrade'></a>Upgrade Path

The supported upgrade paths to <%= vars.k8s_runtime_abbr %> v1.8.1 are from <%= vars.product_short %> v1.7.0 and later.

### <a id="1-8-1-features"></a>Features

This section describes new features and changes in <%= vars.product_short %> v1.8.1.

* Bumps Kubernetes to v1.17.8+vmware.1.
* Bumps NCP to v3.0.1.2.
  - Adds compatibility with NSX-T v3.0.1.1 (v3.0.1 EP1).  See [NSX-T v3.0 Compatibility](#1-8-0-nsxt-compat) below.
* **[Issue Fix]** NCP bump and NSX-T express patch remove [incompatibility with Xenial Stemcells 621.76+ and Ops Manager v2.9.6](#1-8-0-xenial-compat).
  - Because <%= vars.k8s_runtime_abbr %> v1.8.0 is incompatible with 621.76+ stemcells, you must upgrade <%= vars.k8s_runtime_abbr %> to v1.8.1 and update all clusters before upgrading stemcells to 621.76+.
* **[Issue Fix]** NCP bump and NSX-T express patch [introduce beta support](#1-8-1-cvds) for VDS under specific conditions.
* **[Security Fix]** Kubernetes bump fixes the following CVEs:
  * [CVE-2020-8558](https://github.com/kubernetes/kubernetes/issues/92315):
  node setting allows for neighboring hosts to bypass localhost boundary.
  * [CVE-2020-8559](https://github.com/kubernetes/kubernetes/issues/92914):
  privilege escalation from compromised node to cluster.
 
###<a id='1-8-1-known-issues'></a> Known Issues

Except where noted, known issues in <%= vars.product_short %> v1.8.1 are also in <%= vars.product_short %> v1.8.0.
See [Known Issues in <%= vars.product_short %> v1.8.0](#1-8-0-known-issues).

#### <a id='1-8-1-cvds'></a>VDS v7 Beta Support in <%= vars.k8s_runtime_abbr %> v1.8.1

<%= vars.k8s_runtime_abbr %> v1.8.1 integrated with NSX-T 3.0 and later on vSphere v7.0 supports VDS v7 as beta. Refer to [NSX-T support on VDS 7.0](https://docs.vmware.com/en/VMware-NSX-T-Data-Center/3.0/rn/VMware-NSX-T-Data-Center-30-Release-Notes.html?hWord=N4IghgNiBcIM4HcCWAXAxgCwAQDclwFdIkAvMFJAewDsQBfIA#whats_new) in the NSX Data Center 3.0 Release Notes to learn more about NSX-T v3.0 integration with VDS v7 and the considerations for using VDS v7 for converged traffic. 

<%= vars.k8s_runtime_abbr %> v1.8.1 supports VDS v7.0 as a beta feature with the following limitations:

- With Ops Manager v2.9.5 and later.
- With NSX-T v3.0.1.1 (v3.0.1 EP1) or later.
- With new installations of <%= vars.k8s_runtime_abbr %> v1.8.1 and NSX-T 3.0. You cannot migrate existing NSX-T infrastructure for TKGI from N-VDS to VDS.
- With a single VDS per TKGI instance if you are installing TKGI using EPMC. Note that this limitation does not exist when TKGI is deployed with Ops Manager.

## <a id="1.8.0"></a><%= vars.k8s_runtime_abbr %> v1.8.0

**Release Date**: June 30, 2020

### <a id='1-8-0-snapshot'></a>Product Snapshot

<table class="nice">
    <tr>
        <th>Release</th>
        <th>Details</th>
    </tr>
    <tr>
        <td>Version</td>
        <td>v1.8.0</td>
    </tr>
    <tr>
        <td>Release date</td>
        <td>June 30, 2020</td>
    </tr>
    <tr>
        <th>Component</th>
        <th>Version</th>
    </tr>
    <tr>
        <td>Kubernetes</td>
        <td>v1.17.5</td>
    </tr>
    <tr>
        <td>Docker</td>
        <td>v19.03.5</td>
    </tr>
    <tr>
        <td>On-Demand Broker</td>
        <td>v0.38.0</td>
    </tr>
    <tr>
        <td>CoreDNS</td>
        <td>v1.6.2</td>
    </tr>
    <tr>
        <td>NCP</td>
        <td>v3.0.1</td>
    </tr>
    <tr>
        <td>UAA</td>
        <td>v74.5.15</td>
    </tr>
    <tr>
        <td>Percona XtraDB Cluster (PXC)</td>
        <td>v0.22.0</td>
    </tr>
    <tr>
        <td>Metrics Server</td>
        <td>v0.3.6</td>
    </tr>
    <tr>
        <td>etcd</td>
        <td>v3.4.3</td>
    </tr>
    <tr>
        <th>Compatibilities</th>
        <th>Versions</th>
    <tr>
        <td>Ops Manager</td>
        <td><strong>AWS</strong>, <strong>Azure</strong>, <strong>GCP</strong>: See <a href="https://network.pivotal.io/products/pivotal-container-service">VMware Tanzu Network</a><br />
        <strong>vSphere v7.0</strong>: Ops Manager v2.9.3 - v2.9.5<sup>&#42;</sup><br />
        <strong>vSphere v6.7</strong> or <strong>v6.5</strong>: Ops Manager v2.9.3 - v2.9.5<sup>&#42;</sup>, v2.8.2+, v2.7.15+</td>
    </tr>
    <tr>
        <td>vSphere<sup>&#42;&#42;</sup></td>
        <td>See <a href="https://www.vmware.com/resources/compatibility/sim/interop_matrix.php#interop&356=&175=&1=">VMware Product Interoperability Matrices</a></td>
    </tr>
    <tr>
        <td>NSX-T</td>
        <td>v3.0<sup>&#42;&#42;&#42;</sup>, v2.5.1, v2.5.0</td>
    </tr>
        <tr>
        <td>Xenial stemcells<sup>&#42;</sup></td>
        <td>See <a href="https://network.pivotal.io/products/pivotal-container-service"><%= vars.product_network %></a></td>
    </tr>
    <tr>
        <td>Windows stemcells</td>
        <td>
        <strong>vSphere v7.0</strong>: v2019.15<br />
        <strong>vSphere v6.7</strong> or <strong>v6.5</strong>: v2019.15 and later</td>
    </tr>
    <tr>
        <td>Harbor</td>
        <td>v2.0, v1.10.3</td>
    </tr>
    <tr>
        <td>CNS for vSphere</td>
        <td>v1.0.2</td>
    </tr>
    <tr>
        <td>Backup and Restore SDK</td>
        <td>v1.18.0</td>
    </tr>
</table>

<sup>&#42;</sup> See [<%= vars.k8s_runtime_abbr %> v1.8 With NSX-T v3.0 Not Compatible With Xenial Stemcells 621.76+ and Ops Manager v2.9.6](#1-8-0-xenial-compat).

<sup>&#42;&#42;</sup> Excluding VCF 4; see [VCF 4 and VDS v7 Not Supported in <%= vars.k8s_runtime_abbr %> v1.8.0](#1-8-0-cvds).

<sup>&#42;&#42;&#42;</sup> <%= vars.recommended_by %> recommends NSX-T v3.0.1.1 (v3.0.1 EP1) or later for NSX-T v3.0 integration. 
Upgrading to earlier versions of NSX-T v3.0 is not recommended for production or large-scale 
<%= vars.k8s_runtime_abbr %> environments. 
For more information about NSX-T v3.0 support, 
see [NSX-T v3.0 Compatibility](#1-8-0-nsxt-compat) below.


### <a id='1-8-0-upgrade'></a>Upgrade Path

The supported upgrade paths to <%= vars.product_short %> v1.8.0 are from <%= vars.k8s_runtime_abbr %> v1.7.0 and later.

### <a id="1-8-0-features"></a>Features

This section describes new features and changes in <%= vars.product_full %> v1.8.0.

#### <a id="1-8-0-rebranding"></a>Enterprise PKS Renamed to <%= vars.product_short %>

Enterprise PKS has been renamed to
<%= vars.product_short %> (<%= vars.k8s_runtime_abbr %>).

What has changed:

* The <%= vars.product_tile %> v1.8 tile uses the new name.
* <%= vars.product_short %> v1.8 includes
two downloads of the CLI, the <%= vars.k8s_runtime_abbr %> CLI and PKS CLI.
See [PKS CLI Renamed  to <%= vars.k8s_runtime_abbr %> CLI](#1-8-0-tkgi-cli) below.

What has **not** changed:

* Internal components continue to use the old
name and its alternatives, such as `PKS`, `pks`, and `pivotal-container-service`.
This includes, but is not limited to, BOSH names, UAA roles, and text strings
containing the product name in <%= vars.k8s_runtime_abbr %> components
and <%= vars.k8s_runtime_abbr %>-provisioned clusters.

If you intend to continue using the PKS CLI in <%= vars.k8s_runtime_abbr %>
v1.8, no action is required.
However, future releases of <%= vars.k8s_runtime_abbr %> will deprecate and remove the PKS CLI.

#### <a id="1-8-0-tkgi-cli"></a>PKS CLI Renamed  to <%= vars.k8s_runtime_abbr %> CLI

To support the [product name change](#1-8-0-rebranding), <%= vars.product_short %> v1.8
is distributed with a <%= vars.k8s_runtime_abbr %> CLI in addition to a PKS CLI.

Both CLIs work identically and accept the same commands and arguments.
To run a TKGI CLI command,
substitute `tkgi` where you previously used `pks`.
For more information, see [<%= vars.k8s_runtime_abbr %> CLI](./cli/index.html).

To download the <%= vars.k8s_runtime_abbr %> CLI or the PKS CLI, see
[<%= vars.product_network %>](https://network.pivotal.io/products/pivotal-container-service/).

#### <a id="1-8-0-vsphere"></a>vSphere v7 Compatibility

<%= vars.k8s_runtime_abbr %> v1.8 can run on vSphere v7.

#### <a id="1-8-0-nsxt-compat"></a>NSX-T v3.0 Compatibility

On vSphere, <%= vars.k8s_runtime_abbr %> can run with NSX-T v3.0 container networking
with NSX-T v3.0.1.1 (v3.0.1 EP1) or later.

Upgrading <%= vars.k8s_runtime_abbr %> to NSX-T v3.0.1 requires you to apply the v3.0.1 EP1 Express Patch to the NSX-T upgrade.

<p class="note warning">
<strong>Warning:</strong> Intermittent upgrade failures and scale problems may occur if you upgrade to NSX-T v3.0.0, or upgrade to NSX-T v3.0.1 without applying the Express Patch.
</p>

#### <a id="1-8-0-tkgi-core"></a><%= vars.k8s_runtime_abbr %> Control Plane and API

* The <%= vars.k8s_runtime_abbr %> API VM no longer stores a copy of the
control plane database that the v1.7 upgrade migrated to the Database VM.
This deletion frees internal memory in the <%= vars.k8s_runtime_abbr %> API VM.
As a result, users may notice improved control plane performance.

* The **PKS 1.7.x Upgrade - MySQL Clone errand** errand has been removed from the
<%= vars.k8s_runtime_abbr %> tile **Errands** pane.


#### <a id="1-8-0-bosh-lifecycle"></a>Kubernetes Control Plane

* On Azure, <%= vars.k8s_runtime_abbr %> supports disabling the creation of a default outbound SNAT rule for clusters.
See [Kubernetes Cloud Provider](installing-azure.html#cloud-provider) 
for how to disable the default SNAT rule.

#### <a id="1-8-0-logging-monitoring"></a><%= vars.k8s_runtime_abbr %> Monitoring and Logging

* All <%= vars.k8s_runtime_abbr %> components use TLS v1.2 with
strong ciphers, including the `metrics-server` component.
`sslscan` on a `metrics-server` over port `443` now reports only TLS v1.2+
ciphers.

#### <a id="1-8-0-telemetry"></a>Customer Experience Improvement Program (CEIP) and Telemetry

* The legacy Telemetry DB has been removed from the <%= vars.control_plane_db %>. 

#### <a id="1-8-0-component-updates"></a>Component Updates

The following components have been updated:

* Bumps Kubernetes to v1.17.5.
* Bumps NCP to v3.0.1.
* Bumps UAA to v74.5.15.

###<a id="1-8-0-bug-fixes"></a>Bug Fixes

<%= vars.k8s_runtime_abbr %> v1.8.0 includes the following bug fixes:

* `tkgi tasks` returns valid output for all clusters.
* `tkgi upgrade-cluster` errand no longer times out when stopping `dockerd` processes.
* `tkgi get-credentials` works for clusters that have not been upgraded.
* `tkgi update-cluster` retains the `compute_profile` value when changing settings
for clusters created with a Compute Profile.

###<a id='1-8-0-known-issues'></a>Known Issues

<%= vars.k8s_runtime_abbr %> v1.8.0 has the following known issues:

#### <a id='1-8-0-xenial-compat'></a><%= vars.k8s_runtime_abbr %> v1.8 With NSX-T v3.0 Not Compatible With Xenial Stemcells 621.76+ and Ops Manager v2.9.6

This issue is fixed in [<%= vars.k8s_runtime_abbr %> v1.8.1](#1-8-1-features).

<%= vars.k8s_runtime_abbr %> with NSX-T v3.0 is compatible with Linux Ubuntu Xenial stemcell v621.75, but not with stemcell versions v621.76 and later.

Because Ops Manager v2.9.6 uses Xenial stemcell v621.76, <%= vars.k8s_runtime_abbr %> v1.8
with NSX-T on vSphere is incompatible with that version.
<%= vars.k8s_runtime_abbr %> v1.8 on NSX-T is compatible with Ops Manager v2.9.3 - v2.9.5.

#### <a id='1-8-0-cvds'></a>VCF 4 and VDS v7 Not Supported in <%= vars.k8s_runtime_abbr %> v1.8.0

For installations on vSphere v7 with NSX-T v3.0 integration,
<%= vars.k8s_runtime_abbr %> v1.8 supports only N-VDS for NSX-T traffic.
It does not support:

* VDS v7 using the same VDS for both vSphere and NSX-T traffic
* VMware Cloud Foundation (VCF) v4.x, which uses only VDS mode with NSX-T v3.0

In <%= vars.k8s_runtime_abbr %> v1.8.1, VDS support is beta [under certain conditions](#1-8-1-cvds).

For more information, see
[Configure vSphere Networking for ESXi Hosts](nsxt-3-0-install.html#nsxt30-esxi-vswitch)
in _Installing and Configuring NSX-T Data Center v3.0 for <%= vars.product_short %>_.

#### <a id='1-8-0-ops-manager'></a><%= vars.k8s_runtime_abbr %> v1.8 (Windows) on vSphere Not Compatible with Ops Manager v2.9

<%= vars.k8s_runtime_abbr %> v1.8 installations with Windows worker-based
Kubernetes clusters on vSphere (Flannel)
are not compatible with Ops Manager v2.9.
If you do not intend to deploy and run Windows worker-based Kubernetes clusters,
you can use Ops Manager v2.9 with <%= vars.k8s_runtime_abbr %> v1.8.

For Ops Manager compatibility information,
see [<%= vars.product_network %>](https://network.pivotal.io/products/pivotal-container-service).

#### <a id='1-8-0-ping'></a>Pinging Windows Workers Does Not Work

<%= vars.k8s_runtime_abbr %>-provisioned Windows workers inherit a Kubernetes limitation that prevents
outbound ICMP communication from workers.
As a result, pinging Windows workers does not work.

For information about this limitation, see [Limitations > Networking](https://kubernetes.io/docs/setup/production-environment/windows/intro-windows-in-kubernetes/#networking-1)
in the _Windows in Kubernetes_ documentation.

#### <a id="1-8-0-tmc-on-gcp"></a>TMC Integration Not Supported on GCP

<%= vars.k8s_runtime_abbr %> on Google Cloud Platform (GCP) does not support
Tanzu Mission Control integration, which is configured in
the **<%= vars.product_tile %>** tile > the **Tanzu Mission Control (Experimental)** pane.

If you intend to run <%= vars.k8s_runtime_abbr %> v1.8 on GCP,
skip this pane when configuring the <%= vars.product_tile %> tile.

#### <a id="1-8-0-oidc-response-header"></a>502 Bad Gateway After OIDC Login  

**Symptom**  

You experience a "502 Bad Gateway" error from the NSX load balancer after you log in to OIDC.  

**Explanation**  

A large response header has exceeded your NSX-T load balancer maximum 
response header size. The default maximum response header size is 10,240 characters and should 
be resized to 50,000.  

**Workaround**  

If you experience this issue, manually reconfigure your NSX-T `request_header_size` 
and `response_header_size` to 50,000 characters. 
For information about configuring NSX-T default header sizes, 
see [OIDC Response Header Overflow](https://community.pivotal.io/s/article/OIDC-Response-Header-overflow) in the Knowledge Base.  

#### <a id='1-8-0-uuid-length'></a> One Plan ID Longer than Other Plan IDs  

**Symptom**  

One of your plan IDs is one character longer than your other plan IDs.  

**Explanation**  

In <%= vars.k8s_runtime_abbr %>, each plan has a unique plan ID.
A plan ID is normally a UUID consisting of 32 alphanumeric characters and 4 hyphens. 
However, the **Plan 4** ID consists of 33 alphanumeric characters and 4 hyphens.  

**Solution**  

You can safely configure and use **Plan 4**. 
The length of the **Plan 4** ID does not affect the functionality of **Plan 4** clusters.  

If you require all plan IDs to have identical length, do not activate or use **Plan 4**.  

#### <a id='1-8-0-eight-cores-error'></a> NSX-T Pre-Check Errand Fails Due to Edge Node Configuration  

**Symptom**  

You have configured your NSX-T Edge Node VM as `medium` size, 
and the NSX-T Pre-Check Errand fails with the following error: 
"_ERROR: NSX-T Precheck failed due to Edge Node ... no of cpu cores is less than 8_". 

**Explanation**  

The NSX-T Pre-Check Errand is erroneously returning the "_cpu cores is less than 8_" error.

**Solution**  

You can safely configure your NSX-T Edge Node VMs as `medium` size and ignore the error. 

#### <a id='1-8-0-windows-proxy'></a> Difficulty Changing Proxy for Windows Workers

You must configure a global proxy in the <%= vars.product_tile %> tile > **Networking** pane before you create any Windows workers that use the proxy.

You cannot change the proxy configuration for Windows workers in an existing cluster.

#### <a id='1-8-0-http-proxy-password'></a> Character Limitations in HTTP Proxy Password

For vSphere with NSX-T, the HTTP Proxy password field does not support the following special characters: `&` or `;`.

## <a id="management-console-1.8.1"></a> <%= vars.k8s_runtime_abbr %> Management Console 1.8.1

**Release Date**: Not available yet

### <a id="management-console-1.8.1-features"></a>Features

<%= vars.product_short %> Management Console v1.8.1 is a compatibility release with no new features.

### <a id="management-console-1.8.1-snapshot"></a>Product Snapshot

<p class="note"><strong>Note</strong>: <%= vars.product_short %> Management Console provides an opinionated 
    installation of <%= vars.k8s_runtime_abbr %>. The supported versions may differ from or be more limited than
    what is generally supported by <%= vars.k8s_runtime_abbr %>.</p>

<table class="nice">
    <th>Element</th>
    <th>Details</th>
    <tr>
      <td>Version</td>
      <td>v1.8.1</td>
    </tr>
    <tr>
      <td>Release date</td>
      <td>Not available yet</td>
    </tr>
    <tr>
      <td>Installed Tanzu Kubernetes Grid Integrated Edition version</td>
      <td>v1.8.1</td>
    </tr>
    <tr>
      <td>Installed Ops Manager version</td>
      <td>v2.9.6</td>
    </tr>
    <tr>
      <td>Installed Kubernetes version</td>
        <td>v1.17.8+vmware.1</td>
    </tr>
    <tr>
      <td>Compatible NSX-T versions</td>
      <td>v3.0, v2.5.1, v2.5.0</td>
    </tr>
    <tr>
      <td>Installed Harbor Registry version</td>
      <td>v1.10.3</td>
    </tr>
    <tr>
        <td>Windows stemcells</td>
        <td>v2019.15</td>
    </tr>
</table>

### <a id='management-console-1-8-1-upgrade'></a>Upgrade Path

The supported upgrade path to <%= vars.product_short %> Management Console v1.8.1 is from 
<%= vars.product_short %> v1.7.0 and later.

### <a id="management-console-1.8.1-known-issues"></a> Known Issues

The <%= vars.product_short %> Management Console v1.8.1 has the same known issues as <a href="#management-console-1.8.0-known-issues">v1.8.0</a>.

## <a id="management-console-1.8.0"></a> <%= vars.k8s_runtime_abbr %> Management Console 1.8.0

**Release Date**: June 30, 2020

### <a id="management-console-1.8.0-features"></a>Features

<%= vars.product_short %> Management Console v1.8.0 updates include:

- Support for vSphere 7
- Support for NSX-T 3.0
- Rebranding to <%= vars.product_short %> Management Console
- Specify FQDN for the Ops Manager VM during upgrade

### <a id="management-console-1.8.0-snapshot"></a>Product Snapshot

<p class="note"><strong>Note</strong>: <%= vars.product_short %> Management Console provides an opinionated 
    installation of <%= vars.k8s_runtime_abbr %>. The supported versions may differ from or be more limited than
    what is generally supported by <%= vars.k8s_runtime_abbr %>.</p>

<table class="nice">
    <th>Element</th>
    <th>Details</th>
    <tr>
      <td>Version</td>
      <td>v1.8.0</td>
    </tr>
    <tr>
      <td>Release date</td>
      <td>June 30, 2020</td>
    </tr>
    <tr>
      <td>Installed Tanzu Kubernetes Grid Integrated Edition version</td>
      <td>v1.8.0</td>
    </tr>
    <tr>
      <td>Installed Ops Manager version</td>
      <td>v2.9.0</td>
    </tr>
    <tr>
      <td>Installed Kubernetes version</td>
        <td>v1.17.5</td>
    </tr>
    <tr>
      <td>Compatible NSX-T versions</td>
      <td>v3.0, v2.5.1, v2.5.0</td>
    </tr>
    <tr>
      <td>Installed Harbor Registry version</td>
      <td>v1.10.3</td>
    </tr>
    <tr>
        <td>Windows stemcells</td>
        <td>v2019.15</td>
    </tr>
</table>

### <a id='management-console-1-8-0-upgrade'></a>Upgrade Path

The supported upgrade path to <%= vars.product_short %> Management Console v1.8.0 is from 
<%= vars.product_short %> v1.7.0 and later.

### <a id="management-console-1.8.0-known-issues"></a> Known Issues

The <%= vars.product_short %> Management Console v1.8.0 has the following known issues:

#### <a id="management-console-1.8.0-vrli-https"></a> vRealize Log Insight integration does not support HTTPS connections

**Symptom**

The <%= vars.product_short %> Management Console integration to vRealize Log Insight does not support connections to the HTTPs port on the vRealize Log Insight server. 

**Workaround**

1. Use SSH to log in to the <%= vars.product_short %> Management Console appliance VM.
1. Open the file `/lib/systemd/system/pks-loginsight.service` in a text editor.
1. Add `-e LOG_SERVER_ENABLE_SSL_VERIFY=false`  
1. Set `-e LOG_SERVER_USE_SSL=true`. 
   
    The resulting file should look like the following example:
       
    ```
    ExecStart=/bin/docker run --privileged --restart=always --network=pks 
    -v /var/log/journal:/var/log/journal 
    --name=pks-loginsight 
    -e TYPE=gear2-vm 
    -e LOG_SERVER_HOST=${LOGINSIGHT_HOST} 
    -e LOG_SERVER_PORT=${LOGINSIGHT_PORT} 
    -e LOG_SERVER_ENABLE_SSL_VERIFY=false 
    -e LOG_SERVER_USE_SSL=true 
    -e LOG_SERVER_AGENT_ID=${LOGINSIGHT_ID} 
    pksoctopus/vrli-journald:v07092019
    ```      
   
1. Save the file and run `systemctl daemon-reload`.
1. Run `systemctl restart pks-loginsight.service` to restart the vRealize Log Insight service.
 
<%= vars.product_short %> Management Console can now send logs to the HTTPS port on the vRealize Log Insight server.

#### <a id="management-console-1.8.0-vsphere-ha"></a> vSphere HA causes Management Console ovfenv Data Corruption

**Symptom**

If you enable vSphere HA on a cluster, if the TKGI Management Console appliance VM is running on a host in that cluster, and if the host reboots, vSphere HA recreates a new TKGI Management Console appliance VM on another host in the cluster. Due to an issue with vSphere HA, the `ovfenv` data for the newly created appliance VM is corrupted and the new appliance VM does not boot up with the correct network configuration. 

**Workaround**

- In the vSphere Client, right-click the appliance VM and select **Power** > **Shut Down Guest OS**.
- Right-click the appliance again and select Edit Settings.
- Select **VM Options** and click **OK**.
- Verify under Recent Tasks that a `Reconfigure virtual machine` task has run on the appliance VM.
- Power on the appliance VM.

#### <a id="management-console-1.8.0-k8s-profile"></a> Base64 encoded file arguments are not decoded in Kubernetes profiles

**Symptom**

Some file arguments in Kubernetes profiles are base64 encoded. When the management console displays the Kubernetes profile,
some file arguments are not decoded.

**Workaround**

Run `echo "$content" | base64 --decode` 

#### <a id="management-console-1.8.0-network-profile"></a> Network profiles not immediately selectable

**Symptom**

If you create network profiles and then try to apply them in the Create Cluster page, the new profiles 
are not available for selection. 

**Workaround**

Log out of the management console and log back in again.

#### <a id="management-console-1.8.0-cluster-summary"></a> Real-Time IP information not displayed for network profiles

**Symptom**

In the cluster summary page, only default IP pool, pod IP block, node IP block values are displayed, 
rather than the real-time values from the associated network profile. 

**Workaround**

None
