---
title: Release Notes
owner: PKS
topictype: releasenotes
---

This topic contains release notes for <%= vars.product_full %> v1.7.

<p class="note warning"><strong>Warning:</strong> Before installing or upgrading to <%= vars.product_short %> v1.7,
review the <a href="#1-7-0-breaking-changes">Breaking Changes</a> below.
</p>

## <a id="1.7.2"></a>v1.7.2

**Release Date**: September 28, 2020

### <a id='1-7-2-snapshot'></a>Product Snapshot

<table class="nice">
    <tr>
        <th>Release</th>
        <th>Details</th>
    </tr>
    <tr>
        <td>Version</td>
        <td>v1.7.2</td>
    </tr>
    <tr>
        <td>Release date</td>
        <td>September 28, 2020</td>
    </tr>
    <tr>
        <th>Component</th>
        <th>Version</th>
    </tr>
    <tr>
        <td>Kubernetes</td>
        <td>v1.16.14+vmware.1</td>
    </tr>
    <tr>
       <td>CoreDNS</td>
        <td>v1.6.2+vmware.10</td>
    </tr>
    <tr>
        <td>Docker</td>
        <td>v18.09.9</td>
    </tr>
    <tr>
        <td>etcd</td>
        <td>v3.3.17</td>
    </tr>
    <tr>
        <td>Metrics Server</td>
        <td>v0.3.6</td>
    </tr>
    <tr>
        <td>NCP</td>
        <td>v2.5.1</td>
    </tr>
    <tr>
        <td>On-Demand Broker</td>
        <td>v0.38.0</td>
    </tr>
    <tr>
        <td>Percona XtraDB Cluster (PXC)</td>
        <td>v0.28.0</td>
    </tr>
    <tr>
        <td>UAA</td>
        <td>v74.5.18</td>
    </tr>
    <tr>
        <th>Compatibilities</th>
        <th>Versions</th>
    </tr>
    <tr>
        <td>Ops Manager</td>
        <td>See <a href="https://network.pivotal.io/products/pivotal-container-service#/releases/742750"><%= vars.product_network %></a></td>
    </tr>
    <tr>
        <td>Xenial stemcells</td>
        <td>See <a href="https://network.pivotal.io/products/pivotal-container-service#/releases/742750"><%= vars.product_network %></a></td>
    </tr>
    <tr>
        <td>Windows stemcells</td>
        <td>v2019.15 and later</td>
    </tr>
    <tr>
        <td>Backup and Restore SDK</td>
        <td>v1.17.0</td>
    </tr>
    <tr>
        <td>vSphere</td>
        <td>See <a href="https://www.vmware.com/resources/compatibility/sim/interop_matrix.php#interop&356=&175=&1=">VMware Product Interoperability Matrices</a></td>
    </tr>
    <tr>
        <td>CNS for vSphere</td>
        <td>v1.0.2</td>
    </tr>
    <tr>
        <td>NSX-T</td>
        <td>v2.5.2, v2.5.1, v2.5.0, v2.4.3</td>
    </tr>
    <tr>
        <td>Harbor</td>
        <td>v1.10.3</td>
    </tr>
</table>

### <a id='1-7-2-upgrade'></a>Upgrade Path

The supported upgrade paths to Enterprise PKS v1.7.2 are from <%= vars.product_short %> v1.6.0 and later.

### <a id="1-7-2-features"></a>Features

This section describes new features and changes in <%= vars.product_short %> v1.7.2.

* <strong>Bumps Kubernetes to v1.16.14+vmware.1</strong>.
* Bumps Percona XtraDB Cluster (PXC) to v0.28.0.
* Bumps UAA to v74.5.18.
* **[Bug Fix]** Enforces TLS v1.2 for TLS connections to `pxc-mysql`.
* **[Bug Fix]** Adds `syslog_forwarder` to the <%= vars.product_short %> database
VM.

### <a id='1-7-2-breaking-changes'></a>Breaking Changes

All breaking changes in <%= vars.product_short %> v1.7.2 are also in
<%= vars.k8s_runtime_abbr %> v1.7.0.
See [Breaking Changes in <%= vars.product_short %> v1.7.0](#1-7-0-breaking-changes).

###<a id='1-7-2-known-issues'></a> Known Issues

Except where noted, <%= vars.product_short %> v1.7.2 has the same known issues as
<%= vars.product_short %> v1.7.0.
See [Known Issues in <%= vars.product_short %> v1.7.0](#1-7-0-known-issues).

<%#
<%= vars.product_short 
% > v1.7.2 has the following known issues:

#### <a id='1-8-0-stemcell-compat-2'></a> Kubernetes Clusters With Xenial Stemcell v621.85 and Later Fail for <%= vars.k8s_runtime_abbr 
% > v1.7 on vSphere With NSX-T

**Symptom**

When you use <%= vars.k8s_runtime_abbr 
% > on vSphere with NSX-T v1.7.2 and later,
<%= vars.k8s_runtime_abbr 
% >-provisioned Kubernetes clusters with Xenial Stemcell v621.85 and later fail to start.
The cluster start failure log includes the following:

```
Error: Action Failed get_task: Task ... result: Compiling package openvswitch:
Running packaging script: Running packaging script: Command exited with 2;
```

Xenial Stemcell v621.85 is installed by default when installing Ops Manager v2.8.15.

**Workaround**

If you experience this issue, manually revert the stemcell to an earlier compatible version.
For information about reverting stemcells,
see [How to revert a stemcell in TKGI to prevent OVS compilation issues]
(https://pvtl.force.com/s/article/How-to-revert-stemcell-from-621-76-to-621-75-for-TKGI-1-8-0-to-prevent-ovs-compilation-issues?language=en_US)
in the Knowledge Base.
%>

## <a id="1.7.1"></a>v1.7.1

**Release Date**: July 15, 2020

### <a id='1-7-1-snapshot'></a>Product Snapshot

<table class="nice">
    <tr>
        <th>Release</th>
        <th>Details</th>
    </tr>
    <tr>
        <td>Version</td>
        <td>v1.7.1</td>
    </tr>
    <tr>
        <td>Release date</td>
        <td>July 15, 2020</td>
    </tr>
    <tr>
        <th>Component</th>
        <th>Version</th>
    </tr>
    <tr>
        <td>Kubernetes</td>
        <td>v1.16.12+vmware.1</td>
    </tr>
    <tr>
       <td>CoreDNS</td>
        <td>v1.6.2</td>
    </tr>
    <tr>
        <td>Docker</td>
        <td>v18.09.9</td>
    </tr>
    <tr>
        <td>etcd</td>
        <td>v3.3.17</td>
    </tr>
    <tr>
        <td>Metrics Server</td>
        <td>v0.3.6</td>
    </tr>
    <tr>
        <td>NCP</td>
        <td>v2.5.1</td>
    </tr>
    <tr>
        <td>On-Demand Broker</td>
        <td>v0.38.0</td>
    </tr>
    <tr>
        <td>Percona XtraDB Cluster (PXC)</td>
        <td>v0.22.0</td>
    </tr>
    <tr>
        <td>UAA</td>
        <td>v74.5.17</td>
    </tr>
    <tr>
        <th>Compatibilities</th>
        <th>Versions</th>
    </tr>
    <tr>
        <td>Ops Manager</td>
        <td>See <a href="https://network.pivotal.io/products/pivotal-container-service/#/releases/691278"><%= vars.product_network %></a></td>
    </tr>
    <tr>
        <td>Xenial stemcells</td>
        <td>See <a href="https://network.pivotal.io/products/pivotal-container-service/#/releases/691278"><%= vars.product_network %></a></td>
    </tr>
    <tr>
        <td>Windows stemcells</td>
        <td>v2019.15 and later</td>
    </tr>
    <tr>
        <td>Backup and Restore SDK</td>
        <td>v1.17.0</td>
    </tr>
    <tr>
        <td>vSphere</td>
        <td>See <a href="https://www.vmware.com/resources/compatibility/sim/interop_matrix.php#interop&356=&175=&1=">VMware Product Interoperability Matrices</a></td>
    </tr>
    <tr>
        <td>CNS for vSphere</td>
        <td>v1.0.2</td>
    </tr>
    <tr>
        <td>NSX-T</td>
        <td>v2.5.2, v2.5.1, v2.5.0, v2.4.3</td>
    </tr>
    <tr>
        <td>Harbor</td>
        <td>v1.10.3</td>
    </tr>
</table>

### <a id='1-7-1-upgrade'></a>Upgrade Path

The supported upgrade paths to Enterprise PKS v1.7.1 are from <%= vars.product_short %> v1.6.0 and later.

### <a id="1-7-1-features"></a>Features

This section describes new features and changes in <%= vars.product_short %> v1.7.1.

* Bumps Kubernetes to v1.16.12+vmware.1.
* Bumps UAA to v74.5.17.
* [**Security Fix**] All components use TLS v1.2 with strong ciphers for internal communications,
including `metrics-server`.
* **[Security Fix]** Kubernetes bump fixes the following CVEs:
  * [CVE-2020-8558](https://github.com/kubernetes/kubernetes/issues/92315):
  node setting allows for neighboring hosts to bypass localhost boundary.
  * [CVE-2020-8559](https://github.com/kubernetes/kubernetes/issues/92914):
  privilege escalation from compromised node to cluster.
* [**Bug Fix**] Fixes `pks tasks` [command failure](#1-7-0-null-tasks) with v1.6 upgrade tasks.

### <a id='1-7-1-breaking-changes'></a>Breaking Changes

All breaking changes in <%= vars.product_short %> v1.7.1 are also in <%= vars.k8s_runtime_abbr %> v1.7.0.
See [Breaking Changes in <%= vars.product_short %> v1.7.0](#1-7-0-breaking-changes).

###<a id='1-7-1-known-issues'></a> Known Issues

All known issues in <%= vars.product_short %> v1.7.1 are also in <%= vars.product_short %> v1.7.0.
See [Known Issues in <%= vars.product_short %> v1.7.0](#1-7-0-known-issues).


## <a id="1.7.0"></a>v1.7.0

**Release Date**: April 2, 2020

### <a id='1-7-0-snapshot'></a>Product Snapshot

<table class="nice">
    <tr>
        <th>Release</th>
        <th>Details</th>
    </tr>
    <tr>
        <td>Version</td>
        <td>v1.7.0</td>
    </tr>
    <tr>
        <td>Release date</td>
        <td>April 2, 2020</td>
    </tr>
    <tr>
        <th>Component</th>
        <th>Version</th>
    </tr>
    <tr>
        <td>Kubernetes</td>
        <td>v1.16.7</td>
    </tr>
    <tr>
       <td>CoreDNS</td>
       <td>v1.6.2</td>
    </tr>
    <tr>
        <td>Docker</td>
        <td>v18.09.9</td>
    </tr>
    <tr>
        <td>etcd</td>
        <td>v3.3.17</td>
    </tr>
    <tr>
        <td>Metrics Server</td>
        <td>v0.3.6</td>
    </tr>
    <tr>
        <td>NCP</td>
        <td>v2.5.1</td>
    </tr>
    <tr>
        <td>On-Demand Broker</td>
        <td>v0.38.0</td>
    </tr>
    <tr>
        <td>Percona XtraDB Cluster (PXC)</td>
        <td>v0.22.0</td>
    </tr>
    <tr>
        <td>UAA</td>
        <td>v74.5.10</td>
    </tr>
    <tr>
        <th>Compatibilities</th>
        <th>Versions</th>
    </tr>
    <tr>
        <td>Ops Manager</td>
        <td>See <a href="https://network.pivotal.io/products/pivotal-container-service#/releases/742750"><%= vars.product_network %></a></td>
    </tr>
    <tr>
        <td>Xenial stemcells</td>
        <td>See <a href="https://network.pivotal.io/products/pivotal-container-service#/releases/742750"><%= vars.product_network %></a></td>
    </tr>
    <tr>
        <td>Windows stemcells</td>
        <td>v2019.15 and later</td>
    </tr>
    <tr>
        <td>Backup and Restore SDK</td>
        <td>v1.17.0</td>
    </tr>
    <tr>
        <td>vSphere</td>
        <td>See <a href="https://www.vmware.com/resources/compatibility/sim/interop_matrix.php#interop&356=&175=&1=">VMware Product Interoperability Matrices</a></td>
    </tr>
    <tr>
        <td>CNS for vSphere</td>
        <td>v1.0.2</td>
    </tr>
    <tr>
        <td>NSX-T</td>
        <td>v2.5.1, v2.5.0, v2.4.3</td>
    </tr>
</table>

### <a id='1-7-0-upgrade'></a>Upgrade Path

The supported upgrade paths to <%= vars.product_short %> v1.7.0 are from <%= vars.product_short %> v1.6.0 and later.

### <a id="1-7-0-features"></a>Features

This section describes new features and changes in <%= vars.product_full %> v1.7.0.

#### <a id="1-7-0-pks-core"></a>PKS Control Plane and API

* Introduces _Kubernetes profiles_ that customize Kubernetes component settings for <%= vars.k8s_runtime_abbr %>-provisioned clusters.
Kubernetes profiles can include validated configurations supported by the PKS team and unvalidated configurations for evaluation only.
For more information, see [Using Kubernetes Profiles](./k8s-profiles.html).
* Supports [VMware Tanzu Service Mesh by VMware NSX (Beta)](./nsxt-service-mesh.html).
* Uses TLS v1.2+ with strong ciphers for all internal components except for the `metrics-server` (see [Known Issues](#1-7-0-metrics-server)).
* Supports resizing and updating Kubernetes clusters that
have not been upgraded from the previously installed <%= vars.product_short %> version.
* Splits the <%= vars.k8s_runtime_abbr %> control plane into separate <%= vars.control_plane %> and <%= vars.control_plane_db %> VMs.
* Increases to 65,535 the maximum allowed number of open files for the Docker process on worker nodes.
* Removes the **Server SSL Cert AltName** field from under the **<%= vars.product_tile %>** tile > **UAA** > **LDAP Server**.
<%= vars.product_short %> no longer uses this field.

#### <a id="1-7-0-bosh-lifecycle"></a>Kubernetes Control Plane

* For security, PKS v1.7 does not include the Kubernetes Dashboard Web UI because it uses [weak TLS ciphers](https://github.com/kubernetes/dashboard/issues/3058).
Clusters created with PKS v1.7 do not have Dashboard installed on creation.
  - You can install Dashboard on new clusters by following the [Deploying the Dashboard UI](https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/#deploying-the-dashboard-ui) instructions in the Kubernetes documentation.
  - Clusters upgraded from PKS v1.6 to v1.7 have leftover, cached dashboard objects.
      - The service of these cached Dashboard objects may degrade over time, as resize or upgrade operations rebuild node VMs.
      - To remove the leftover Dashboard objects from a cluster, run
`kubectl delete service,deployment,configmap,rolebinding,role,serviceaccount -l k8s-app=kubernetes-dashboard -n kube-system`

* The bump to Kubernetes v1.16 removes some API version definitions in favor of newer, more stable elements.
See [Deprecated APIs Removed In 1.16: Here’s What You Need To Know] (https://kubernetes.io/blog/2019/07/18/api-deprecations-in-1-16/) in the _Kubernetes Blog_.
Kubernetes recommends that you:
  - Update the following to reference the newer API definitions:
      - Custom integrations and controllers
      - Third-party tools such as ingress controllers and continuous delivery systems
  - <%= vars.recommended_by %> recommends that customers selectively upgrade Kubernetes 1.15.x clusters before upgrading a fleet of clusters.
  See [Upgrade a Single Cluster](./upgrade-clusters.html#upgrade-cluster) for how to upgrade clusters selectively.

* Replaces the [proxy](https://coredns.io/plugins/proxy/) plugin with the [forward](https://coredns.io/plugins/forward/) plugin for CoreDNS,
as recommended in upstream Kubernetes.
For more information, see [CoreDNS-1.4.0 Release](https://coredns.io/2019/03/03/coredns-1.4.0-release/) in the CoreDNS documentation.

#### <a id="1-7-0-logging-monitoring"></a>PKS Monitoring and Logging

* Adds new metrics that <%= vars.product_short %> can send to your monitoring service:
  - Node Exporter metrics from the <%= vars.control_plane %> VM and worker nodes in Prometheus format
  - Metrics from the Kubernetes controller manager and Kubernetes API server on master nodes

    For configuration instructions, see [Telegraf](installing-pks-vsphere.html#telegraf) in the _Installing_ topic for your IaaS.
* Improves the in-cluster Node Exporter component,
which is enabled in the **<%= vars.product_tile %>** tile > **In-Cluster Monitoring**.
* Deprecates the billing database.
The billing database is scheduled to be removed in <%= vars.product_short %> v1.9.
You can use Telemetry instead of the billing database.
For more information about the billing database and Telemetry, see [Viewing Usage Data](view-usage-data.html#billing-db)
and [Telemetry](telemetry.html).
If you are impacted by this deprecation,
please reach out to <a href="mailto:pks-telemetry@groups.vmware.com">pks-telemetry@groups.vmware.com</a>.

####<a id="1-7-0-windows-features"></a>Windows on PKS

* Makes HTTP and HTTPS proxy available on Windows nodes, to support downloading Windows Docker images through a proxy.
  - You must configure a global proxy in the <%= vars.product_short %> tile > **Networking** pane before you create any Windows workers that use the proxy.

* When upgrading Windows clusters, <%= vars.product_short %> upgrades Windows workers with the latest Windows stemcell.

* Enables Windows pods to access DNS services from `kube-dns` (CoreDNS).

* Windows pods can now consume the same pod CIDR as Linux pods.

* Windows pods can now egress to applications or workloads outside of the pod’s IP subnet.

####<a id="1-7-0-nsx-t-features"></a>PKS with NSX-T Networking

* Supports highly resilient workloads using stretched clusters.
See [Solution Guide for Enabling Highly Resilient Kubernetes Workloads Using vSAN Stretched Clusters](https://d1fto35gcfffzn.cloudfront.net/VMware-Enterprise-PKS-Using-vSAN-Stretched-Clusters-to-Enable-Resilient-Kubernetes-Workloads.pdf).
* Changes the default topology for newly-created clusters to Shared Tier-1 Router topology, from Dedicated Tier-1 Router topology.
  - This change does not affect the topology of previously-existing clusters, which continue to run during and after upgrade to v1.7.
  - You cannot change the topology of an existing cluster from Dedicated Tier-1 to Shared Tier-1.
  - See [Network Profile for Dedicated Tier-1 Topology](./network-profiles-shared-t1.html#dedicated-t1-profile) for how to use a network profile to override the default and create new Dedicated Tier-1 Router clusters.


* Supports backup and restore of the PKS control plane, Kubernetes clusters, and stateless workloads networked with vSphere NSX-T.

#### <a id="1-7-0-telemetry"></a>Customer Experience Improvement Program (CEIP) and Telemetry

* Adds proxy support for Telemetry.
You can configure your <%= vars.product_short %> proxy settings in the **<%= vars.product_tile %>** tile > **Networking**.

#### <a id="1-7-0-component-updates"></a>Component Updates

* Bumps Kubernetes to v1.16.7.
* Bumps NCP to v2.5.1.
* Bumps UAA to v74.5.10.

####<a id="1-7-0-bug-fixes"></a>Bug Fixes

<%= vars.product_short %> v1.7.0 includes the following bug fixes:

* Improves the behavior of the `pks get-kubeconfig` and `pks get-credentials` commands during cluster updates and upgrades.
You can now run the `pks get-kubeconfig` command during single- and multi-master cluster updates.
Additionally, you can run the `pks get-credentials` command during multi-master cluster upgrades.

* When upgrading Windows clusters, <%= vars.product_short %> upgrades the Windows stemcell and Kubernetes version on Windows worker nodes.
See [Windows on PKS](#1-7-0-windows-features) for more information.

### <a id='1-7-0-breaking-changes'></a>Breaking Changes

<%= vars.product_short %> v1.7.0 has the following breaking changes:

#### <a id='1-7-0-api-cert'></a> No PKS API Access with Certificate Hostname Mismatch

Prior versions of <%= vars.product_short %> did not require the hostname in the PKS API certificate to match the PKS API hostname.
In PKS v1.7, the PKS API certificate hostname must contain a valid hostname for the PKS API.
This breaks the existing PKS API certificate if it has an invalid hostname.

To fix this issue, create a new PKS API certificate with the hostname that you entered in the Enterprise PKS tile > **PKS API** pane > **API Hostname (FQDN)** field.

#### <a id='1-7-0-dashboard'></a> Removal of the Dashboard UI

Clusters created with PKS v1.7 do not have Dashboard installed on creation.
For more information, see the [Kubernetes Control Plane](#1-7-0-bosh-lifecycle) section.

#### <a id='1-7-0-api'></a> Removal of API Version Definitions in Kubernetes v1.16

The bump to Kubernetes v1.16 removes some API version definitions in favor of newer, more stable definitions.
This change may break some integrations, controllers, and pipelines.
For more information, see the [Kubernetes Control Plane](#1-7-0-bosh-lifecycle) section.

#### <a id='1-7-0-db-migration'></a><%= vars.product_short %> Database Migration

This release migrates the <%= vars.product_short %> control plane database
from the PKS API VM to a new database VM, PKS Database.

<%= vars.product_short %> performs the database migration as part of your <%= vars.product_short %> upgrade to v1.7.
During the upgrade, the <%= vars.product_short %> control plane and the PKS CLI will be unavailable.
Your Kubernetes workloads remain accessible throughout the upgrade.

To upgrade to <%= vars.product_short %> v1.7,
follow the instructions in [Upgrading <%= vars.product_short %> (Flannel Networking)](upgrade-pks.html) or [Upgrading <%= vars.product_short %> (NSX-T Networking)](upgrade-pks-nsxt.html).

These topics contain important preparation and upgrade configuration steps
you must follow before and during your upgrade to v1.7.

<p class="note warning"><strong>Warning:</strong> If your <%= vars.product_short %>
environment was originally created using <%= vars.product_short %> v1.2 or earlier,
see <a href="#1-7-0-drop-tables">Leftover Tables From PKS v1.2 Prevent Database Migration</a> in the Known Issues below.</p>

###<a id='1-7-0-known-issues'></a>Known Issues

<%= vars.product_short %> v1.7.0 has the following known issues:

#### <a id="1.7.0-azure-apply-changes"></a> Error: Could Not Execute "Apply-Changes" in Azure Environment

**Symptom**

After clicking **Apply Changes** on the <%= vars.k8s_runtime_abbr %> tile in an Azure environment, you experience
an error '_...could not execute "apply-changes"..._' with either of the following descriptions:

* _{"errors":{"base":["undefined method 'location' for nil:NilClass"]}}_
* _FailedError.new("Resource Groups in region '#{location}' do not support Availability Zones"))_

For example:

```
INFO | 2020-09-21 03:46:49 +0000 | Vessel::Workflows::Installer#run | Install product (apply changes)
2020/09/21 03:47:02 could not execute "apply-changes": installation failed to trigger: request failed: unexpected response from /api/v0/installations:
HTTP/1.1 500 Internal Server Error
Transfer-Encoding: chunked
Cache-Control: no-cache, no-store
Connection: keep-alive
Content-Type: application/json; charset=utf-8
Date: Mon, 21 Sep 2020 17:51:50 GMT
Expires: Fri, 01 Jan 1990 00:00:00 GMT
Pragma: no-cache
Referrer-Policy: strict-origin-when-cross-origin
Server: Ops Manager
Strict-Transport-Security: max-age=31536000; includeSubDomains
X-Content-Type-Options: nosniff
X-Download-Options: noopen
X-Frame-Options: SAMEORIGIN
X-Permitted-Cross-Domain-Policies: none
X-Request-Id: f5fc99c1-21a7-45c3-7f39
X-Runtime: 9.905591
X-Xss-Protection: 1; mode=block

44
{"errors":{"base":["undefined method `location' for nil:NilClass"]}}
0
```


**Explanation**

The Azure CPI endpoint used by Ops Manager has been changed and
your installed version of Ops Manager is not compatible with the new endpoint.

**Workaround**

Run the following Ops Manager CLI command:

```
om --skip-ssl-validation --username USERNAME --password PASSWORD --target https://OPSMAN-API curl --silent --path /api/v0/staged/director/verifiers/install_time/IaasConfigurationVerifier -x PUT -d '{ "enabled": false }'
```

Where:

* `USERNAME` is the account to use to run Ops Manager API commands.
* `PASSWORD` is the password for the account.
* `OPSMAN-API` is the IP address for the Ops Manager API


For more information, see [Error 'undefined method location' is received when running Apply Change on Azure]
(https://community.pivotal.io/s/article/undefined-method-location-when-running-Apply-Change-on-Azure?language=en_US)
in the VMware Tanzu Knowledge Base.

#### <a id='1-7-0-drop-tables'></a> Leftover Tables From PKS v1.2 Prevent Database Migration

PKS v1.2 created a `pksdata` and `pkswatermark` tables in the Telemetry database on the `pivotal-container-service` VM. These leftover tables interfere with database migration.

If your <%= vars.k8s_runtime_abbr %> installation was originally created using PKS v1.2 or earlier,
you must complete the procedures in the KB article [For environments created before VMware Enterprise PKS 1.3, upgrading to PKS 1.7 fails during data migration when running clone-db errand](https://community.pivotal.io/s/article/environments-created-before-PKS-13-upgrading-to-PKS-1-7-fails) before upgrading to <%= vars.product_short %> v1.7.


#### <a id='1-7-0-null-tasks'></a> pks tasks Command Fails on v1.6 Tasks in Upgrade Table

This issue is [fixed](#1-7-1-features) in <%= vars.k8s_runtime_abbr %> v1.7.1.

**Symptom**

Running `pks tasks` produces `Error: An error occurred in the PKS API when processing`,
after which `pks-api.log` shows `java.lang.NullPointerException: null`.

This error does not stop cluster upgrades from completing or otherwise affect the cluster upgrade process.

**Explanation**

Timestamps on task objects and the `Date` column for `pks tasks` output are new for v1.7. Existing v1.6 tasks do not have these values, and the `pks tasks` query does not take this into account.

**Workaround**

Monitor cluster upgrades through `pks cluster` and other commands.


#### <a id='1-7-0-get-creds-fail'></a> pks get-credentials Command Fails on N-1 Clusters

The command `pks get-credentials` does not work for clusters that have not upgraded
to the current version of the <%= vars.k8s_runtime_abbr %> control plane,
and are therefore running the previously-installed version of <%= vars.k8s_runtime_abbr %>.

#### <a id='1-7-0-ingress-ip'></a> Ingress IP in Network Profile Is Duplicated to Clusters Created Later

**Symptom**

After you create a PKS cluster with a network profile that specifies a value for `ingress_ip`, clusters created subsequently, either with or without a network profile, incorrectly retain this same value for `ingress_ip`.
Because this address is already allocated in the FIP pool, creating the new clusters either fails to create HTTP and HTTPS servers for the clusters, or creates them with conflicting addresses.

**Explanation**

Creating a cluster with a network profile that defines `ingress_ip` also sets the value for `http_and_https_ingress_ip` in the NSX-T Container Plugin (NCP) configuration, at `/var/vcap/jobs/ncp/config/ncp.ini`.
When <%= vars.product_short %> creates new clusters, it does not overwrite this value in the NCP configuration.

**Workaround**

See the **Resolution** section in the Knowledge Base article [Same ingress IP used across multiple clusters in PKS 1.7](https://community.pivotal.io/s/article/PKS-NSX-T-cluster-creation-duplicating-same-ingress-ip-for-multiple-clusters).


#### <a id='1-7-0-upgrade-cluster-timeout'></a> Cluster Upgrade Fails Due to Timeout

**Symptom**

Running `pks upgrade-cluster` fails with an error that stopping the `dockerd` process timed out after 60 seconds.

**Explanation**

Changes to how Docker shuts down containers affects interaction between kubelet stop and Docker stop,
and requires graceful shutdown to start with kubelet.

**Workaround**

1. For each worker node in the cluster, run `monit unmonitor all` and `monit stop all`.

1. Run `pks upgrade-cluster` again.

#### <a id='1-7-0-ops-manager'></a><%= vars.product_short %> v1.7 (Windows) on vSphere Not Compatible with Ops Manager v2.9

<%= vars.product_short %> v1.7 installations with Windows worker-based
Kubernetes clusters on vSphere (Flannel)
are not compatible with Ops Manager v2.9.
If you do not intend to deploy and run Windows worker-based Kubernetes clusters,
you can use Ops Manager v2.9 with <%= vars.product_short %> v1.7.

For Ops Manager compatibility information,
see [<%= vars.product_network %>](https://network.pivotal.io/products/pivotal-container-service).

#### <a id='1-7-0-ping'></a>Pinging Windows Workers Does Not Work

<%= vars.product_short %>-provisioned Windows workers inherit a Kubernetes limitation that prevents
outbound ICMP communication from workers.
As a result, pinging Windows workers does not work.

For information about this limitation, see [Limitations > Networking](https://kubernetes.io/docs/setup/production-environment/windows/intro-windows-in-kubernetes/#networking-1)
in the _Windows in Kubernetes_ documentation.

#### <a id="1-7-0-tmc-on-gcp"></a>Tanzu Mission Control Integration Not Supported on GCP

<%= vars.product_short %> on Google Cloud Platform (GCP) does not support
Tanzu Mission Control (TMC) integration, which is configured in
the **<%= vars.product_tile %>** tile > the **Tanzu Mission Control (Experimental)** pane.

If you intend to run <%= vars.product_short %> v1.7 on GCP,
skip this pane when configuring the <%= vars.product_tile %> tile.

#### <a id='1-7-0-tmc-api'></a>Existing <%= vars.k8s_runtime_abbr %> Clusters No Longer Automatically Attach to TMC

**Symptom**  

Previously, your <%= vars.k8s_runtime_abbr %> clusters had attached to TMC automatically. 
Now, existing <%= vars.k8s_runtime_abbr %> clusters no longer automatically attach to TMC.  

**Explanation**  

The TMC API has been updated and is incompatible with the API calls from existing <%= vars.k8s_runtime_abbr %> clusters. 

**Workaround** 

Manually attach your existing <%= vars.k8s_runtime_abbr %> clusters to TMC. For more information, see 
[Attach an Existing Cluster](https://docs.vmware.com/en/VMware-Tanzu-Mission-Control/services/tanzumc-getstart/GUID-F0162E40-8D47-45D7-9EA1-83B64B380F5C.html) 
in the VMware Tanzu Mission Control Product Documentation.  

#### <a id="1-7-0-oidc-response-header"></a>502 Bad Gateway After OIDC Login

**Symptom**

You experience a "502 Bad Gateway" error from the NSX load balancer after you log in to OIDC.

**Explanation**

A large response header has exceeded your NSX-T load balancer maximum
response header size. The default maximum response header size is 10,240 characters and should
be resized to 50,000.

**Workaround**

If you experience this issue, manually reconfigure your NSX-T `request_header_size`
and `response_header_size` to 50,000 characters.
For information about configuring NSX-T default header sizes,
see [OIDC Response Header Overflow](https://community.pivotal.io/s/article/OIDC-Response-Header-overflow) in the Knowledge Base.

#### <a id='1-7-0-uuid-length'></a> One Plan ID Longer than Other Plan IDs

**Symptom**

One of your plan IDs is one character longer than your other plan IDs.

**Explanation**

In <%= vars.product_short %>, each plan has a unique plan ID.
A plan ID is normally a UUID consisting of 32 alphanumeric characters and 4 hyphens.
However, the **Plan 4** ID consists of 33 alphanumeric characters and 4 hyphens.

**Solution**

You can safely configure and use **Plan 4**.
The length of the **Plan 4** ID does not affect the functionality of **Plan 4** clusters.

If you require all plan IDs to have identical length, do not activate or use **Plan 4**.

#### <a id='1-7-0-eight-cores-error'></a> NSX-T Pre-Check Errand Fails Due to Edge Node Configuration

**Symptom**

You have configured your NSX-T Edge Node VM as `medium` size,
and the NSX-T Pre-Check Errand fails with the following error:
"_ERROR: NSX-T Precheck failed due to Edge Node ... no of cpu cores is less than 8_".

**Explanation**

The NSX-T Pre-Check Errand is erroneously returning the "_cpu cores is less than 8_" error.

**Solution**

You can safely configure your NSX-T Edge Node VMs as `medium` size and ignore the error.

#### <a id='1-7-0-windows-proxy'></a> Difficulty Changing Proxy for Windows Workers

You must configure a global proxy in the <%= vars.product_tile %> tile > **Networking** pane before you create any Windows workers that use the proxy.

You cannot change the proxy configuration for Windows workers in an existing cluster.

#### <a id='1-7-0-metrics-server'></a> Metrics Server uses Weak Ciphers

This issue is [fixed](#1-7-1-features) in <%= vars.k8s_runtime_abbr %> v1.7.1.

The `metrics-server` component communicates over TLS v1.2 with weak ciphers.
All other <%= vars.product_short %> components use TLS v1.2 with strong ciphers.

#### <a id='1-7-0-http-proxy-password'></a> Character Limitations in HTTP Proxy Password

For vSphere with NSX-T, the HTTP Proxy password field does not support the following special characters: `&` or `;`.

## <a id="management-console-1.7.2"></a> <%= vars.product_short %> Management Console 1.7.2

**Release Date**: September 28, 2020

### <a id="management-console-1.7.2-features"></a>Features

Other than support for <%= vars.product_short %> v1.7.2, <%= vars.product_short %> Management Console 1.7.2 has no new features.

### <a id="management-console-1.7.2-snapshot"></a>Product Snapshot

<p class="note"><strong>Note</strong>: <%= vars.product_short %> Management Console provides an opinionated installation of <%= vars.product_short %>. The supported versions may differ from or be more limited than what is generally supported by <%= vars.product_short %>.</p>

<table class="nice">
    <th>Element</th>
    <th>Details</th>
    <tr>
      <td>Version</td>
      <td>v1.7.2</td>
    </tr>
    <tr>
      <td>Release date</td>
      <td>September 28, 2020</td>
    </tr>
    <tr>
      <td>Installed Enterprise PKS version</td>
      <td>v1.7.2</td>
    </tr>
    <tr>
      <td>Installed Ops Manager version</td>
      <td>v2.9.10</td>
    </tr>
    <tr>
      <td>Installed Kubernetes version</td>
        <td>v1.16.12+vmware.1</td>
    </tr>
    <tr>
      <td>Compatible NSX-T versions</td>
      <td>v2.5.1, v2.5.0, v2.4.3</td>
    </tr>
    <tr>
      <td>Installed Harbor Registry version</td>
      <td>v1.10.3</td>
    </tr>
        <tr>
        <td>Windows stemcells</td>
        <td>v2019.15 and later</td>
    </tr>
</table>

### <a id='management-console-1-7-2-upgrade'></a>Upgrade Path

The supported upgrade path to <%= vars.product_short %> Management Console v1.7.2 is from <%= vars.product_short %> v1.6.0 and later.

### <a id="management-console-1.7.2-known-issues"></a> Known Issues

The <%= vars.product_short %> Management Console v1.7.2 appliance and user interface have the same [known issues](#management-console-1.7.0-known-issues) as v1.7.0.

## <a id="management-console-1.7.1"></a> <%= vars.product_short %> Management Console 1.7.1

**Release Date**: July 15, 2020

### <a id="management-console-1.7.1-features"></a>Features

Other than support for <%= vars.product_short %> v1.7.1, <%= vars.product_short %> Management Console 1.7.1 has no new features.

### <a id="management-console-1.7.1-snapshot"></a>Product Snapshot

<p class="note"><strong>Note</strong>: <%= vars.product_short %> Management Console provides an opinionated installation of <%= vars.product_short %>. The supported versions may differ from or be more limited than what is generally supported by <%= vars.product_short %>.</p>

<table class="nice">
    <th>Element</th>
    <th>Details</th>
    <tr>
      <td>Version</td>
      <td>v1.7.1</td>
    </tr>
    <tr>
      <td>Release date</td>
      <td>July 15, 2020</td>
    </tr>
    <tr>
      <td>Installed Enterprise PKS version</td>
      <td>v1.7.1</td>
    </tr>
    <tr>
      <td>Installed Ops Manager version</td>
      <td>v2.9.6</td>
    </tr>
    <tr>
      <td>Installed Kubernetes version</td>
        <td>v1.16.12+vmware.1</td>
    </tr>
    <tr>
      <td>Compatible NSX-T versions</td>
      <td>v2.5.1, v2.5.0, v2.4.3</td>
    </tr>
    <tr>
      <td>Installed Harbor Registry version</td>
      <td>v1.10.3</td>
    </tr>
        <tr>
        <td>Windows stemcells</td>
        <td>v2019.15 and later</td>
    </tr>
</table>

### <a id='management-console-1-7-1-upgrade'></a>Upgrade Path

The supported upgrade path to <%= vars.product_short %> Management Console v1.7.1 is from <%= vars.product_short %> v1.6.0 and later.

### <a id="management-console-1.7.1-known-issues"></a> Known Issues

The <%= vars.product_short %> Management Console v1.7.1 appliance and user interface have the same [known issues](#management-console-1.7.0-known-issues) as v1.7.0.

## <a id="management-console-1.7.0"></a> <%= vars.product_short %> Management Console 1.7.0

**Release Date**: April 16, 2020

### <a id="management-console-1.7.0-features"></a>Features

<%= vars.product_short %> Management Console v1.7.0 updates include:

* Role-based access control (RBAC). For information, see [Identity Management in the Management Console](./console/console-identity-management.html).
* Quota management. For information, see [Assign Resource Quotas to Users](./console/resource-quotas.html).
* Create, reconfigure, and delete clusters. For information, see [Create Clusters in the Management Console](./console/create-clusters.html).
* Network profiles. For information, see [Working with Network Profiles](./console/network-profile-console.html).
* Kubernetes profile support. For information, see [Create Clusters in the Management Console](./console/create-clusters.html).
* Ops Manager FQDN support. For information, see [Generate Configuration File and Deploy Enterprise PKS](./console/deploy-ent-pks-wizard.html#deploy).
* Hybrid NAT mode for bring your own topology (BYOT) deployments. For information, see [Configure a Bring Your Own Topology Deployment to NSX-T Data Center](./console/deploy-ent-pks-wizard.html#nsxt-byot).
* LDAP validation. For information, see [Use an External LDAP Server](./console/deploy-ent-pks-wizard.html#identity-ldap).
* Set multiple reserved IP ranges. For information, see [Generate Configuration File and Deploy Enterprise PKS](./console/deploy-ent-pks-wizard.html#deploy).
* Set DRS VM-host affinity rule to `SHOULD` if you select host groups for availability zones. For information, see [Configure Availability Zones](./console/deploy-ent-pks-wizard.html#availability-zones).
* Enable HA on the Linux worker nodes that provide services to Windows clusters. For information, see [Configure Plans](./console/deploy-ent-pks-wizard.html#plans).

### <a id="management-console-1.7.0-snapshot"></a>Product Snapshot

<p class="note"><strong>Note</strong>: <%= vars.product_short %> Management Console provides an opinionated installation of <%= vars.product_short %>. The supported versions may differ from or be more limited than what is generally supported by <%= vars.product_short %>.</p>

<table class="nice">
    <th>Element</th>
    <th>Details</th>
    <tr>
      <td>Version</td>
      <td>v1.7.0</td>
    </tr>
    <tr>
      <td>Release date</td>
      <td>April 16, 2020</td>
    </tr>
    <tr>
      <td>Installed Enterprise PKS version</td>
      <td>v1.7.0</td>
    </tr>
    <tr>
      <td>Installed Ops Manager version</td>
      <td>v2.8.5</td>
    </tr>
    <tr>
      <td>Installed Kubernetes version</td>
        <td>v1.16.7</td>
    </tr>
    <tr>
      <td>Compatible NSX-T versions</td>
      <td>v2.5.1, v2.5.0, v2.4.3</td>
    </tr>
    <tr>
      <td>Installed Harbor Registry version</td>
      <td>v1.10.1</td>
    </tr>
    <tr>
        <td>Windows stemcells</td>
        <td>v2019.15 and later</td>
    </tr>
</table>

### <a id='management-console-1-7-0-upgrade'></a>Upgrade Path

The supported upgrade path to <%= vars.product_short %> Management Console v1.7.0 is from <%= vars.product_short %> v1.6.0 and later.

### <a id="management-console-1.7.0-known-issues"></a> Known Issues

The <%= vars.product_short %> Management Console v1.7.0 appliance and user interface have the following known issues.

#### <a id="management-console-1.7.0-vrli-https"></a> vRealize Log Insight Integration Does Not Support HTTPS Connections

**Symptom**

The <%= vars.product_short %> Management Console integration to vRealize Log Insight does not support connections to the HTTPS port on the vRealize Log Insight server.

**Workaround**

1. Use SSH to log in to the <%= vars.product_short %> Management Console appliance VM.
1. Open the file `/lib/systemd/system/pks-loginsight.service` in a text editor.
1. Add the following to the file:

    ```
    -e LOG_SERVER_ENABLE_SSL_VERIFY=false
    -e LOG_SERVER_USE_SSL=true
    ```

    The resulting file should look like the following example:

    ```
    ExecStart=/bin/docker run --privileged --restart=always --network=pks
    -v /var/log/journal:/var/log/journal
    --name=pks-loginsight
    -e TYPE=gear2-vm
    -e LOG_SERVER_HOST=${LOGINSIGHT_HOST}
    -e LOG_SERVER_PORT=${LOGINSIGHT_PORT}
    -e LOG_SERVER_ENABLE_SSL_VERIFY=false
    -e LOG_SERVER_USE_SSL=true
    -e LOG_SERVER_AGENT_ID=${LOGINSIGHT_ID}
    pksoctopus/vrli-journald:v07092019
    ```

1. Save the file.
1. Run `systemctl daemon-reload`.
1. To restart the vRealize Log Insight service, run `systemctl restart pks-loginsight.service`.

<%= vars.product_short %> Management Console can now send logs to the HTTPS port on the vRealize Log Insight server.

#### <a id="management-console-1.7.0-k8s-profile"></a> Base64 encoded file arguments are not decoded in Kubernetes profiles

**Symptom**

Some file arguments in Kubernetes profiles are base64 encoded. When the management console displays the Kubernetes profile, some file arguments are not decoded.

**Workaround**

Run `echo "$content" | base64 --decode`

#### <a id="management-console-1.7.0-opsman-fqdn"></a> Cannot specify Ops Manager EQDN addresses during upgrade.

**Symptom**

This release allows you to specify an FQDN for the Ops Manager VM during deployment of <%= vars.product_short %>. This only applies to new deployments. You cannot specify an FQDN during upgrade.

**Workaround**

None

#### <a id="management-console-1.7.0-network-profile"></a> Network profiles not immediately selectable

**Symptom**

If you create network profiles and then try to apply them in the Create Cluster page, the new profiles are not available for selection.

**Workaround**

Log out of the management console and log back in again.

#### <a id="management-console-1.7.0-cluster-summary"></a> Real-Time IP information not displayed for network profiles.

**Symptom**

In the cluster summary page, only default IP pool, pod IP block, node IP block values are displayed, rather than the real-time values from the associated network profile.

**Workaround**

None
