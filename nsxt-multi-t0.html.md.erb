---
title: Isolating Tenants
owner: TKGI
---

This topic describes how to isolate tenants in <%= vars.product_full %> (<%= vars.k8s_runtime_abbr %>) multi-tenant environments.  


<br>
## <a id='about'></a> About Tenant Isolation

You can isolate a cluster and its workloads using NSX-T Tier-0 (T0) logical routers or VRF Tier-0 gateways:  

* [Using a Multi-T0 Router Configuration for Tenant Isolation](#about-t0-router-isolation)  
* [Using a VRF Tier-0 Gateway Configuration for Tenant Isolation](#about-vrf-gateway-isolation)  

<br>
### <a id='about-t0-router-isolation'></a> Using a Multi-T0 Router Configuration for Tenant Isolation

<%= vars.product_short %> multi-T0 lets you provision, manage, and secure Kubernetes cluster deployments on isolated tenant networks. 
As shown in the diagram below, instead of having a single T0 router, there are multiple T0 routers. 
The Shared Tier-0 router handles traffic between the <%= vars.k8s_runtime_abbr %> management network and the vSphere standard network 
where vCenter and NSX-T Manager are deployed. 
There are two Tenant Tier-0 routers that connect to the Shared Tier-0 over an NSX-T logical switch using a virtual LAN (VLAN) or Overlay transport zone. 
Using each dedicated T0, Kubernetes clusters are deployed in complete isolation on each tenant network.

  <img src="images/nsxt/mt0/mt0-03.png" alt="Multi-T0 Router">

To isolate a cluster and its workloads behind T0 routers:  

* [Prerequisites](#prereqs)  
* [Configure Multi-T0 Router-Based Tenant Isolation](#base-config)  
* [Configure Multi-T0 Security](#security-config)  



<br>
### <a id='about-vrf-gateway-isolation'></a> Using a VRF Tier-0 Gateway Configuration for Tenant Isolation

<%= vars.product_short %> on vSphere with NSX-T Policy API also supports provisioning, managing, and securing Kubernetes cluster deployments using a VRF gateway.  

As shown in the diagram below, instead of using one or more T0 routers, clusters are isolated behind a VRF gateway. 
The Shared Tier-0 router handles traffic between the <%= vars.k8s_runtime_abbr %> management network and the 
vSphere standard network where vCenter and NSX-T Manager are deployed. 
Using Tenant VRF Tier-0 gateways to connect to the Shared Tier-0, Kubernetes clusters are deployed in complete isolation on tenant networks.  

<img src="images/nsxt/mt0/mt0-03-vrf.png" alt="VRF Tier-0 Gateway">

To isolate a cluster and its workloads behind a VRF gateway:  

* [Prerequisites](#prereqs)  
* [Configure VRF Tier-0 Gateway-Based Tenant Isolation](#base-config-vrf)  
<% if vars.product_version == "COMMENTED"  %>
* [Configure VRF Gateway Security](#security-config-vrf)  
<% end %>

<br>
## <a id="prereqs"></a>Prerequisites

The prerequisites for tenant isolation depend on the configuration used:  

* [Multi-T0-Based Tenant Isolation Prerequisites](#prereqs-multi-t0)  
* [VRF Tier-0 Gateway-Based Tenant Isolation Prerequisites](#prereqs-vrf)  



### <a id="prereqs-multi-t0"></a>Multi-T0-Based Tenant Isolation Prerequisites

To implement Multi-T0-based tenant isolation, verify the following prerequisites:

* Supported version of vSphere IaaS is installed. See <a href="./vsphere-nsxt-requirements.html#vsphere-version">vSphere with NSX-T Version Requirements</a>.
* Supported version of VMware NSX-T Data Center is installed. See <a href="./vsphere-nsxt-requirements.html#tkgi-with-nsx-t">vSphere with NSX-T Version Requirements</a>.
* If you are using NAT mode for the Shared Tier-0 router, review [Considerations for NAT Topology on Shared Tier-0](#consider-nat-shared) and [Considerations for NAT Topology on Tenant Tier-0](#consider-nat-tenant) before proceeding.


### <a id="prereqs-vrf"></a>VRF Tier-0 Gateway-Based Tenant Isolation Prerequisites

To implement VRF Tier-0 Gateway-based tenant isolation:

* <%= vars.k8s_runtime_abbr %> on vSphere with NSX-T Policy API.  
* Three VLANs for the VRF Tier-0 gateway.  


<br>
## <a id="base-config"></a> Configure Multi-T0 Router-Based Tenant Isolation

To isolate tenants using a multi-T0 router-based configuration:

1. [Plan and Provision Additional NSX-T Edge Nodes for Each Multi-T0 Router](#edge-nodes)  
1. [Configure Inter-T0 Logical Switch](#logical-switch)  
1. [Configure a New Uplink Interface on the Shared Tier-0 Router](#router-port)  
1. [Provision Tier-0 Router for Each Tenant](#provision)  
1. [Create Two Uplink Interfaces on Each Tenant Tier-0 Router](#port-interfaces)  
1. [Verify the Status of the Shared and Tenant Tier-0 Routers](#port-interfaces)  
1. [Configure Static Routes](#static-routes)  
1. [Considerations for NAT Topology on Shared Tier-0](#consider-nat-shared)  
1. [Considerations for NAT Topology on Tenant Tier-0](#consider-nat-tenant)  
1. [Configure BGP on Each Tenant Tier-0 Router](#bgp)  
1. [Configure BGP on the Shared Tier-0 Router](#bgp-shared)  
1. [Test the Base Configuration](#test)  


### <a id="edge-nodes"></a>Step 1: Plan and Provision Additional NSX-T Edge Nodes for Each Multi-T0 Router

Multi-T0 requires a minimum of four NSX-T Edge Nodes: Configure two nodes per T0. Use the T0 attached to the <%= vars.k8s_runtime_abbr %> management plane as the Shared Tier-0 router that connects all T0 routers. In addition, deploy an additional T0 router for each tenant you want to isolate.

  <img src="images/nsxt/mt0/mt0-02.png" alt="Multi-T0 Router">

Each Tenant Tier-0 router requires a minimum of two NSX-T Edge Nodes. The formula for determining the minimum number of nodes for all tenants is as follows:

```
2 + (TENANTS x 2)
```

Where `TENANTS` is the number of tenants you want to isolate.

For example, if you want to isolate three tenants, use the following calculation:

```
2 + (3 x 2) = 8 NSX-T Edge Nodes
```

To isolate ten tenants, use the following calculation:

```
2 + (10 x 2) = 22 NSX-T Edge Nodes
```

Using the NSX-T Manager interface, deploy at least the minimum number of Edge Nodes you need for each Tenant Tier-0 and join these Edge Nodes to an Edge Cluster. For more information, see [Installing and Configuring NSX-T Data Center v3.0 for <%= vars.k8s_runtime_abbr %>](./nsxt-3-0-install.html).

<p class="note"><strong>Note</strong>: An Edge Cluster can have a maximum of 10 Edge Nodes. If the provisioning requires more Edge Nodes than what a single Edge Cluster can support, multiple Edge Clusters must be deployed.</p>

### <a id="logical-switch"></a>Step 2: Configure Inter-T0 Logical Switch

Connect all NSX-T Edge Nodes using an overlay logical switch. This overlay network is used to transport traffic between the T0 routers. Plan to allocate a network of sufficient size to accommodate all Tier-0 router interfaces that need to be connected to this network. You must allocate each T0 router one or more IP addresses from that range.

For example, if you plan to deploy two Tenant Tier-0 routers, a subnet with prefix size /28 may be sufficient, such as `50.0.0.0/28`.

Once you have physically connected the Edge Nodes, define a logical switch to connect the Shared Tier-0 router to the Tenant Tier-0 router or routers.

To define a logical switch based on an Overlay or VLAN transport zone, follow the steps below:

1. In NSX-T Manager, go to **Networking** > **Switching** > **Switches**.
1. Click **Add** and create a logical switch (LS).
1. Name the switch descriptively, such as `inter-t0-logical-switch`.
1. Connect the logical switch to the transport zone defined when deploying NSX-T. See [Installing and Configuring NSX-T Data Center v3.0 for <%= vars.k8s_runtime_abbr %>](./nsxt-3-0-install.html).

### <a id="router-port"></a>Step 3: Configure a New Uplink Interface on the Shared Tier-0 Router

The Shared Tier-0 router already has an uplink interface to the external (physical) network that was configured when it was created. For more information, see [Installing and Configuring NSX-T Data Center v3.0 for <%= vars.k8s_runtime_abbr %>](./nsxt-3-0-install.html).

To enable Multi-T0, you must configure a second uplink interface on the Shared Tier-0 router that connects to the inter-T0 network (`inter-t0-logical-switch`, for example). To do this, complete the following steps:

1. In NSX-T Manager, go to **Networking > Routers**.
1. Select the Shared Tier-0 router.
1. Select **Configuration** > **Router Ports** and click **Add**.
1. Configure the router port as follows:
	1. For the logical switch, select the inter-T0 logical switch you created in the previous step (for example, `inter-t0-logical-switch`).
	1. Provide an IP address from the allocated range. For example, `50.0.0.1/24`.

### <a id="provision"></a>Step 4: Provision Tier-0 Router for Each Tenant

Create a Tier-0 logical router for each tenant you want to isolate. For more information, see [Create Tier-0 Router](./nsxt-3-0-install.html#nsxt30-t0-router-create) in _Installing and Configuring NSX-T Data Center v3.0 for <%= vars.k8s_runtime_abbr %>_.

When creating each Tenant Tier-0 router, make sure you set the router to be active/passive, and be sure to name the logical switch descriptively, such as `t0-router-customer-A`.

### <a id="port-interfaces"></a>Step 5: Create Two Uplink Interfaces on Each Tenant Tier-0 Router

Similar to the Shared Tier-0 router, each Tenant Tier-0 router requires at a minimum two uplink interfaces.

- The first uplink interface provides an uplink connection from the Tenant Tier-0 router to the tenant's corporate network.

- The second uplink interface provides an uplink connection to the Inter-T0 logical switch that you configured. For example, `inter-t0-logical-switch`.

For instructions, see [Create Tier-0 Router](./nsxt-3-0-install.html#nsxt30-t0-router-create) in _Installing and Configuring NSX-T Data Center v3.0 for <%= vars.k8s_runtime_abbr %>_. When creating the uplink interface that provides an uplink connection to the Inter-T0 logical switch, be sure to give this uplink interface an IP address from the allocated pool of IP addresses.

### <a id="port-interfaces"></a>Step 6: Verify the Status of the Shared and Tenant Tier-0 Routers

When you have completed the configuration of the Shared and Tenant Tier-0 routers as described above, verify your progress up to this point. On the Shared Tier-0 router, you should have two uplink interfaces, one to the external network and the other to the inter-T0 logical switch. On the Tenant Tier-0 router, you should have two uplink interfaces, one to the inter-T0 logical switch and the other to the external network. Each uplink interface is connected to a transport node.

The images below provide an example checkpoint for verifying the uplink interfaces for the Shared and Tenant Tier-0 routers. In this example, the Shared Tier-0 has one uplink interface at `10.40.206.10/25` on the transport Edge Node `edge-TN1`, and the second uplink interface at `10.40.206.9/25` on the transport Edge Node `edge-TN2`.

  <img src="images/nsxt/mt0/uplink-interfaces-shared.png" alt="Shared-T0 Uplink Interfaces">

Similarly, the Tenant Tier-0 has one uplink interface at `10.40.206.13/25` on the transport Edge Node `edge-TN3`, and the second uplink interface at `10.40.206.14/25` on the transport Edge Node `edge-TN4`.

  <img src="images/nsxt/mt0/uplink-interfaces-tenant.png" alt="Tenant-T0 Uplink Interfaces">

### <a id="static-routes"></a>Step 7: Configure Static Routes

For each T0 router, including the Shared Tier-0 and all Tenant Tier-0 routers, define a static route to the external network. For instructions, see [Create Tier-0 Router](./nsxt-3-0-install.html#nsxt30-t0-router-create) in _Installing and Configuring NSX-T Data Center v3.0 for <%= vars.k8s_runtime_abbr %>_.

For the Shared Tier-0 router, the default static route points to the external management components such as vCenter and NSX-T Manager and provides internet connectivity. As shown in the image below, the Shared Tier-0 defines a static route for vCenter and NSX-T Manager as `192.168.201.0/24`, and the static route for internet connectivity as `0.0.0.0/0`:

![T0-shared-route](images/nsxt/mt0/nsxt-static-routes-01.png)

For each Tenant Tier-0 router, the default static route should point to the tenant's corporate network. As shown in the image below, the Tenant Tier-0 defines a static route to the corporate network as `0.0.0.0/0`:

![T0-customer-route](images/nsxt/mt0/nsxt-static-routes-02.png)

### <a id="consider-nat-shared"></a>Step 8: Considerations for NAT Topology on Shared Tier-0

The Multi-T0 configuration steps documented here apply to deployments where NAT mode is **not** used on the Shared Tier-0 router. For more information, see <a href="./nsxt-topologies.html">NSX-T Deployment Topologies for <%= vars.product_short %></a>.

For deployments where NAT-mode is used on the Shared Tier-0 router, additional provisioning steps must be followed to preserve NAT functionality to external networks while bypassing NAT rules for traffic flowing from the Shared Tier-0 router to each Tenant Tier-0 router.

Existing <%= vars.product_short %> deployments where NAT mode is configured on the Shared Tier-0 router cannot be re-purposed to support a Multi-T0 deployment following this documentation.

### <a id="consider-nat-tenant"></a>Step 9: Considerations for NAT Topology on Tenant Tier-0

<p class="note"><strong>Note</strong>: This step only applies to NAT topologies on the Tenant Tier-0 router. For more information on NAT mode, see <a href="./nsxt-topologies.html">NSX-T Deployment Topologies for <%= vars.k8s_runtime_abbr %></a>.</p>

<p class="note"><strong>Note</strong>: NAT mode for Tenant Tier-0 routers is enabled by defining a non-routable custom Pods IP Block using a Network Profile. For more information, see <a href="./network-profiles-define.html">Defining Network Profiles</a>.</p>

In a Multi-T0 environment with NAT mode, traffic on the Tenant Tier-0 network going from Kubernetes cluster nodes to <%= vars.k8s_runtime_abbr %> management components residing on the Shared Tier-0 router must bypass NAT rules. This is required because TKGI-managed components such as BOSH Director connect to Kubernetes nodes based on routable connectivity without NAT.

To avoid NAT rules being applied to this class of traffic, you need to create two high-priority **NO_SNAT** rules on each Tenant Tier-0 router. These NO_SNAT rules allow "selective" bypass of NAT for the relevant class of traffic, which in this case is connectivity from Kubernetes node networks to <%= vars.k8s_runtime_abbr %> management components such as the <%= vars.control_plane %>, Ops Manager, and BOSH Director, as well as to infrastructure components such as vCenter and NSX-T Manager.

For each Tenant Tier-0 router, define two NO_SNAT rules to classify traffic. The source for both rules is the [Nodes IP Block](./nsxt-prepare-env.html#plan-ip-blocks) CIDR. The destination for one rule is the <%= vars.k8s_runtime_abbr %> Management network where <%= vars.k8s_runtime_abbr %>, Ops Manager, and BOSH Director are deployed. The destination for the other rule is the external network where NSX-T Manager and vCenter are deployed.

For example, the following image shows two NO_SNAT rules created on a Tenant Tier-0 router. The first rule un-NATs traffic from Kubernetes nodes (`30.0.128.0/17`) to the <%= vars.k8s_runtime_abbr %> management network (`30.0.0.0/24`). The second rule un-NATs traffic from Kubernetes nodes (`30.0.128.0/17`) to the external network (`192.168.201.0/24`).

![NO_SNAT Example 1](images/nsxt/mt0/no-snat-01.png)

![NO_SNAT Example 2](images/nsxt/mt0/no-snat-02.png)

The end result is two NO_SNAT rules on each Tenant Tier-0 router that bypass the NAT rules for the specified traffic.

![NO_SNAT Example 2](images/nsxt/mt0/snat-01.png)

### <a id="bgp"></a>Step 10: Configure BGP on Each Tenant Tier-0 Router

Use Border Gateway Protocol (BGP) to route redistribution and filtering across all Tier-0 routers. 
BGP allows the Shared Tier-0 router to dynamically discover the location of Kubernetes clusters (Node networks) 
deployed on each Tenant Tier-0 router.  

To configure BGP on each tenant Tier-0 router:  

* [Considerations When Configuring BGP on Tenant Tier-0 Routers](#bgp-considerations)  
* [Configure BGP AS Number](#bgp-as-config)  
* [Configure BGP Route Distribution](#bgp-route-tenant)  
* [Configure IP Prefix Lists](#ip-prefix-tenant)  
* [Configure BGP Peer](#bgp-peer-tenant)  


### <a id="bgp-considerations"></a>Considerations When Configuring BGP on Tenant Tier-0 Routers

In a Multi-T0 deployment, special consideration must be given to the network design to preserve reliability and fault tolerance of the Shared and Tenant Tier-0 routers.

![BGP for Multi-T0](images/nsxt/mt0/bgp-01.png)

Failover of a logical router is triggered when the router is losing all of its BGP sessions. If multiple BGP sessions are established across different uplink interfaces of a Tier-0 router, failover will only occur if **all** such sessions are lost. Thus, to ensure high availability on the Shared and Tenant Tier-0 routers, BGP can only be configured on uplink interfaces facing the Inter-Tier-0 network. This configuration is shown in the diagram below.

<p class="note"><strong>Note</strong>: In a Multi-T0 deployment, BGP cannot be configured on external uplink interfaces. Uplink external connectivity must use VIP-HA with NSX-T to provide high availability for external interfaces. For more information, see <a href="./nsxt-3-0-install.html#nsxt30-edge-nodes">Deploy NSX-T Edge Nodes</a> in <em>Installing and Configuring NSX-T Data Center v3.0 for <%= vars.k8s_runtime_abbr %></em>.</p>

![Tier-0 HA](images/nsxt/mt0/Tier-0-HA.png)

You must configure BGP routing on each Tier-0 router. The steps that follow are for each Tenant Tier-0 router. The instructions for the Shared Tier-0 are provided in subsequent steps. As a prerequisite, assign a unique Autonomous System Number to each Tier-0 router. Each AS number you assign must be private within the range `64512-65534`. For more information, see <a href="https://docs.vmware.com/en/VMware-NSX-T-Data-Center/2.4/administration/GUID-4BC07C6A-DC62-496B-A6A9-7219ACF5AC7B.html">Configure BGP on a Tier-0 Logical Router</a> in the NSX-T documentation.

<p class="note"><strong>Note</strong>: To configure BGP for the Tenant Tier-0, you will need to use the Shared Tier-0 AS number. As such, identify the AS numbers you will use for the Tenant and Shared Tier-0 routers before proceeding.</p>



#### <a id="bgp-as-config"></a>Configure BGP AS Number

Once you have chosen the AS number for the Tenant Tier-0 router, configure BGP with the chosen AS number as follows:

1. In NSX-T Manager, select **Networking** > **Routers**.
1. Select the Tenant Tier-0 router.
1. Select **Routing** > **BGP**, the click **ADD**.
1. Add the AS number to the BGP configuration in the `local AS` field.
1. Click on the `enabled` slider to activate BGP.
1. Lastly, deactivate the ECMP slider.

#### <a id="bgp-route-tenant"></a>Configure BGP Route Distribution

To configure BGP route distribution for each Tenant Tier-0 router, follow the steps below:

1. In NSX-T Manager, select the Tenant Tier-0 router.
1. Select **Routing** > **Route Redistribution**.
  ![Route Distribution 1](images/nsxt/mt0/bgp-02.png)
1. Click **Add** and configure as follows:
	1. **Name**: NSX Static Route Redistribution
	1. **Sources**: Select **Static**, **NSX Static**, and **NSX Connected**

#### <a id="ip-prefix-tenant"></a>Configure IP Prefix Lists

In this step you define an **IP Prefix List** for each Tenant Tier-0 router to advertise any Kubernetes node network of standard prefix size /24, as specified by the less-than-or-equal-to (le) and greater-than-or-equal-to (ge) modifiers in the configuration. The CIDR range to use for the definition of the list entry is represented by the Nodes IP Block network, for example `30.0.0.0/16`.

For more information about IP Prefix Lists, see <a href="https://docs.vmware.com/en/VMware-NSX-T-Data-Center/2.4/administration/GUID-FCA01567-73E6-4EE1-AC76-74EDC24CFA29.html">Create an IP Prefix List</a> in the NSX-T documentation.

To configure an IP Prefix List for each Tenant Tier-0 router, follow the steps below:  

1. In NSX-T Manager, select the Tenant Tier-0 router.  
1. Select **Routing** > **IP Prefix Lists**.  
1. Click **Add** and configure as follows:  
	1. **Name**: Enter a descriptive name.  
	1. Click **Add** and create a **Permit** rule that allows redistribution of the exact /24 network, carved from the **Nodes IP Block**.  
	1. Click **Add** and create a **Deny** rule that denies everything else on the network `0.0.0.0/0`.  
	![IP Prefix Example 2](images/nsxt/mt0/ip-prefix-01.png)

#### <a id="bgp-peer-tenant"></a>Configure BGP Peer

To configure BGP peering for each Tenant Tier-0 router, follow the steps below:

1. In NSX-T Manager, select the Tenant Tier-0 router.
1. Go to **Routing** > **BGP**.
1. Click **Add** and configure the BGP rule as follows:
	1. **Neighbor Address**: Enter the IP address of the Shared Tier-0 router.
	1. **Local Address**: Select the individual uplink interfaces facing the inter-tier0 logical switch.
	1. **Address Families**: Click **Add** and configure as follows:  
		1. **Type**: `IPV4_UNICAST`.  
		1. **State**: `Enabled`.  
		1. **Out Filter**: Select the IP Prefix List created above.  
		1. Click **Add**.  
	1. Back at the **Routing** > **BGP** screen:  
		1. Enter the Shared Tier-0 AS number.  
		1. After creating the BGP neighbor, select **Edit** and click **Enable BGP**.  

### <a id="bgp-shared"></a>Step 11: Configure BGP on the Shared Tier-0 Router

The configuration of BGP on the Shared Tier-0 is similar to the BGP configuration each Tenant Tier-0, with the exception of the IP Prefix list that permits traffic to the <%= vars.k8s_runtime_abbr %> management network where <%= vars.k8s_runtime_abbr %>, BOSH, and Ops Manager are located.

As with each Tenant Tier-0 router, you will need to assign a unique private AS number within the private range `64512-65534` to the Shared Tier-0 router. Once the AS number is assigned, use NSX-T Manager to configure the following BGP rules for the Shared Tier-0 router.

#### <a id="bgp-as-config"></a>Configure BGP AS Number

To configure BGP on the Shared Tier-0 with the AS number, complete the corresponding set of instructions in the tenant BGP section above.

#### <a id="bgp-route-shared"></a>Configure BGP Route Distribution

To configure BGP route distribution for the Shared Tier-0 router, complete the corresponding set of instructions in the BGP tenant section above.

#### <a id="ip-prefix-shared"></a>Configure IP Prefix Lists

To configure IP prefix lists for each Tenant Tier-0 router, follow the steps below:

1. In NSX-T Manager, select the Tenant Tier-0 router.
1. Select **Routing** > **IP Prefix Lists**.
1. Click **Add** and configure as follows:
	1. **Name**: Enter a descriptive name.
	1. Click **Add** and create a **Permit** rule for the infrastructure components vCenter and NSX-T Manager.
	1. Click **Add** and create a **Permit** rule for the <%= vars.k8s_runtime_abbr %> management components (<%= vars.k8s_runtime_abbr %>, Ops Manager, and BOSH).
	1. Click **Add** and create a **Deny** rule that denies everything else on the network `0.0.0.0/0`.
  ![IP Prefix Lists](images/nsxt/mt0/ip-prefix-03.png)

#### <a id="bgp-peer-shared"></a>Configure BGP Peer

1. In NSX-T Manager, select the Tenant Tier-0 router.
1. Go to **Routing** > **BGP**.
1. Click **Add** and configure the BGP rule as follows:
	1. **Neighbor Address**: Enter the IP address of the Shared Tier-0 router.
	1. **Local Address**: Select **All Uplinks**.
	1. **Address Families**: Click **Add** and configure as follows:
		1. **Type**: IPV4_UNICAST
		1. **State**: Enabled
		1. **Out Filter**: Select the IP Prefix List that includes the network where vCenter and NSX-T Manager are deployed, as well as the network where the <%= vars.k8s_runtime_abbr %> management plane is deployed.
		1. Click **Add**.
	1. Back at the **Routing** > **BGP** screen:
		1. Enter the Tenant Tier-0 AS number.
		1. After creating the BGP neighbor, select **Edit** and click **Enable BGP**.

<p class="note"><strong>Note</strong>: You must repeat this step for each Tenant Tier-0 router you want to peer with the Shared Tier-0 router.</p>

### <a id="test"></a>Step 12: Test the Base Configuration

Perform the following validation checks on all Tier-0 routers:  

* [Shared Tier-0 Validation](#test-shared-tier-0)  
* [Tenant Tier-0 Validation](#test-tenant-tier-0)  

Perform the validation checks on the Shared Tier-0 first followed by each Tenant Tier-0 router. 
For each Tier-0, the validation should alternate among checking for the BGP summary and the router Routing Table.  


#### <a id="test-shared-tier-0"></a>Shared Tier-0 Validation

Verify that the Shared Tier-0 has an active peer connection to each Tenant Tier-0 router. 

To verify BGP Peering:  

1. In NSX-T Manager, select the Shared Tier-0 router and choose **Actions** > **Generate BGP Summary**.
1. Validate that the Shared Tier-0 router has one active peer connection to each Tenant Tier-0 router.

Verify that the Shared Tier-0 routing table includes all BGP routes to each Shared Tier-0:

1. In NSX-T Manager, select **Networking** > **Routers** > **Routing**.
1. Select the Shared Tier-0 router and choose **Actions** > **Download Routing Table**.
1. Download the routing table for the Shared Tier-0 and verify the routes.

#### <a id="test-tenant-tier-0"></a>Tenant Tier-0 Validation

Verify that the Shared Tier-0 has an active peer connection to each Tenant Tier-0 router. 

To verify BGP Peering:

1. In NSX-T Manager, select the Tenant Tier-0 router and choose **Actions** > **Generate BGP Summary**.
1. Validate that the Tenant Tier-0 router has one active peer connection to the Shared Tier-0 router.
1. Repeat for all other Tenant Tier-0 routers.

Verify that the T0 routing table for each Tenant Tier-0 includes all BGP routes to reach vCenter, 
NSX-T Manager, and the <%= vars.k8s_runtime_abbr %> management network:

1. In NSX-T Manager, select **Networking** > **Routers** > **Routing**.
1. Select the T0 router and choose **Actions** > **Download Routing Table**.
1. Download the routing table for each of the Tenant Tier-0 routers.

<p class="note"><strong>Note</strong>: At this point, the Shared Tier-0 has no BGP routes because you have not deployed any Kubernetes clusters. The Shared Tier-0 will show BGP routes when you deploy Kubernetes clusters to the Tenant Tier-0 routers. Each Tenant Tier-0 router shows a BGP exported route that makes each Tenant Tier-0 router aware of the <%= vars.k8s_runtime_abbr %> management network and other external networks where NSX-T and vCenter are deployed.</p>


<br>
## <a id="security-config"></a> Configure Multi-T0 Security

In a multi-T0 environment, you can secure two types of traffic:

- Traffic between tenants. See [Secure Inter-Tenant Communications](#secure-inter-tenant).  
- Traffic between clusters in the same tenant. See [Secure Intra-Tenant Communications](#secure-intra-tenant).  

### <a id="secure-inter-tenant"></a> Secure Inter-Tenant Communications

Securing traffic between tenants isolates each tenant and ensures the traffic between the Tenant Tier-0 routers and the Shared Tier-0 router is restricted to the legitimate traffic path.  

To secure traffic between tenants:

1. [Define IP Sets](#ip-sets)  
1. [Create Edge Firewall](#edge-firewall)  
1. [Add Firewall Rules](#firewall-rules)  
1. [Create DFW Section](#dfw-section)  
 

#### <a id="ip-sets"></a>Step 1: Define IP Sets

In NSX-T an **IP Set** is a group of IP addresses that you can use as sources and destinations in firewall rules. For a Multi-T0 deployment you need to create several IP Sets as described below. For more information about creating IP Sets, see <a href="https://docs.vmware.com/en/VMware-NSX-T-Data-Center/2.4/administration/GUID-99F67483-8584-4ECC-A948-29E3C857619C.html">Create an IP Set</a> in the NSX-T documentation.

The image below shows a summary of the three required IP Sets you will need to create for securing Multi-T0 deployments:

![IP Set Summary](images/nsxt/mt0/ip-set-07.png)

First, define an IP Set that includes the IP addresses for the NSX-T Manager and vCenter hosts. In the following IP Set example, `192.168.201.51` is the IP address for NSX and `192.168.201.20` is the IP address for vCenter.

![NSX and VC IP Set](images/nsxt/mt0/ip-set-01.png)

Next, define an IP Set that includes the network CIDR for <%= vars.k8s_runtime_abbr %> management components. In the following IP Set example, `30.0.0.0/24` is the CIDR block for the <%= vars.k8s_runtime_abbr %> Management network.

![<%= vars.k8s_runtime_abbr %> Admin CIDR IP Set](images/nsxt/mt0/ip-set-02.png)

Lastly, define an IP Set for the Inter-T0 CIDR created during the base configuration.

![<%= vars.k8s_runtime_abbr %> Admin CIDR IP Set](images/nsxt/mt0/ip-set-08.png)

<p class="note"><strong>Note</strong>: These are the minimum IP Sets you need to create. You may want to define additional IP Sets for convenience.</p>

#### <a id="edge-firewall"></a>Step 2: Create Edge Firewall

NSX-T Data Center uses Edge Firewall sections and rules to specify traffic handling in and out of the network. A firewall section is a collection of firewall rules. For more information, see <a href="https://docs.vmware.com/en/VMware-NSX-T-Data-Center/2.4/administration/GUID-22DF2616-8B3F-4E13-8116-B7501D2A8E6D.html">About Firewall Rules</a> in the NSX-T documentation.

For each Tenant Tier-0 router, create an Edge Firewall and section as follows:

1. In NSX Manager, go to **Networking > Routers**.
1. Select the Tenant Tier-0 router and click **Services > Edge Firewall**.
1. Select the **Default LR Layer 3 Section**.
1. Click **Add Section** > **Add Section Above**.
  ![Edge Firewall](images/nsxt/mt0/firewall-01.png)
1. Configure the section as follows:
	1. **Section Name**: Enter a unique name for the firewall section.
	1. **State**: **Stateful**
    ![Edge Firewall](images/nsxt/mt0/firewall-02.png)

#### <a id="firewall-rules"></a>Step 3: Add Firewall Rules

The last step is to define several firewall rules for the Edge Firewall. The firewall rules allow only legitimate control plane traffic to traverse the inter-Tier-0 logical switch, and deny all other traffic.

The following image shows a summary of the five firewall rules you will create:
![Edge Firewall](images/nsxt/mt0/firewall-03.png)

<p class="note"><strong>Note</strong>: All firewall rules are applied to the Inter-T0-Uplink interface.</p>

Select the Edge Firewall **Section** you just created, then select **Add Rule**. Add the following five firewall rules:

* [BGP Firewall Rule](#bgp-firewall-rule)  
* [Clusters Masters Firewall Rule](#masters-firewall-rule)  
* [Node Network to Management Firewall Rule](#nodes-firewall-rule)  
* [<%= vars.k8s_runtime_abbr %> Firewall Rule](#tkgi-firewall-rule)  
* [Deny All Firewall Rule](#deny-all-firewall-rule)  


##### <a id="bgp-firewall-rule"></a>BGP Firewall Rule

- **Name**: `BGP`
- **Direction**: in and out
- **Source**: IP Set defined for the Inter-T0 CIDR
- **Destination**: IP Set for Inter-T0 CIDR
- **Service**: Any
- **Action**: Allow
- Apply the rule to the Inter-T0-Uplink interface.
- Save the firewall rule.

##### <a id="masters-firewall-rule"></a>Clusters Masters Firewall Rule

The source for this firewall rule is a Namespace Group (NSGroup) you define in NSX Manager. The NSGroup is the Bootstrap Security Group specified in the Network Profile associated with this tenant. See <a href="./network-profiles-define.html#ns-groups">Bootstrap Security Group (NSGroup)</a>.

Once you have defined the NSGroup, configure the firewall rule as follows.

- **Name**: `Clusters-Masters-to-NSX-and-VC`
- **Direction**: out
- **Source**: NSGroup for Kubernetes Control Plane Nodes
- **Destination**: IP Set for Inter-T0 CIDR
- **Service**: Any
- **Action**: Allow
- Apply the rule to the Inter-T0-Uplink interface.
- Save the firewall rule.

##### <a id="nodes-firewall-rule"></a>Node Network to Management Firewall Rule

This firewall rule allows Kubernetes node traffic to reach <%= vars.k8s_runtime_abbr %> management VMs and the standard network.

- **Name**: `Node-Network-to-Management`
- **Direction**: out
- **Source**: IP Set defined for the Nodes IP Block network
- **Destination**: IP Sets defined for vCenter, NSX Manager, and <%= vars.k8s_runtime_abbr %> management plane components
- **Service**: Any
- **Action**: Allow
- Apply the rule to the Inter-T0-Uplink interface.
- Save the firewall rule.

##### <a id="tkgi-firewall-rule"></a><%= vars.k8s_runtime_abbr %> Firewall Rule

This firewall rule allows <%= vars.k8s_runtime_abbr %> management plane components to talk to Kubernetes nodes.

- **Name**: `TKGI-to-Node-Network`
- **Direction**: ingress
- **Source**: IP Set defined for the <%= vars.k8s_runtime_abbr %> management network
- **Destination**: IP Set defined for the Nodes IP Block network
- **Service**: Any
- **Action**: Allow
- Apply the rule to the Inter-T0-Uplink interface.
- Save the firewall rule.

##### <a id="deny-all-firewall-rule"></a>Deny All Firewall Rule

- **Name**: `Deny All`. This setting drops all other traffic that does not meet the criteria of the first three rules.
- **Direction**: in and out
- **Source**: Any
- **Destination**: Any
- **Service**: Any
- **Action**: Drop
- Apply the rule to the Inter-T0-Uplink interface.
- Save the firewall rule.

#### <a id="dfw-section"></a> (Optional) Step 4: Create DFW Section

To use distributed firewall (DFW) rules, you must create a DFW section for the DFW rule set.
The DFW section must exist before you create a Kubernetes cluster.

This optional step is recommended for inter-tenant security.
It is required for intra-tenant security as described in [Secure Intra-Tenant Communications](#secure-intra-tenant).
Because you need to create the DFW section only once,
you can use the DFW section you configure in this step when defining DFW rules for intra-tenant communications.

Even if you do not currently plan to use DFW rules, you can create the DFW section and
use it later if you decide to define DFW rules.
Those rules will apply to any cluster created after you define the DFW section for the tenant Tier-0 router.

<p class="note"><strong>Note:</strong> You must perform this procedure before you deploy a Kubernetes cluster to the target tenant Tier-0 router.</p>

1. In NSX Manager, navigate to **Security > DFW**, select the top-most rule, and click **Add Section Above**.
1. Configure the section as follows:
  1. In the **Section Name** field, enter a name for your DFW section.
  For example, `tkgi-dfw`.
  1. Use the defaults for all other settings on the **New Section** page.
  1. Navigate to the **Manage Tags** page and add a new tag.
      1. In the **Tag** field, enter `top`.
      1. In the **Scope** field, enter `ncp/fw_sect_marker`.

### <a id="secure-intra-tenant"></a> Secure Intra-Tenant Communications

To secure communication between clusters in the same tenancy, you must disallow any form of communication between
Kubernetes clusters created by <%= vars.k8s_runtime_abbr %>.
Securing inter-cluster communications is achieved by provisioning security groups and DFW rules.

<p class="note"><strong>Note</strong>: You must perform the global procedures, the first three steps described below, before you deploy a Kubernetes cluster to the target tenant Tier-0 router.</p>

To secure communication between clusters in the same tenancy:  

1. [Create NSGroup for All <%= vars.product_short %> Clusters](#ns-group)  
1. [Create DFW Section](#dfw-section)  
1. [Create NSGroups](#ns-groups)  
1. [Create DFW Rules](#dfw-rules) 


#### <a id="ns-group"></a>Step 1: Create NSGroup for All <%= vars.product_short %> Clusters

1. In NSX Manager, navigate to **Inventory > Groups > Groups** and **Add new group**.
1. Configure the new NSGroup as follows:
  1. In the **Name** field, enter `All-TKGI-Clusters`.
  1. In the **Membership Criteria** tab, add the following two criteria:
      1. For the first criterion, select **Logical switch**.
      1. For **Scope** > **Equals**, enter `pks/clusters`.
      1. For **Scope** > **Equals**, enter `pks/floating_ip`.
      1. For the second criterion, select **Logical switch**.
      1. For **Scope** > **Equals**, enter `ncp/cluster`.
      ![NSGroup-All-TKGI-Clusters](images/nsxt/mt0/nsgroup-for-all-pks-clusters.png)

<p class="note"><strong>Note</strong>: The <code>pks/clusters</code>, <code>pks/floating_ip</code>, or <code>ncp/cluster</code> values are the exact values you must enter when configuring <strong>Scope</strong> > <strong>Equals</strong>. They map to NSX-T objects.</p>

After you configure the `All-TKGI-Clusters` NSGroup, the **Membership Criteria** tab looks as follows:

![NSGroup-All-TKGI-Clusters](images/nsxt/mt0/nsgroup-all-pks-clusters.png)

#### <a id="dfw-section"></a>Step 2: Create DFW Section

Before you create distributed firewall rules,
you must create a DFW section for the DFW rule set you define later.

To create a DFW section, follow the instructions in [Create DFW Section](#dfw-section).

#### <a id="ns-groups"></a>Step 3: Create NSGroups

Before creating NSGroups, retrieve the UUID of the cluster that you want to secure.
To retrieve the cluster UUID, run the `tkgi cluster YOUR-CLUSTER-NAME` command.
For more information about the <%= vars.k8s_runtime_abbr %> CLI, see [<%= vars.k8s_runtime_abbr %> CLI](./cli/index.html).

##### Create NSGroup for Cluster Nodes

1. In NSX Manager, navigate to **Inventory > Groups > Groups** and click **Add new group**.
1. Configure the new NSGroup as follows:
  1. In the **Name** field, enter the cluster UUID or cluster name and append `-nodes` to the end of the name to distinguish it.
  The cluster name must be unique.
  1. In the **Membership Criteria** tab, add the following criterion:
      1. Select **Logical Switch**.
      1. For **Tag** > **Equals**, enter `tkgi-cluster-YOUR-CLUSTER-UUID`.
      1. For **Scope** > **Equals**, enter `pks/cluster`.
      1. For **Scope** > **Equals**, enter `pks/floating_ip`. For this scope,
      leave the **Tag** field empty as shown in the image below.
      ![NSGroup-Nodes](images/nsxt/mt0/nsgroup-nodes.png)

After you configure the NSGroup for cluster nodes, the **Membership Criteria** tab looks as follows:

![NSGroup-Nodes](images/nsxt/mt0/nsgroup-for-nodes.png)

##### Create NSGroup for Cluster Pods

1. In NSX Manager, navigate to **Inventory > Groups > Groups** and click **Add new group**.
1. Configure the new NSGroup as follows:
  1. In the **Name** field, enter the cluster UUID or cluster name and append `-pods` to the end of the name to distinguish it.
  The cluster name must be unique.
  1. In the **Membership Criteria** tab, add the following criterion:
      1. Select **Logical Port**.
      1. For **Tag** > **Equals**, enter `tkgi-cluster-YOUR-CLUSTER-UUID`.
      1. For **Scope** > **Equals**, enter `ncp/cluster`.
      ![NSGroup-Pods](images/nsxt/mt0/nsgroup-pods.png)

After you configure the NSGroup for cluster pods, the **Membership Criteria** tab looks as follows:

![NSGroup-Pods](images/nsxt/mt0/nsgroup-for-pods.png)

##### Create NSGroup for Cluster Nodes and Pods

1. In NSX Manager, navigate to **Inventory > Groups > Groups** and click **Add new group**.
1. Configure the new NSGroup as follows:
  1. In the **Name** field, enter the cluster UUID or cluster name and append `-nodes-pods` to the end of the name to distinguish it.
  The cluster name must be unique.
  1. In the **Membership Criteria** tab, add the following two criteria:
      1. For the first criterion, select **Logical Port**.
      1. For **Tag** > **Equals**, enter `tkgi-cluster-YOUR-CLUSTER-UUID`.
      1. For **Scope** > **Equals**, enter `ncp/cluster`.
      1. For the second criterion, select **Logical Switch**.
      1. For **Tag** > **Equals**, enter `tkgi-cluster-YOUR-CLUSTER-UUID`.
      1. For **Scope** > **Equals**, enter `pks/cluster`.
      ![NSGroup-Nodes-Pods](images/nsxt/mt0/nsgroup-nodes-pods.png)

After you configure the NSGroup for cluster nodes and pods, the **Membership Criteria** tab looks as follows:

![NSGroup-Nodes-Pods](images/nsxt/mt0/nsgroup-for-nodes-pods.png)

#### <a id="dfw-rules"></a>Step 4: Create DFW Rules

Select the DFW section you created above and configure the following three DFW rules:  

* [Deny Everything Else](#deny-else)  
* [Prevent Pod to Node Communication](#pods-rule)  
* [Allow Node to Node and Nodes to Pods Communications](#nodes-rule)  


##### <a id="deny-else"></a>DFW Rule 1: Deny Everything Else

This is a global deny rule. Configure the rule as follows:

1. Click **Add Rule**.
1. In the **Name** field, enter a name for your DFW rule.
1. For **Source**, select the `All-TKGI-Clusters` NSGroup.
1. For **Destination**, select the `All-TKGI-Clusters` NSGroup.
1. For **Service**, select **Any**.
1. For **Apply To**, select the `YOUR-CLUSTER-UUID-nodes-pods` NSGroup.
1. For **Action**, select **Drop**.

##### <a id="pods-rule"></a>DFW Rule 2: Prevent Pod to Node Communication

Configure this rule as follows:

1. Click **Add Rule**.
1. In the **Name** field, enter a name for your DFW rule.
1. For **Source**, select the `YOUR-CLUSTER-UUID-pods` NSGroup.
1. For **Destination**, select `YOUR-CLUSTER-UUID-nodes` NSGroup.
1. For **Service**, select **Any**.
1. For **Apply To**, select the `YOUR-CLUSTER-UUID-nodes-pods` NSGroup.
1. For **Action**, select **Drop**.

##### <a id="nodes-rule"></a>DFW Rule 3: Allow Node to Node and Nodes to Pods Communications

Configure this rule as follows:

1. Click **Add Rule**.
1. In the **Name** field, enter a name for your DFW rule.
1. For **Source**, select the `YOUR-CLUSTER-UUID-nodes-pods` NSGroup.
1. For **Destination**, select `YOUR-CLUSTER-UUID-nodes-pods` NSGroup.
1. For **Service**, select **Any**.
1. For **Apply To**, select `YOUR-CLUSTER-UUID-nodes-pods` NSGroup.
1. For **Action**, select **Allow**.

For example, see the three configured DFW rules below:

![DFW Rules](images/nsxt/mt0/dfw-rules.png)


<br>
## <a id="base-config-vrf"></a> Configure VRF Tier-0 Gateway-Based Tenant Isolation

To isolate a cluster and its workloads behind a VRF gateway:  

* [Review Your Network Configuration](#review-network-vrf)  
* [Create VRF Gateway Segments](#create-gateway-segments-vrf)  
* [Create VRF Gateways](#create-gateway-vrf)  
* [Create a Network Profile](#create-network-profile-vrf)  
* [Configure a Cluster with a VRF Gateway](#update-cluster-vrf)  
<% if vars.product_version == "COMMENTED"  %>
* [Configure VRF Gateway Security](#security-config-vrf)  
<% end %>


<br>
### <a id="review-network-vrf"></a>Step 1: Review Your Network Configuration

To review the network configuration of your three VLANs:  

1. To determine the VLAN IDs of your three VLANs, run either of the following for each VLAN:  

    * Method one:  
    
        ```
        sudo cat /proc/net/vlan/VLAN-NAME |grep VID
        ```
    
        Where VLAN-NAME is the name of a single VLAN.  
    
    * Method two:  
    
        ```
        ip -d link show dev VLAN-NAME |grep id
        ```
    
        Where VLAN-NAME is the name of a single VLAN.  
     
1. Confirm that a t0-shared gateway uses the VLAN IDs 
and that its segment matches the segments returned by the commands above.  


<br>
### <a id="create-gateway-segments-vrf"></a>Step 2: Create VRF Gateway Segments

You must create two VLAN-backed segments for your VRF gateways. 
For information on creating a VLAN-backed segment, 
see [Add a Segment](https://docs.vmware.com/en/VMware-NSX-T-Data-Center/3.2/administration/GUID-D018DB03-0C07-4980-887D-AF3B3E93EF63.html) 
in the VMware NSX-T Data Center documentation.  

To create two gateway segments:  

1. Create a VLAN-backed segment for one of your VRF gateway VLANs with the following configuration:  

    * Configure **Segment Name**. For example, `internet-vlan-vrf-0-seg`.  
    * Configure **Transport Zone**. For example, `internet-tz-vlan-0`.  
    * Configure **VLAN**. Specify one of the VLAN IDs determined above.  

1. Create a VLAN-backed segment for your remaining VRF gateway VLAN with the following configuration:  

    * Configure **Segment Name** with a new unique name. For example, `internet-vlan-vrf-1-seg`.  
    * Configure **Transport Zone** with the same zone used by the first segment.  
    * Configure **VLAN** with the VLAN ID for the second VLAN.  


<br>
### <a id="create-gateway-vrf"></a>Step 3: Create VRF Gateways

You must create two VRF gateways to isolate your tenants. 
For information on creating a VRF gateway, 
see [Add a VRF Gateway](https://docs.vmware.com/en/VMware-NSX-T-Data-Center/3.1/administration/GUID-B971A6D9-E4DB-4D98-A7FD-D3887BBCA0BC.html) 
in the VMware NSX-T Data Center documentation.  
    
1. Create a VRF gateway with the following configuration:  

    1. Provide a **Name** for the VRF gateway. For example, `t0-vrf-0`.  
    1. Connect the VRF gateway to your Tier-0 Gateway.  
    1. Save your configuration.
    1. Set the interface for the gateway. For example, `t0-vrf-0-uplink-0` and `t0-vrf-0-uplink-1`.  
    1. Assign a high availability VIP. For example, `192.168.116.2`.  
    
1. Create a second VRF gateway with the following configuration:  

    * Configure **Name** with a unique name. For example, `t0-vrf-1`.  
    * Configure **Interface** with unique settings. For example, `t0-vrf-1-uplink-0` and `t0-vrf-1-unlink-1`.  
    * Configure **HA VIP** with a unique IP Address. For example, `192.168.117.2`.  

1. To test your configuration, Ping each gateway uplink VIP.  
    
    For example:  
    
    <pre class="terminal">
    $ ping 192.168.116.2  
    PING 192.168.116.2 (192.168.116.2) 56(84) bytes of data.  
    64 bytes from 192.168.116.2: icmp_seq=1 ttl=64 time=0.478 ms  
    64 bytes from 192.168.116.2: icmp_seq=2 ttl=64 time=0.520 ms  
    ^C  
    --- 192.168.116.2 ping statistics ---  
    2 packets transmitted, 2 received, 0% packet loss, time 999ms  
    rtt min/avg/max/mdev = 0.478/0.499/0.520/0.021 ms  
      
    $ ping 192.168.117.2  
    PING 192.168.117.2 (192.168.117.2) 56(84) bytes of data.  
    64 bytes from 192.168.117.2: icmp_seq=1 ttl=64 time=0.531 ms  
    64 bytes from 192.168.117.2: icmp_seq=2 ttl=64 time=0.504 ms  
    ^C  
    --- 192.168.117.2 ping statistics ---  
    2 packets transmitted, 2 received, 0% packet loss, time 999ms  
    rtt min/avg/max/mdev = 0.504/0.517/0.531/0.026 ms	 
    </pre>

1. (Optional) To allow communication to an external data path, add a default router for each VRF gateway. 
For each router, add a default route and configure **Network** and **Next Hop**.  


<br>
### <a id="create-network-profile-vrf"></a>Step 4: Create a Network Profile

You must use a Network Profile to isolate a cluster behind a VRF gateway.  

To configure a Network Profile for connecting to a VRF gateway:

1. Create a network profile configuration JSON file that defines the gateway as the `"t0_router_id"` value:  

    ```
    {
        "name": "PROFILE-NAME",
        "description": "PROFILE-DESCRIP",
        "parameters": {

            "t0_router_id":"VRF-GATEWAY-NAME",
            "infrastructure_networks":[NETWORK-RANGES],
            "cni_configurations": {
                "type": "nsxt",
                "parameters": {

                    "extensions":{
                        "ncp":{
                            "nsx_v3":{
                            },
                            "coe":{
                            },
                            "ha":{
                            },
                            "k8s":{
                            }
                        },
                        "nsx-node-agent":{
                        }
                    }
                }
            }
        }
    }
    ```

    Where:  

    * `VRF-GATEWAY-NAME` is the name of the VRF gateway the cluster should use.  
    * `NETWORK-RANGES` is an array of IP ranges the cluster can access.  
    * `PROFILE-NAME` is the internal name for your network profile.  
    * `PROFILE-DESCRIP` is an internal description for your network profile.  

    For example:
    
    ```
    {
        "name": "np-1",
        "description": "",
        "parameters": {

            "t0_router_id":"vrf-103",
            "infrastructure_networks":["88.0.0.0/24","192.168.111.98","192.168.111.46"],
            "cni_configurations": {
                "type": "nsxt",
                "parameters": {

                    "extensions":{
                        "ncp":{
                            "nsx_v3":{
                            },
                            "coe":{
                            },
                            "ha":{
                            },
                            "k8s":{
                            }
                        },
                        "nsx-node-agent":{
                        }
                    }
                }
            }
        }
    }
    ```
    
For more information on creating a Network Profile, 
see [Creating and Managing Network Profiles](network-profiles-define.html).
    

<br>
### <a id="create-update-cluster-vrf"></a>Step 5: Configure a Cluster with a VRF Gateway

To configure a cluster to use a VRF gateway, assign the Network Profile to the cluster:  

* Create a new cluster using the VRF gateway Network Profile.  

    For more information on creating clusters using a Network Profile, 
    see [Create a Cluster with a Network Profile](network-profiles.html#create-new) 
    in _Using Network Profiles_.
    
* Update an existing cluster using the VRF gateway Network Profile.  

    For more information on updating existing clusters with a Network Profile, 
    see [Assign a Network Profile to an Existing Cluster](network-profiles.html##assign-profile) 
    in _Using Network Profiles_.

<% if vars.product_version == "COMMENTED"  %>
<br>
## <a id="security-config-vrf"></a>Configure VRF Gateway Security

In a VRF gateway-isolated environment, you can secure two types of traffic:

- Traffic between tenants. See [Secure VRF Gateway Inter-Tenant Communications](#secure-inter-tenant-vrf).  
- Traffic between clusters in the same tenant. See [Secure VRF Gateway Intra-Tenant Communications](#secure-intra-tenant-vrf).  


### <a id="secure-inter-tenant-vrf"></a>Secure VRF Gateway Inter-Tenant Communications



### <a id="secure-intra-tenant-vrf"></a>Secure VRF Gateway Intra-Tenant Communications
<% end %>