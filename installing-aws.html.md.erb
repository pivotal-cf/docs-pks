---
title: Installing Tanzu Kubernetes Grid Integrated Edition on AWS (Antrea and Flannel Networking)
owner: TKGI
iaas: AWS
---

This topic describes how to install and configure <%= vars.product_full %> on Amazon Web Services
(AWS).

##<a id='prerequisites'></a>Prerequisites

Before performing the procedures in this topic, you must have deployed and configured Ops Manager.
For more information, see [AWS Prerequisites and Resource Requirements](aws-requirements.html).

This topic assumes that you have prepared the AWS environment for this <%= vars.product_full %> deployment. 
For more information, see [Installing and Configuring Ops Manager on AWS](aws-om-install-config.html).

<%= partial 'prerequisites' %>

##<a id='install'></a> Step 1: Install <%= vars.product_short %>

<%= partial 'install' %>

##<a id='configure'></a> Step 2: Configure <%= vars.product_short %>

Click the orange **<%= vars.product_tile %>** tile to start the configuration process.

![<%= vars.k8s_runtime_abbr %> tile on the Ops Manager installation dashboard](images/tkgi-tile-orange.png)

  <p class="note warning"><strong>WARNING</strong>: When you configure the <%= vars.product_tile %> tile,
  do not use spaces in any field entries. This includes spaces between characters as well as
  leading and trailing spaces. If you use a space in any field entry, the deployment of <%= vars.product_short %> fails.</p>

###<a id='azs-networks'></a> Assign AZs and Networks

<%= partial 'azs-networks' %>

###<a id='tkgi-api'></a> <%= vars.control_plane %>

<%= partial 'api' %>

###<a id='plans'></a> Plans

<%= partial 'plans' %>

###<a id='cloud-provider'></a> Kubernetes Cloud Provider

To configure your Kubernetes cloud provider settings, follow the procedures below:

1. Click **Kubernetes Cloud Provider**.

1. Under **Choose your IaaS**, select **AWS**.

    <img src="images/cloud-aws.png" alt="AWS pane configuration" width="325">

1. Enter your **AWS Master Instance Profile IAM**. This is the instance profile name associated with the control plane node. 

1. Enter your **AWS Worker Instance Profile IAM**. This is the instance profile name associated with the worker node. 

1. Click **Save**.

###<a id='networking'></a> Networking

To configure networking, do the following:

1. Click **Networking**.
1. Under **Container Networking Interface**, select **Antrea**.
    <img src="images/networking-antrea.png" alt="Networking pane configuration" width="425">  

    Antrea is selected as the default Container Networking Interface (CNI). <%= vars.recommended_by %> recommends that you use Antrea
    as your CNI.  
    <p class="note">
    <strong>Note:</strong> Support for the Flannel Container Networking Interface (CNI) is deprecated.
    <%= vars.recommended_by %> recommends that you upgrade your Flannel CNI-configured clusters to the Antrea CNI.
    For more information about Flannel CNI deprecation, see 
    <a href="understanding-upgrades.html#cni">About Upgrading from the Flannel CNI to the Antrea CNI</a> 
    in <i>About <%= vars.product_short %> Upgrades</i>.
    </p>
1. (Optional) Enter values for **Kubernetes Pod Network CIDR Range** and **Kubernetes Service Network CIDR Range**.
    * Ensure that the CIDR ranges do not overlap and have sufficient space for your deployed services.
    * Ensure that the CIDR range for the **Kubernetes Pod Network CIDR Range** is large enough to accommodate the expected maximum number of pods.
<br>
1. (Optional) Configure a global proxy for all outgoing HTTP and HTTPS traffic from your Kubernetes clusters and 
the <%= vars.control_plane %> server. See [Using Proxies with <%= vars.product_short %> on AWS](proxies-aws.html) for instructions to enable a proxy.
1. (Optional) If you do not use a NAT instance, select **Allow outbound internet access from Kubernetes cluster vms (IaaS-dependent)**. Enabling this functionality assigns external IP addresses to VMs in clusters.  

1. Click **Save**.

###<a id='uaa'></a> UAA

<%= partial 'uaa' %>

###<a id='syslog'></a> (Optional) Host Monitoring

<%= partial 'host-monitoring' %>

###<a id='cluster-monitoring'></a> (Optional) In-Cluster Monitoring

<%= partial 'cluster-monitoring' %>

###<a id='tmc'></a> Tanzu Mission Control

<%= partial 'tmc' %>

###<a id='telemetry'></a> CEIP and Telemetry
<%= partial 'usage-data' %>

###<a id='errands'></a> Errands
<%= partial 'errands' %>

###<a id='resource-config'></a> Resource Config

To modify the resource configuration of <%= vars.product_short %> and specify your <%= vars.control_plane %> load balancer, follow the steps below:

1. Select **Resource Config**.

1. <%= partial 'resource-config' %>

1. For the **<%= vars.control_plane_db %>** job:
    * Leave the **LOAD BALANCERS** field blank.
    * (Optional) If you do not use a NAT instance, select **INTERNET CONNECTED**. This allows component instances direct access to the internet.

1. For the **<%= vars.control_plane %>** job:
    * In the `LOAD BALANCERS` field, enter all values of `pks_api_target_groups` from the Terraform output, prefixed with `alb:`.
    For example, `alb:ENV-pks-tg-9021,alb:ENV-pks-tg-8443`. Replace `ENV` with the `env_name` that you defined when you set up Terraform.
    For example, `alb:pcf-pks-tg-9021,alb:pcf-pks-tg-8443`.
    <%= partial 'lb-resource-config' %> * (Optional) If you do not use a NAT instance, select **INTERNET CONNECTED**. This allows component instances direct access to the internet.

  <p class="note warning"><strong>Warning:</strong> To avoid workload downtime, use the resource configuration recommended in 
  <a href="understanding-upgrades.html">About <%= vars.product_short %> Upgrades</a> 
  and <a href="maintain-uptime.html">Maintaining Workload Uptime</a>.
  </p>

##<a id='apply-changes'></a> Step 3: Apply Changes

<%= partial 'apply-changes' %>

##<a id='retrieve-tkgi-api'></a> Step 4: Retrieve the <%= vars.control_plane %> Endpoint

<%= partial 'share-endpoint' %>

## <a id='clis'></a> Step 5: Install the <%= vars.k8s_runtime_abbr %> and Kubernetes CLIs

<%= partial 'install-cli' %>

## <a id='auth'></a> Step 6: Configure Authentication for <%= vars.product_short %>
 
Follow the procedures in [Setting Up <%= vars.product_short %> Admin Users on AWS](aws-configure-users.html).

##<a id='next-steps'></a> Next Steps

After installing <%= vars.product_short %> on AWS, you might want to do one or more of the following:

* Create a load balancer for your <%= vars.product_short %> clusters. For more information, see [Creating and Configuring an AWS Load Balancer for <%= vars.product_short %> Clusters](aws-cluster-load-balancer.html).
* Create your first <%= vars.product_short %> cluster. For more information, see [Creating Clusters](create-cluster.html).
