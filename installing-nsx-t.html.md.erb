---
title: Installing Tanzu Kubernetes Grid Integrated Edition on vSphere with NSX-T
owner: TKGI
iaas: vSphere-NSX-T
---

This topic describes how to install and configure <%= vars.product_full %> (<%= vars.k8s_runtime_abbr %>) on vSphere with NSX-T
integration.

##<a id="prerequisites"></a>Prerequisites

Before you begin this procedure, ensure that you have successfully completed all preceding steps for installing <%= vars.product_short %> on vSphere with NSX-T, including:

<ul>
  <li>
    <a href="./vsphere-nsxt-index-prepare.html">Preparing to Install <%= vars.product_short %> on vSphere with NSX-T Data Center</a>
  </li>
  <li><a href="./nsxt-3-0-install.html">Installing and Configuring NSX-T Data Center v3.0 for <%= vars.product_short %></a></li>
  <li><a href="./nsxt-3-1-install-delta.html">Configuring NSX-T Data Center v3.1 Transport Zones and Edge Node Switches for <%= vars.product_short %></a></li>
  <li>
    <a href="./vsphere-nsxt-om-deploy.html">Deploying Ops Manager with NSX-T for <%= vars.product_short %></a>
  </li>
  <li>
    <a href="./nsxt-3-0-install.html#nsxt30-mgmt-ssl">Generate and Register the NSX-T Management SSL Certificate and Private Key</a> in <em>Installing and Configuring NSX-T Data Center v3.0 for <%= vars.k8s_runtime_abbr %></em>
  </li>
  <li>
    <a href="./vsphere-nsxt-om-config.html">Configuring BOSH Director with NSX-T for <%= vars.product_short %></a>
  </li>
  <li>
    <a href="./nsxt-generate-pi-cert.html">Generating and Registering the NSX Manager Superuser Principal Identity Certificate and Key for <%= vars.product_short %></a>
  </li>
</ul>

##<a id="install"></a> Step 1: Install <%= vars.product_short %>

<%= partial 'install' %>

##<a id="configure"></a> Step 2: Configure <%= vars.product_short %>

Click the orange **<%= vars.product_tile %>** tile to start the configuration process.

<p class="note"><strong>Note</strong>: Configuration of NSX-T or Flannel <strong>cannot</strong> be changed after initial installation and configuration of <%= vars.product_short %>.</p>

![<%= vars.k8s_runtime_abbr %> tile on the Ops Manager installation dashboard](images/tkgi-tile-orange.png)

  <p class="note warning"><strong>WARNING</strong>: When you configure the <%= vars.product_tile %> tile,
  do not use spaces in any field entries. This includes spaces between characters as well as
  leading and trailing spaces. If you use a space in any field entry, the deployment of <%= vars.product_short %> fails.</p>

###<a id="azs-networks"></a> Assign AZs and Networks

To configure the availability zones (AZs) and networks 
used by the <%= vars.product_short %> control plane:

1. Click **Assign AZs and Networks**.

1. Under **Place singleton jobs in**, select the availability zone (AZ) where you want to deploy 
the <%= vars.control_plane %> and <%= vars.control_plane_db %> VMs.

    ![Assign AZs and Networks pane in Ops Manager](images/azs-networks.png)

1. Under **Balance other jobs in**, select the AZ for balancing other <%= vars.product_short %> control plane jobs.  
    <p class="note"><strong>Note</strong>: You must specify the <strong>Balance other jobs in</strong> AZ, but the selection has no effect in the current version of <%= vars.product_short %>.
    </p>
1. Under **Network**, select the <%= vars.k8s_runtime_abbr %> Management Network linked to the `ls-tkgi-mgmt` NSX-T logical switch you created in the [Create Networks Page](vsphere-nsxt-om-config.html#create-networks) step of _Configuring BOSH Director with NSX-T for <%= vars.product_short %>_. This provides network placement for <%= vars.product_short %> component VMs, such as the <%= vars.control_plane %> and <%= vars.control_plane_db %> VMs.
1. Under **Service Network**, your selection depends on whether you are installing a new <%= vars.product_short %> deployment or upgrading from a previous version of <%= vars.product_short %>.
  * If you are deploying <%= vars.product_short %> with NSX-T for the first time, select the <%= vars.k8s_runtime_abbr %> Management Network that you specified in the **Network** field. 
  You do not need to create or define a service network because <%= vars.product_short %> creates the service network for you during the installation process. 
  * If you are upgrading from a previous version of <%= vars.product_short %>, then select the **Service Network** linked to the `ls-tkgi-service` NSX-T logical switch that <%= vars.product_short %> created for you during installation. The service network provides network placement for existing on-demand Kubernetes cluster service instances that were created by the <%= vars.product_short %> broker.
1. Click **Save**.

###<a id="tkgi-api"></a> <%= vars.control_plane %>

<%= partial 'api' %>

###<a id="plans"></a> Plans

<%= partial 'plans' %>

###<a id="cloud-provider"></a> Kubernetes Cloud Provider

<%= partial 'cloud-provider' %>

###<a id="networking"></a> Networking

To configure networking, do the following:

1. Click **Networking**.  
1. Under **Container Networking Interface**, select **NSX-T**.  
  ![NSX-T Networking configuration pane in <%= vars.k8s_runtime_abbr %> tile](images/networking-nsx-t.png)
  1. For **NSX Manager hostname**, enter the hostname or IP address of your NSX-T Manager.  
  1. For **NSX Manager Super User Principal Identify Certificate**, copy and paste the contents and private key of the Principal Identity certificate you created in [Generating and Registering the NSX Manager Superuser Principal Identity Certificate and Key](nsxt-generate-pi-cert.html).  
  1. For **NSX Manager CA Cert**, copy and paste the contents of the NSX-T Manager CA certificate you created in [Generate and Register the NSX-T Management SSL Certificate and Private Key](./nsxt-3-0-install.html#nsxt30-mgmt-ssl). Use this certificate and key to connect to the NSX-T Manager.
  1. The **Disable SSL certificate verification** checkbox is **not** selected by default. In order to deactivate TLS verification, select the checkbox. You may want to deactivate TLS verification if you did not enter a CA certificate, or if your CA certificate is self-signed.  
  <p class="note">
    <strong>Note</strong>: The <strong>NSX Manager CA Cert</strong> field and the <strong>Disable SSL certificate verification</strong> option are intended to be mutually exclusive. If you deactivate SSL certificate verification, leave the CA certificate field blank. If you enter a certificate in the <strong>NSX Manager CA Cert</strong> field, do not deactivate SSL certificate verification. If you populate the certificate field and deactivate certificate validation, insecure mode takes precedence.
  </p>
  1. If you are using a NAT deployment topology, leave the **NAT mode** checkbox selected. If you are using a No-NAT topology, clear this checkbox. For more information, see [NSX-T Deployment Topologies for Tanzu Kubernetes Grid Integrated Edition](nsxt-topologies.html).
  1. If you are using the NSX-T Policy API (Beta), select **Policy API mode**.  
    <p class="note warning"><strong>Warning:</strong>
      Support for the NSX-T Policy API is currently in beta and is intended for evaluation and test purposes only. 
      Do not use this feature in a production environment.
    </p>
  1. Configure the NSX-T networking objects, including the **Pods IP Block ID**, **Nodes IP Bock ID**, **T0 Router ID**, **Floating IP Pool ID**, **Nodes DNS**, **vSphere Cluster Names**, and **Kubernetes Service Network CIDR Range**. Each of the these fields are described in more detail beneath the example screenshots. If you are using the NSX-T Policy API, you must have created the **Pods IP Block ID**, **Nodes IP Bock ID**, **T0 Router ID**, and **Floating IP Pool ID** objects using the NSX-T Policy API. See [Create NSX-T Objects for Kubernetes Clusters Using the Policy Interface](./nsxt-install-objects-k8s.html#nsxt3-k8s-objects-policy).
    ![NSX-T Networking configuration pane in Ops Manager](images/networking-nsx-t-3.png)
    [View a larger version of this image.](images/networking-nsx-t-3.png)
    <br>
    ![NSX-T Networking configuration pane in Ops Manager for Policy API Mode](images/networking-nsx-t-5.png)
    [View a larger version of this image.](images/networking-nsx-t-5.png)
    <br>
      * **Pods IP Block ID**: Enter the UUID of the IP block to be used for Kubernetes pods. <%= vars.product_short %> allocates IP addresses for the pods when they are created in Kubernetes. Each time a namespace is created in Kubernetes, a subnet from this IP block is allocated. The current subnet size that is created is /24, which means a maximum of 256 pods can be created per namespace.  
      * **Nodes IP Block ID**: Enter the UUID of the IP block to be used for Kubernetes nodes. <%= vars.product_short %> allocates IP addresses for the nodes when they are created in Kubernetes. The node networks are created on a separate IP address space from the pod networks. The current subnet size that is created is /24, which means a maximum of 256 nodes can be created per cluster.  
      For more information, including sizes and the IP blocks to avoid using, see [Plan IP Blocks](nsxt-prepare-env.html#plan-ip-blocks) in _Preparing NSX-T Before Deploying <%= vars.product_short %>_.  
      * **T0 Router ID**: Enter the `t0-tkgi` T0 router UUID. Locate this value in the NSX-T UI router overview.  
      * **Floating IP Pool ID**: Enter the `ip-pool-vips` ID that you created for load balancer VIPs. For more information, see [Plan Network CIDRs](nsxt-prepare-env.html#plan-cidrs) in _Network Planning for Installing Tanzu Kubernetes Grid Integrated Edition with NSX-T_. <%= vars.product_short %> uses the floating IP pool to allocate IP addresses to the load balancers created for each of the clusters. The load balancer routes the API requests to the control plane nodes and the data plane.  
      * **Nodes DNS**: Enter one or more Domain Name Servers used by the Kubernetes nodes.  
      * **vSphere Cluster Names**: Enter a comma-separated list of the vSphere clusters where you will deploy Kubernetes clusters. The NSX-T pre-check errand uses this field to verify that the hosts from the specified clusters are available in NSX-T. You can specify clusters in this format: `cluster1,cluster2,cluster3`.  
      * **Kubernetes Service Network CIDR Range**: Specify an IP address and subnet size depending on the number of Kubernetes services that you plan to deploy within a single Kubernetes cluster, for example: `10.100.200.0/24`. The IP address used here is internal to the cluster and can be anything, such as `10.100.200.0`. A `/24` subnet provides 256 IPs. If you have a cluster that requires more than 256 IPs, define a larger subnet, such as `/20`.  
      * Under **<%= vars.k8s_runtime_abbr %> Operation Timeout**, enter the timeout in milliseconds for <%= vars.k8s_runtime_abbr %>-API operation. 
      When needed, increase the **<%= vars.k8s_runtime_abbr %> Operation Timeout** setting to avoid timeouts during cluster deletion in large-scale NSX-T environments. 
      To determine the optimal Operation Timeout setting, see [Cluster Deletion Fails](troubleshoot-issues.html#cluster-delete-fail) in _General Troubleshooting_.  
1. (Optional) Configure a global proxy for all outgoing HTTP and HTTPS traffic from your Kubernetes clusters and the <%= vars.control_plane %> server. See [Using Proxies with <%= vars.product_short %> on NSX-T](proxies.html) for instructions on how to enable a proxy.  
1. Under **Allow outbound internet access from Kubernetes cluster vms (IaaS-dependent)**, ignore the **Enable outbound internet access** checkbox.  
1. Click **Save**.

###<a id="uaa"></a> UAA

<%= partial 'uaa' %>

###<a id="syslog"></a> (Optional) Host Monitoring

<%= partial 'host-monitoring' %>

###<a id="cluster-monitoring"></a> (Optional) In-Cluster Monitoring

<%= partial 'cluster-monitoring' %>

###<a id="tmc"></a> Tanzu Mission Control

<%= partial 'tmc' %>

###<a id="telemetry"></a> CEIP and Telemetry

<%= partial 'usage-data' %>

###<a id='storage-config'></a> Storage

<%= partial 'storage-config' %>

###<a id="errands"></a> Errands
<%= partial 'errands' %>

###<a id="resource-config"></a> Resource Config

To modify the resource configuration of <%= vars.product_short %>, follow the steps below:

1. Select **Resource Config**.

1. <%= partial 'resource-config' %>

1. Under each job, leave **NSX-T CONFIGURATION** and **NSX-V CONFIGURATION** blank.

  <p class="note warning"><strong>Warning:</strong> To avoid workload downtime, use the resource configuration recommended in 
  <a href="understanding-upgrades.html">About <%= vars.product_short %> Upgrades</a> 
  and <a href="maintain-uptime.html">Maintaining Workload Uptime</a>.
  </p>

## <a id="apply-changes"></a> Step 3: Apply Changes

After configuring the <%= vars.product_tile %> tile, follow the steps below to deploy the tile:

<%= partial 'apply-changes' %>

## <a id="clis"></a> Step 4: Install the <%= vars.k8s_runtime_abbr %> and Kubernetes CLIs

<%= partial 'install-cli' %>

## <a id="retrieve-endpoint"></a>Step 5: Verify NAT Rules

If you are using NAT mode, verify that you have created the required NAT rules for the <%= vars.product_short %> Management Plane. See [Create Management Plane](./nsxt-3-0-install.html#nsxt30-mgmt-plane) in _Installing and Configuring NSX-T Data Center v3.0 for <%= vars.k8s_runtime_abbr %>_ for details.

In addition, for NAT and No-NAT modes, verify that you created the required NAT rule for Kubernetes control plane nodes to access NSX-T Manager. For details, see [Create IP Blocks and Pool for Compute Plane](./nsxt-3-0-install.html#nsxt30-ip-blocks-pool) in _Installing and Configuring NSX-T Data Center v3.0 for <%= vars.k8s_runtime_abbr %>_.

If you want your developers to be able to access the <%= vars.k8s_runtime_abbr %> CLI from their external workstations, create a DNAT rule that maps a routable IP address to the <%= vars.control_plane %> VM. This must be done after <%= vars.product_short %> is successfully deployed and it has an IP address. See [Create Management Plane](./nsxt-3-0-install.html#nsxt30-mgmt-plane) in _Installing and Configuring NSX-T Data Center v3.0 for <%= vars.k8s_runtime_abbr %>_ for details.

## <a id="auth"></a> Step 6: Configure Authentication for <%= vars.product_short %>

Follow the procedures in [Setting Up <%= vars.product_short %> Admin Users on vSphere](vsphere-configure-users.html) in *Installing <%= vars.product_short %> > vSphere*.

##<a id="next-steps"></a> Next Steps

After installing <%= vars.product_short %> on vSphere with NSX-T integration, complete the following tasks:

* <%= partial 'harbor' %>
* [Setting Up <%= vars.product_short %> Admin Users on vSphere](./vsphere-configure-users.html).
* [Creating an <%= vars.product_short %> Cluster](create-cluster.html).
