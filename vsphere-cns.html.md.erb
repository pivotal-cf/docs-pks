---
title: Deploying and Managing Cloud Native Storage (CNS) on vSphere
owner: TKGI
---

This topic describes how to use and configure the vSphere Container Storage Interface (CSI) Driver 
to enable VMware Tanzu Kubernetes Grid Integrated Edition (TKGI) provisioned clusters on vSphere to use external container storage.
On vSphere, TKGI automatically installs the vSphere CSI Driver to all TKGI-provisioned clusters.  


## <a id='overview'></a>Overview

vSphere Cloud Native Storage (CNS) provides comprehensive data management for stateful, containerized apps,
enabling apps to survive restarts and outages.
Stateful containers can use vSphere storage primitives such as standard volume, persistent volume, and dynamic provisioning, 
independent of VM and container lifecycle.  

You can install vSphere CNS on TKGI-provisioned clusters by configuring TKGI 
to automatically install a vSphere CSI Driver. 
To enable automatic CSI driver installation on your clusters, 
see [Storage](installing-vsphere.html#storage-config) in _Installing TKGI on vSphere_.  

When automatic vSphere CSI Driver installation is enabled, your clusters 
use your tile **Kubernetes Cloud Provider** storage settings as the default vSphere CNS configuration.  

The automatically deployed vSphere CSI Driver supports high availability (HA) configurations. HA support 
is automatically enabled on clusters with multiple control plane nodes and uses only one active CSI Controller.  

Use the vSphere client to review your cluster storage volumes and their backing virtual disks, and to 
set a storage policy on your storage volumes or monitor policy compliance. 
vSphere storage backs up your cluster volumes.  

For more information about VMware CNS, see [Getting Started with VMware Cloud Native Storage](https://docs.vmware.com/en/VMware-vSphere/7.0/com.vmware.vsphere.storage.doc/GUID-CF1D7196-E49C-4430-8C50-F8E35CAAE060.html).  

For more information about using the Kubernetes CSI Driver, see
[Persistent Volumes](https://kubernetes.io/docs/concepts/storage/persistent-volumes/) in the Kubernetes documentation.  


In TKGI, you can configure the vSphere CSI Driver to:  
    
* Customize, deploy and manage vSphere CNS volumes:  
    * To customize file volumes, see [Customize vSphere File Volumes](#create-file-volumes) below.  
    * To use and customize CNS Volumes, see [Create or Use CNS Block Volumes](#use-cns) below.  
    * To migrate in-tree storage volumes to the vSphere CSI Driver, 
    see [Migrate an In-Tree vSphere Storage Volume to the vSphere CSI Driver](#migrate-to-csi) below.  

* Customize clusters to support vSphere Topology-Aware Volume Provisioning:     
    * To apply vSphere Topology-Aware Volume Provisioning to your cluster, see 
    [Customize a Cluster with vSphere Topology-Aware Volume Provisioning](#vsphere-topology-provisioning) below.  
    
* Customize and manage vSphere CNS:  
    * To use CNS in a multi-data center environment, see [Configure CNS Data Centers](#cns-datacenters) below.  
* Customize the maximum number of snapshots for a volume
  * To customize the maximum number of snapshots for each persistent volume, 
  see [Customize the Maximum Number of Volume Snapshots](#customize-max-snapshot) below.  

<br>
<br>
## <a id='concerns'></a> Requirements and Limitations of the vSphere CSI Driver

For information about the supported features and the known limitations of the vSphere CSI Driver, see:  

* [vSphere CSI Driver Supported Features and Requirements](#supported-feature)  
* [Unsupported Features and Limitations](#unsupported-features)  


<br>
### <a id='supported-feature'></a> vSphere CSI Driver Supported Features and Requirements

The vSphere CSI Driver supports different features depending on driver version, environment and storage type.  

TKGI supports only the following vSphere CSI Driver features:  

* Dynamic Block PV support
* Dynamic File PV support
* Dynamic Virtual Volume (vVols) PV support  
* Encryption support via VMcrypt
* Enhanced Object Health in UI for vSAN Datastores  
* Kubernetes Multi-node Control Plane support  
* Offline volume expansion (Block volumes only)
* Online volume expansion (Block volumes only)
* Raw block volume support
* Static PV Provisioning  
* Topology-aware volume provisioning  
* Volume snapshot and restore
* XFS file system

<br>
For more information about these supported vSphere CSI Driver features, see the relevant sections in the 
VMware vSphere Container Storage Plug-in documentation:

 * **Usage Limitations, environment, and version requirements**: [Supported Kubernetes Functionality](https://docs.vmware.com/en/VMware-vSphere-Container-Storage-Plug-in/3.0/vmware-vsphere-csp-getting-started/GUID-D4AAD99E-9128-40CE-B89C-AD451DA8379D.html#GUID-E59B13F5-6F49-4619-9877-DF710C365A1E__GUID-ADF6F066-9A96-4E54-8154-66519AA65B39) 
in _Compatibility Matrices for vSphere Container Storage Plug-in_.

 * **Scaling Limitations**: [Configuration Maximums for vSphere Container Storage Plug-in](https://docs.vmware.com/en/VMware-vSphere-Container-Storage-Plug-in/3.0/vmware-vsphere-csp-getting-started/GUID-C0350554-F5DB-4C27-9786-720AC0F0B28A.html).  

 * **vCenter, Datastore, and cluster types**: [vSphere Functionality Supported by vSphere Container Storage Plug-in](https://docs.vmware.com/en/VMware-vSphere-Container-Storage-Plug-in/3.0/vmware-vsphere-csp-getting-started/GUID-5B7955AA-E720-4C73-8330-1C34A0B054B3.html).  

<br>
### <a id='unsupported-features'></a> Unsupported Features and Limitations

vSphere Storage DRS, Manual Storage vMotion, and other VMware vSphere features are not supported by the vSphere Container Storage Plug-in 
and cannot be used by the TKGI clusters that use or migrate to the vSphere CSI Driver.  

For more information on the limitations of the VMware vSphere Container Storage Plug-in, see 
[vSphere Functionality Supported by vSphere Container Storage Plug-in](https://docs.vmware.com/en/VMware-vSphere-Container-Storage-Plug-in/3.0/vmware-vsphere-csp-getting-started/GUID-5B7955AA-E720-4C73-8330-1C34A0B054B3.html) 
in the VMware vSphere Container Storage Plug-in documentation.  


<br>
## <a id='create-file-volumes'></a>Customize vSphere File Volumes

To create, modify or remove a customized vSphere file volume:  

* [Create a Cluster with Customized File Volume Parameters](#file-volumes-create)  
* [Modify a Cluster with Customized File Volume Parameters](#file-volumes-modify)  
* [Remove File Volume Parameters from a Cluster](#file-volumes-remove)  

### <a id='file-volumes-prereqs'></a>Prerequisites 

To use file volumes, you must enable vSAN File Services in the vSphere vCenter. 
For information about enabling vSAN File Services, see 
[Configure File Services](https://docs.vmware.com/en/VMware-vSphere/7.0/com.vmware.vsphere.vsan.doc/GUID-CA9CF043-9434-454E-86E7-DCA9AD9B0C09.html) 
in the VMware vSphere documentation.  


<br>
### <a id='file-volumes-create'></a>Create a Cluster with Customized File Volume Parameters

To create a new cluster with a vSphere file volume:  

1. Create a JSON or YAML formatted volume configuration file containing the following:  

    ```
    {
      "target_vsan_fileshare_datastore_urls": "DS-URLS",
      "net_permissions": [
        {
          "name": "PERMISSION-NAME",
          "ips": "IP-ADDRESS",
          "permissions": "PERMISSION",
          "rootsquash": "ACCESS-LEVEL"
        },
        {
          "name": "PERMISSION-NAME",
          "ips": "IP-ADDRESS",
          "permissions": "PERMISSION",
          "rootsquash": "ACCESS-LEVEL"
        }
      ]
    }
    ```
    
    Where:  
    
    * `DS-URLS` is a comma-separated list of datastores for deploying file share volumes. 
    For example: `"ds:///vmfs/volumes/vsan:52635b9067079319-95a7473222c4c9cd/"`.  
    * `PERMISSION-NAME` is your name for a NetPermission.  
    * `IP-ADDRESS` is the IP range or IP subnet affected by a NetPermission restriction.  
    * `PERMISSION` is the access permission to the file share volume for a NetPermission restriction.  
    * `ACCESS-LEVEL` is the security access level for the file share volume for a NetPermission restriction.  

    For information, see [File Volume Configuration](#file-volumes-params) below.  
    
1. To create a cluster with attached file volumes:

    ```
    tkgi create-cluster CLUSTER-NAME --config-file CONFIG-FILE 
    ```
    
    Where:  

    * `CLUSTER-NAME` is the name of your cluster.  
    * `CONFIG-FILE` is the name of your configuration file.  

    For example:  
    
    ```console
    tkgi create-cluster demo -e demo.cluster --plan Small --config-file ./conf1.json
    ```


<br>
### <a id='file-volumes-modify'></a>Modify a Cluster with Customized File Volume Parameters

To modify an existing cluster with a vSphere file volume:  

1. Create a file volume configuration file. For information, see [File Volume Configuration](#file-volumes-params) below.   
1. To update your cluster with file volumes:

    ```
    tkgi update-cluster CLUSTER-NAME --config-file CONFIG-FILE 
    ```

    Where:  

    * `CLUSTER-NAME` is the name of your cluster.  
    * `CONFIG-FILE` is the name of your configuration file.  

<p class="note warning"><strong>WARNING</strong>: Update the configuration file only on a TKGI cluster that has been upgraded to the current TKGI version. For more information, see <a href="understanding-upgrades.html#control-plane-upgrades-supported-tasks">Tasks Supported Following a TKGI Control Plane Upgrade</a> in <em>About Tanzu Kubernetes Grid Integrated Edition Upgrades</em>.
</p>

<br>
### <a id='file-volumes-remove'></a>Remove File Volume Parameters from a Cluster

To remove a vSphere file volume configuration from a cluster:  

1. Create a file volume configuration file containing either the `disable_target_vsan_fileshare_datastore_urls` or `disable_net_permissions`
parameters set to `true` to deactivate an existing file volume parameter.  
<br>
    For more information, see [File Volume Configuration](#file-volumes-params) below.  
    
1. To remove the configured file volume parameter from your cluster:

    ```
    tkgi update-cluster CLUSTER-NAME --config-file CONFIG-FILE 
    ```

    Where:  

    * `CLUSTER-NAME` is the name of your cluster.  
    * `CONFIG-FILE` is the name of your configuration file.  

<%# Examples: https://confluence.eng.vmware.com/pages/viewpage.action?spaceKey=PKS&title=Detailed+design+document+for+supporting+file+volume+specific+configurations #%>

<p class="note warning"><strong>WARNING</strong>: Update the configuration file only on a TKGI cluster that has been upgraded to the current TKGI version. For more information, see <a href="understanding-upgrades.html#control-plane-upgrades-supported-tasks">Tasks Supported Following a TKGI Control Plane Upgrade</a> in <em>About Tanzu Kubernetes Grid Integrated Edition Upgrades</em>.
</p>


<br>
### <a id='file-volumes-params'></a>File Volume Configuration

Create a JSON or YAML formatted File Volume configuration file to enable or deactivate vSphere file volume support.  

For example:  

* The following configuration enables all File Volume features:  

    ```
    {
      "target_vsan_fileshare_datastore_urls": "ds:///vmfs/volumes/vsan:52635b9067079319-95a7473222c4c9cd/",
      "net_permissions": [
        {
          "name": "demo1",
          "ips": "192.168.0.0/16",
          "permissions": "READ_WRITE",
          "rootsquash": false
        },
        {
          "name": "demo2",
          "ips": "10.0.0.0/8",
          "permissions": "READ_ONLY",
          "rootsquash": false
        }
      ]
    }
    ```
    
* The following configuration deactivates File Volume features:  

    ```
    {
      "disable_target_vsan_fileshare_datastore_urls": true,
      "disable_net_permissions": true
    }
    ```

<br>
#### <a id='file-volumes-params-datastores'></a>File Volume DataStores Configuration

The following are accepted Datastore URLs parameters:

<table>
    <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
    </tr>
    <tr>
        <td>target_vsan_fileshare_datastore_urls</td>
        <td>string</td>
        <td>A comma separated list of datastores for deploying file share volumes.</td>
    </tr>
    <tr>
        <td>disable_target_vsan_fileshare_datastore_urls</td>
        <td>Boolean</td>
        <td>Deactivate the target_vsan_fileshare_datastore_urls.<br>Values: <code>true</code>, <code>false</code>.<br>Default Value: <code>false</code>.</td>
    </tr>
</table>


#### <a id='file-volumes-params-netperm'></a>File Volume NetPermissions Object Configuration

The following are accepted NetPermissions objects:
<table>
    <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
    </tr>
    <tr>
        <td>net_permissions</td>
        <td>Array</td>
        <td>Properties defining a NetPermissions object.</td>
    </tr>
    <tr>
        <td>disable_net_permissions</td>
        <td>Boolean</td>
        <td>Deactivate the net_permissions.<br>Values: <code>true</code>, <code>false</code>.<br>Default Value: <code>false</code>.</td>
    </tr>
</table>  

The following are supported NetPermissions object parameters:
<table>
    <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
    </tr>
    <tr>
        <td>name</td>
        <td>string</td>
        <td>Name of the NetPermission object.</td>
    </tr>
    <tr>
        <td>ips</td>
        <td>string</td>
        <td>IP range or IP subnet affected by the NetPermission restrictions.<br>Default Value: <code>"*"</code>.</td>
    </tr>
    <tr>
        <td>permissions</td>
        <td>string</td>
        <td>Access permission to the file share volume.<br>Values: <code>"READ_WRITE"</code>, <code>"READ_ONLY"</code>, <code>"NO_ACCESS"</code>.<br>Default Value: <code>"READ_WRITE"</code>.</td>
    </tr>
    <tr>
        <td>rootsquash</td>
        <td>Boolean</td>
        <td>Security access level for the file share volume.<br>Values: <code>true</code>, <code>false</code>.<br>Default Value: <code>false</code>.</td>
    </tr>
</table>

For more information on NetPermissions object parameters, 
see [Procedure](https://docs.vmware.com/en/VMware-vSphere-Container-Storage-Plug-in/3.0/vmware-vsphere-csp-getting-started/GUID-BFF39F1D-F70A-4360-ABC9-85BDAFBE8864.html#procedure-1) 
in _Create a Kubernetes Secret for vSphere Container Storage Plug-in_.  

<%#FROM Confluence:  PKS Core pks-api/spec.yaml: https://confluence.eng.vmware.com/pages/viewpage.action?spaceKey=PKS&title=Detailed+design+document+for+supporting+file+volume+specific+configurations #%>
<br>
## <a id='use-cns'></a> Create or Use CNS Block Volumes

To dynamically provision a block volume using the vSphere CSI Driver:  

1. [Create a vSphere Storage Class](#create-storage)
1. [Create a PersistentVolumeClaim](#persistent-volumes-create)
1. [Create Workloads Using Persistent Volumes](#persistent-volumes-workloads)

For more information on vSphere CSI Driver configuration, see the `example/vanilla-k8s-block-driver` configuration 
for the CSI driver version you are using 
in [vsphere-csi-driver](https://github.com/kubernetes-sigs/vsphere-csi-driver/) in the VMware kubernetes-sigs GitHub repo.  


<br>
### <a id='create-storage'></a>Create a vSphere Storage Class

To create a vSphere Storage Class:

1. Open vCenter.
1. Open the vSAN Datastore Summary pane.

    ![vSAN Datastore Summary pane in vCenter](images/vsphere/datastore.png)

1. Determine the `datastoreurl` value for your Datastore.  
1. Create the following YAML:

    ```
    apiVersion: storage.k8s.io/v1
    kind: StorageClass
    metadata:
      name: demo-sts-storageclass
      annotations:
          storageclass.kubernetes.io/is-default-class: "true"
    provisioner: csi.vsphere.vmware.com
    allowVolumeExpansion: ALLOW-EXPANSION
    parameters:
      datastoreurl: "DATASTORE-URL"
    ```

    Where:  

    * `ALLOW-EXPANSION` defines whether the cluster's persistent volume size is either resizable or static. 
    Set to `true` for resizable and `false` for static size.  
    * `DATASTORE-URL` is the URL to your Datastore. 
    For a non-vSAN datastore, the `datastoreurl` value looks like 
    `ds:///vmfs/volumes/5e66e525-8e46bd39-c184-005056ae28de/`.  
<br>
    For example:

    ```
    apiVersion: storage.k8s.io/v1
    kind: StorageClass
    metadata:
      name: demo-sts-storageclass
      annotations:
          storageclass.kubernetes.io/is-default-class: "true"
    provisioner: csi.vsphere.vmware.com
    allowVolumeExpansion: true
    parameters:
      datastoreurl: "ds:///vmfs/volumes/vsan:52d8eb4842dbf493-41523be9cd4ff7b7/"
    ```
For more information about StorageClass, see [Storage Classes](https://kubernetes.io/docs/concepts/storage/storage-classes/) in the Kubernetes documentation.



<br>
### <a id='persistent-volumes-create'></a>Create a PersistentVolumeClaim

To create a Persistent Volume using the vSphere CSI Driver:

1. Create a Storage Class. For more information, see [Create a vSphere Storage Class](#create-storage) below.  
1. To apply the StorageClass configuration:
    ```
    kubectl apply -f CONFIG-FILE
    ```
    Where `CONFIG-FILE` is the name of your StorageClass configuration file. 
1. Create the PersistentVolumeClaim configuration for the file volume.
For information about configuring a PVC, see [Persistent Volumes](https://kubernetes.io/docs/concepts/storage/persistent-volumes/) in the Kubernetes documentation.      
<br>
    For example:  

    ```
    apiVersion: v1
    kind: PersistentVolumeClaim
    metadata:
      name: example-vanilla-block-pvc
    spec:
      accessModes:
        - ReadWriteOnce
      resources:
        requests:
          storage: 5Gi
      storageClassName: example-vanilla-block-sc
    ```
1. To apply the PVC configuration:  

    ```
    kubectl apply -f CONFIG-FILE 
    ```

    Where `CONFIG-FILE` is the name of your PVC configuration file.  


<br>
### <a id='persistent-volumes-workloads'></a>Create Workloads Using Persistent Volumes

1. Create a Pod configuration file containing `volumeMounts` and `volumes` parameters.  
<br>
    For example:  

    ```
    apiVersion: v1
    kind: Pod
    metadata:
      name: example-vanilla-block-pod
    spec:
      containers:
        - name: test-container
          image: gcr.io/google_containers/busybox:1.24
          command: ["/bin/sh", "-c", "echo 'hello' > /mnt/volume1/index.html  && chmod o+rX /mnt /mnt/volume1/index.html && while true ; do sleep 2 ; done"]
          volumeMounts:
            - name: test-volume
              mountPath: /mnt/volume1
      restartPolicy: Never
      volumes:
        - name: test-volume
          persistentVolumeClaim:
            claimName: example-vanilla-block-pvc
    ```
    <%# example-pod.yaml  https://github.com/kubernetes-sigs/vsphere-csi-driver/blob/master/example/vanilla-k8s-block-driver/example-pod.yaml #%>

1. To apply the Pod configuration to your workload:  

    ```
    kubectl apply -f CONFIG-FILE 
    ```

    Where `CONFIG-FILE` is the name of your configuration file.  

For more information and examples of Pod configurations, see the `example` configurations 
for the CSI driver version you are using 
in [vsphere-csi-driver](https://github.com/kubernetes-sigs/vsphere-csi-driver/) in the VMware kubernetes-sigs GitHub repo.


<br>
## <a id='vsphere-topology-provisioning'></a>Customize a Cluster with vSphere Topology-Aware Volume Provisioning

TKGI supports the vSphere Container Storage Plug-in's topology-aware volume provisioning features.  

For more information on volume provisioning features, see [Allowed Topologies](https://kubernetes.io/docs/concepts/storage/storage-classes/#allowed-topologies) in the Kubernetes documentation 
and [Topology-Aware Volume Provisioning](https://docs.vmware.com/en/VMware-vSphere-Container-Storage-Plug-in/3.0/vmware-vsphere-csp-getting-started/GUID-61646244-E24F-4E7E-AB1A-F95B5A5DD518.html) 
in the VMware vSphere Container Storage Plug-in documentation.  

<br>
### <a id='overview-topology'></a>Topology Overview 

TKGI supports clusters with topology-aware volume provisioning.  

To create a cluster with topology-aware volume provisioning:  

1. [Prepare for Topology](#prepare-for-topology)  
1. See [Guidelines and Best Practices for Deployment with Topology](https://docs.vmware.com/en/VMware-vSphere-Container-Storage-Plug-in/3.0/vmware-vsphere-csp-getting-started/GUID-162E7582-723B-4A0F-A937-3ACE82EAFD31.html#guidelines-and-best-practices-for-deployment-with-topology-0) 
    in _Deploying vSphere Container Storage Plug-in with Topology_ in the VMware vSphere Container Storage Plug-in documentation.  
1. [Create a Cluster with Topology](#create-topology)  

<br>
To manage a cluster configured with topology-aware volume provisioning:  

1. [Prepare for Topology](#prepare-for-topology)  
1. See [Guidelines and Best Practices for Deployment with Topology](https://docs.vmware.com/en/VMware-vSphere-Container-Storage-Plug-in/3.0/vmware-vsphere-csp-getting-started/GUID-162E7582-723B-4A0F-A937-3ACE82EAFD31.html#guidelines-and-best-practices-for-deployment-with-topology-0) 
    in _Deploying vSphere Container Storage Plug-in with Topology_ in the VMware vSphere Container Storage Plug-in documentation.  
1. [Manage Clusters with Topology-Aware Volumes](#manage-topology)  


<p class="note"><strong>Note</strong>: You cannot add topology-aware volume provisioning to an existing cluster within TKGI.
</p>

<br>
### <a id='prepare-for-topology'></a>Prepare for Topology

Before creating a new cluster with Topology-aware volume provisioning:  

1. Verify your environment meets the requirements listed in [Topology Limitations and Prerequisites](#prereqs-topology) below.  
1. Review the vSphere CSI Topology deployment recommendations. For more information, see 
[Guidelines and Best Practices for Deployment with Topology](https://docs.vmware.com/en/VMware-vSphere-Container-Storage-Plug-in/3.0/vmware-vsphere-csp-getting-started/GUID-162E7582-723B-4A0F-A937-3ACE82EAFD31.html#guidelines-and-best-practices-for-deployment-with-topology-0) 
in _Deploying vSphere Container Storage Plug-in with Topology_ in the VMware vSphere Container Storage Plug-in documentation.  
1. Create vSphere Center categories and tags as described in 
[Procedures](https://docs.vmware.com/en/VMware-vSphere-Container-Storage-Plug-in/3.0/vmware-vsphere-csp-getting-started/GUID-162E7582-723B-4A0F-A937-3ACE82EAFD31.html#procedure-5) 
in _Deploying vSphere Container Storage Plug-in with Topology_ in the VMware vSphere Container Storage Plug-in documentation.  

    
For more information on creating vSphere Center tags and categories, 
see [Create, Edit, or Delete a Tag Category](https://docs.vmware.com/en/VMware-vSphere/7.0/com.vmware.vsphere.vcenterhost.doc/GUID-BA3D1794-28F2-43F3-BCE9-3964CB207FB6.html) 
in the VMware vSphere documentation.  


<br>
### <a id='prereqs-topology'></a>Topology Limitations and Prerequisites 

In TKGI you can create a new cluster with topology-aware volume provisioning enabled. 
You cannot add topology-aware volume provisioning to an existing cluster.

TKGI support for Topology-aware volume provisioning requires:

* The **vSphere CSI Driver Integration** option must be enabled on the TKGI tile. 
For more information, see [Storage](installing-vsphere.html#storage-config) in installing TKGI on vSphere.  

* You have created vSphere CSI topology categories and tags in your vSphere environment. 
For more information, see [Prepare for Topology](#prepare-for-topology) below.  

* You have prepared your environment as described in the vSphere CSI Topology deployment recommendations. For more information, see 
[Guidelines and Best Practices for Deployment with Topology](https://docs.vmware.com/en/VMware-vSphere-Container-Storage-Plug-in/3.0/vmware-vsphere-csp-getting-started/GUID-162E7582-723B-4A0F-A937-3ACE82EAFD31.html#guidelines-and-best-practices-for-deployment-with-topology-0) 
in _Deploying vSphere Container Storage Plug-in with Topology_ in the VMware vSphere Container Storage Plug-in documentation.  

* The topology zone tags you create on your vSphere Client must be consistent with the existing AZs created in BOSH. 
Create topology zone tags on your vSphere Client using only AZ names existing for BOSH.  

* The topology feature does not support clusters with a Compute Profile that includes AZ settings.

<br>
### <a id='create-topology'></a>Create a Cluster with Topology

To create a new cluster with a vSphere Topology configuration:  

1. Create a JSON or YAML configuration file containing the following:  

    ```
    {
      "csi_topology_labels": {
        "topology_categories": "REGION-TAG,ZONE-TAG"
      }
    }
    ```
    
    Where:  
    
    * `REGION-TAG` is the vSphere Center region tag you created in [Prepare for Topology](#prepare-for-topology) above.  
    * `ZONE-TAG` is one of the vSphere Center zone tags you created in [Prepare for Topology](#prepare-for-topology) above.  

    For example:  

    ```
    {
      "csi_topology_labels": {
        "topology_categories": "k8s-region,k8s-zone"
      }
    }
    ```
    
    For more information, see 
    [Guidelines and Best Practices for Deployment with Topology](https://docs.vmware.com/en/VMware-vSphere-Container-Storage-Plug-in/3.0/vmware-vsphere-csp-getting-started/GUID-162E7582-723B-4A0F-A937-3ACE82EAFD31.html#guidelines-and-best-practices-for-deployment-with-topology-0) 
    in _Deploying vSphere Container Storage Plug-in with Topology_ in the VMware vSphere Container Storage Plug-in documentation.  
    
1. To create a cluster with Topology-aware volume provisioning:

    ```
    tkgi create-cluster CLUSTER-NAME --config-file CONFIG-FILE 
    ```
    
    Where:  

    * `CLUSTER-NAME` is the name of your cluster.  
    * `CONFIG-FILE` is the name of your configuration file.  

    For example:  
    
    ```console
    tkgi create-cluster demo -e demo.cluster --plan Small --config-file ./conf1.json
    ```


<br>
### <a id='manage-topology'></a>Manage Clusters with Topology-Aware Volumes

As you manage your clusters with topology-aware volume provisioning enabled, note the following limitations on existing clusters.  

When running `tkgi update-cluster` on a cluster created with a topology-aware volume:  

* You must use the same `csi_topology_labels` configuration that was used during cluster creation.  

* You cannot add or remove topology-aware volume provisioning from the cluster.  

<br>
<br>
## <a id='windows-configure-csi'></a>Configure vSphere CSI for Windows 

You can use the vSphere CSI Driver with TKGI Windows worker nodes.  

<p class="note"><strong>Note</strong>: vSphere CSI driver support for Windows worker nodes is in Alpha.  
</p>

<br>
### <a id='windows-overview'></a>Overview

Before using the vSphere CSI Driver with a TKGI Windows worker node:  

* Verify that your environment meets the [Prerequisites](#windowsfile-volumes-prereqs).  
* Verify that the [Limitations of the vSphere CSI Driver](#windows-concerns) meet your requirements.  

To use the vSphere CSI Driver with a Windows worker:  

* [Prepare a Windows Stemcell for vSphere CSI](#windows-prepare-stemcell).  
* [Prepare vSphere CSI for a Windows Cluster](#windows-prepare-windows-cluster).  

<br>
### <a id='windowsfile-volumes-prereqs'></a>Prerequisites 

vSphere CSI Driver support for Windows workers has the following requirements:

* Windows worker nodes must have Windows Server 2019.  
* CSI Proxy must be installed on each Windows node.  
* The vSphere CSI Driver also has role and privilege requirements that apply to both Linux and Windows worker nodes. 
For more information, see 
[vSphere Roles and Privileges](https://docs.vmware.com/en/VMware-vSphere-Container-Storage-Plug-in/3.0/vmware-vsphere-csp-getting-started/GUID-0AB6E692-AA47-4B6A-8CEA-38B754E16567.html#GUID-043ACF65-9E0B-475C-A507-BBBE2579AA58__GUID-E51466CB-F1EA-4AD7-A541-F22CDC6DE881) 
in the VMware vSphere Container Storage Plug-in documentation.   

For more information, see [Prerequisite](https://github.com/kubernetes-sigs/vsphere-csi-driver/blob/release-2.7/docs/book/features/csi_driver_on_windows.md#prerequisite-) in the Kubernetes SIG vsphere-csi-driver GitHub repository.  

<br>
### <a id='windows-concerns'></a> Limitations of the vSphere CSI Driver

The vSphere CSI driver supports Windows worker nodes as an Alpha feature. This support has additional limitations compared to Linux worker node support.

The vSphere CSI driver does not support the following features for Windows worker nodes:  

* `ReadWriteMany` volumes based on vSAN file service.
* Raw Block Volumes.  

The vSphere CSI Driver has additional limitations. 
For more information, see [Introduction](https://github.com/kubernetes-sigs/vsphere-csi-driver/blob/release-2.7/docs/book/features/csi_driver_on_windows.md#introduction-) in the Kubernetes SIG vsphere-csi-driver GitHub repository.  

<br>
### <a id='windows-prepare-stemcell'></a>Prepare a Windows Stemcell for vSphere CSI

To create a Windows stemcell for vSphere CSI:  

1. [Prepare a CSI Proxy](#windows-prepare-csiproxy).  
1. [Create a Windows Stemcell for vSphere CSI](#windows-create-stemcell).  
1. [Upload and Test the Windows Stemcell](#windows-upload-stemcell).  

<br>
#### <a id='windows-prepare-csiproxy'></a>Prepare a CSI Proxy

The vSphere CSI Driver requires you install a CSI Node Proxy, which exposes local storage operation APIs to Windows nodes, on each Windows node.  

To create a CSI Node Proxy, build a CSI Proxy binary. For more information, see [Build](https://github.com/kubernetes-csi/csi-proxy#build) in the kubernetes-csi / csi-proxy GitHub repository.  

<br>
#### <a id='windows-create-stemcell'></a>Create a Windows Stemcell for vSphere CSI

To create a Windows stemcell that supports vSphere CSI:  

1. Complete the steps in 
[Step 1: Create a Base VM for the BOSH Stemcell](https://docs.vmware.com/en/VMware-Tanzu-Application-Service/5.0/tas-for-vms/create-vsphere-stemcell-automatically.html#create-base-vm).  

1. Complete the steps in 
[Step 2: Configure the Base VM](https://docs.vmware.com/en/VMware-Tanzu-Application-Service/5.0/tas-for-vms/create-vsphere-stemcell-automatically.html#install-windows-updates).  

1. Copy the `csi-proxy.exe` executable file created above to a location for the Windows stemcell. For example, copy the executable to `C:\etc\kubernetes\node\bin\`.  

1. To install csi-proxy.exe, complete the steps in [Installation](https://github.com/kubernetes-csi/csi-proxy#installation) in the kubernetes-csi / csi-proxy GitHub repository.  

    <p class="note"><strong>Note</strong>: If you copied <code>csi-proxy.exe</code> to a location other than <code>C:\etc\kubernetes\node\bin\ </code> you must change the <code>binPath</code> property in the installation script to match.
    </p>

1. Complete the remainder of the standard Windows stemcell creation procedure:

    * [Step 3: Clone the Base VM](https://docs.vmware.com/en/VMware-Tanzu-Application-Service/5.0/tas-for-vms/create-vsphere-stemcell-automatically.html#clone-vm)
    * [Step 4: Construct the BOSH Stemcell](https://docs.vmware.com/en/VMware-Tanzu-Application-Service/5.0/tas-for-vms/create-vsphere-stemcell-automatically.html#construct-stemcell)
    * [Step 5: Package the BOSH Stemcell](https://docs.vmware.com/en/VMware-Tanzu-Application-Service/5.0/tas-for-vms/create-vsphere-stemcell-automatically.html#package-stemcell)
    * [Step 6: Upload the BOSH Stemcell to Ops Manager](https://docs.vmware.com/en/VMware-Tanzu-Application-Service/5.0/tas-for-vms/create-vsphere-stemcell-automatically.html#upload-stemcell)

<br>
#### <a id='windows-upload-stemcell'></a>Upload and Test the Windows Stemcell

To test your Windows stemcell:  

1. In Ops Manager, navigate to **Stemcell Library**.  

1. Upload the Windows stemcell.

1. To test your Windows stemcell:  

    1. Create a test windows cluster.
    1. Complete the steps in [Apply a CSI Manifest to a Windows Cluster](#windows-apply-csi-manifest) below.  

<br>
### <a id='windows-prepare-windows-cluster'></a>Prepare vSphere CSI for a Windows Cluster

To use vSphere CSI with a TKGI Windows worker:  

1. [Apply a CSI Manifest to a Windows Cluster](#windows-apply-csi-manifest)  
1. [Provision a Windows Persistent Volume](#windows-create-pv)  

<br>
#### <a id='windows-apply-csi-manifest'></a>Apply a CSI Manifest to a Windows Cluster

To prepare and apply a vSphere CSI Windows manifest to a Windows worker:  

1. Create a Windows CSI Node manifest file containing the contents listed in [Windows CSI Node Manifest](#windows-csi-manifest) below.  

1. Update your cluster with the Windows CSI Node manifest.  

<br>
#### <a id='windows-create-pv'></a>Provision a Windows Persistent Volume

To provision a Windows persistent volume for a Windows application, 
configure a Windows PersistentVolumeClaim. For more information, see 
[Deploying vSphere Container Storage Plug-in on Windows](https://docs.vmware.com/en/VMware-vSphere-Container-Storage-Plug-in/3.0/vmware-vsphere-csp-getting-started/GUID-85203E57-71DD-42B7-AD98-8CE06AEDF4C3.html)
 in the VMware vSphere Container Storage Plug-in documentation.  

<br>
<br>
## <a id='manage-cns'></a>Customize and Manage vSphere CNS 

To configure or manage your vSphere CSI Driver:  

* [Customize the Maximum Number of Persistent Volumes](#customize-max-persistent-volumes)  
* [Customize the Maximum Number of Volume Snapshots](#customize-max-snapshot)  
* [Configure CNS Data Centers](#cns-datacenters)  
* [Manage Topology After Switching to the Automatically Deployed vSphere CSI Driver](#uninstall-csi-after-topology)  

To configure or manage vSphere CSI on a Windows cluster:  

* [Windows CSI Node Manifest](#windows-csi-manifest)  


<br>
### <a id='customize-max-persistent-volumes'></a>Customize the Maximum Number of Persistent Volumes

You can limit the number of persistent volumes attached to a Linux cluster node on vSphere. 
You can configure the maximum number of node persistent volumes on an existing cluster and during cluster creation.  

By default, TKGI configures Linux clusters on vSphere with a maximum of 45 attached persistent volumes. 
You can decrease the maximum number of attached persistent volumes from 45 down to a minimum of 1. 
On vSphere 8 you can also increase the maximum number of attached persistent volumes. 
Contact VMware Support to determine the maximum number of attached persistent volumes supported by your vSphere environment.  

To configure the maximum number of persistent volumes attached to a Linux cluster node:  

1. Create a configuration file containing the following:  

    ```
    {
      max_volumes_per_node: MAX-VOLS
    }
    ```
    
    Where `MAX-VOLS` is the maximum number of persistent volumes that can be attached to a cluster's nodes. 
    On vSphere 7 the range of supported `max_volumes_per_node` values is `1` to `45`. 
    On vSphere 8 the theoretical range is from `1` to `192`. Contact VMware Support to determine the maximum number of attached persistent volumes supported by your vSphere environment.  
    
    For example: 
    
    ```
    {
      max_volumes_per_node: 45
    }
    ```

1. To update an existing cluster with the configuration:

    ```
    tkgi update-cluster CLUSTER-NAME --config-file CONFIG-FILE 
    ```

    Where:  

    * `CLUSTER-NAME` is the name of your cluster.  
    * `CONFIG-FILE` is the name of the configuration file you created in the preceding steps.  


<br>
### <a id='customize-max-snapshot'></a>Customize the Maximum Number of Volume Snapshots
The vSphere CSI driver lets you customize the maximum number of snapshots for a persistent volume.  By default, the system sets a maximum of three
volume snapshots as suggested by the VMware snapshots best practices in a vSphere environment. 

In your cluster configuration file, use the following parameters to customize the maximum number of snapshots:

- `global-max-snapshots-per-block-volume` for the block volumes on VMFS datastores. If you do not use this parameter, the system sets the 
maximum snapshots for the block volumes to 3.
- `granular-max-snapshots-per-block-volume-vsan` for the volumes on VMware vSAN. If you do not use this parameter, the system sets the 
maximum snapshots for the vSAN volumes to the value specified for `global-max-snapshots-per-block-volume`.

To customize the maximum number of snapshots on a persistent volume, create a JSON or YAML formatted configuration file containing the following:  

```
{
  "snapshot_parameters":{
      "global_max_snapshots_per_block_volume": NUMBER       
  }    
}
```
    
Where:  
    
* `NUMBER` is the maximum number of snapshots on the volume.
    
For example:  
    
```
{
  "snapshot_parameters":{
      "global_max_snapshots_per_block_volume": 4
  }
}
  
```
    
To customize the maximum number of snapshots on a vSAN volume, create a JSON or YAML formatted configuration file containing the following:  
    
```
{
  "snapshot_parameters":{
      "granular_max_snapshots_per_block_volume_vsan": NUMBER       
  }    
}
```
        
Where:  
        
* `NUMBER` is the maximum number of snapshots on the volume.
        
For example:  
        
```
{
  "snapshot_parameters":{
      "granular_max_snapshots_per_block_volume_vsan": 4
  }
}  
```
To create a new cluster or update an existing cluster with the new snapshot configuration:  
    
  * To create a cluster:  
            
    ```
    tkgi create-cluster CLUSTER-NAME --config-file CONFIG-FILE  
    ```
            
    Where:  
      
    * `CLUSTER-NAME` is the name of your cluster.  
    * `CONFIG-FILE` is the name of your configuration file.  
    
    For example:  
    
    ```console
    tkgi create-cluster demo -e demo.cluster –plan Small –config-file ./snapshot.json
    ```
    
  * To update an existing cluster:  
    
    ```
    tkgi update-cluster CLUSTER-NAME --config-file CONFIG-FILE 
    ```
    
    Where:  
    
    * `CLUSTER-NAME` is the name of your cluster.  
    * `CONFIG-FILE` is the name of your configuration file. 
      
    For example:  
    
    ```console
    tkgi update-cluster demo --config-file ./snapshot.json
    ```
    <p class="note warning"><strong>WARNING</strong>: Update the configuration file only on a TKGI cluster that has been upgraded to the current TKGI version. For more information, see <a href="understanding-upgrades.html#control-plane-upgrades-supported-tasks">Tasks Supported Following a TKGI Control Plane Upgrade</a> in <em>About Tanzu Kubernetes Grid Integrated Edition Upgrades</em>.
    </p>
    
For more information on volume snapshots, see 
[Volume Snapshot and Restore](https://docs.vmware.com/en/VMware-vSphere-Container-Storage-Plug-in/3.0/vmware-vsphere-csp-getting-started/GUID-E0B41C69-7EEB-450F-A73D-5FD2FF39E891.html) 
in VMware vSphere Container Storage Plug-in Documentation.


<br>
### <a id='cns-datacenters'></a>Configure CNS Data Centers

If your clusters are in a multi-data center environment, 
configure the data centers that must mount CNS storage for the clusters.  

<p class="note"><strong>Note</strong>: 
You must configure CNS data centers when the Topology feature is enabled in a multi-data center environment. 
</p>

To configure CNS data centers for a multi-data center environment:  
 
1. Create a JSON or YAML formatted configuration file containing the following:  

    ```
    {
      "csi_datacenters": "DATA-CENTER-LIST"
    }
    ```
    
    Where:  
    
    * `DATA-CENTER-LIST` is a comma-separated list of vCenter data centers that must mount your CNS storage. 
    The default data center for a cluster is the data center defined on the TKGI tile 
    in **Kubernetes Cloud Provider** > **Datacenter Name**.  
    
    For example:  
    
    ```
    {
      "csi_datacenters": "kubo-dc1,kubo-dc2"
    }
    ```
    For more information on the `csi_datacenters` parameter, 
    see the description of `datacenters` in 
    [Procedure](https://docs.vmware.com/en/VMware-vSphere-Container-Storage-Plug-in/3.0/vmware-vsphere-csp-getting-started/GUID-BFF39F1D-F70A-4360-ABC9-85BDAFBE8864.html#procedure-1) 
    in _Create a Kubernetes Secret for vSphere Container Storage Plug-in_.  
    
1. To create a new cluster or update an existing cluster with your vCenter data centers:  

    * To create a cluster:  
        
        ```
        tkgi create-cluster CLUSTER-NAME --config-file CONFIG-FILE  
        ```
        
        Where:  

        * `CLUSTER-NAME` is the name of your cluster.  
        * `CONFIG-FILE` is the name of your configuration file.  

        For example:  
        
        ```console
        tkgi create-cluster demo -e demo.cluster --plan Small --config-file ./conf1.json
        ```


    * To update an existing cluster:  

        ```
        tkgi update-cluster CLUSTER-NAME --config-file CONFIG-FILE 
        ```

        Where:  

        * `CLUSTER-NAME` is the name of your cluster.  
        * `CONFIG-FILE` is the name of your configuration file.  
        
        <p class="note warning"><strong>WARNING</strong>: Update the configuration file only on a TKGI cluster that has been upgraded to the current TKGI version. For more information, see <a href="understanding-upgrades.html#control-plane-upgrades-supported-tasks">Tasks Supported Following a TKGI Control Plane Upgrade</a> in <em>About Tanzu Kubernetes Grid Integrated Edition Upgrades</em>.
        </p>        
<br>
### <a id='uninstall-csi-after-topology'></a>Manage Topology After Switching to the Automatically Deployed vSphere CSI Driver

After switching from a manually installed vSphere CSI Driver to the TKGI automatically deployed CSI Driver, 
the topology configuration must not be changed.  

Configure topology based on the manually installed vSphere CSI Driver configuration:  

* **Region and Zone Topology Labels**:  

    You must continue to use `region`, and `zone` labels if your manually deployed vSphere CSI Driver was configured using 
    the legacy `region`, and `zone` topology configuration labels.  


    Your revised cluster configuration file must include a `csi_topology_labels` parameter that assigns `region` and `zone` values.  
    
    For example, if your vSphere Secret configuration for the manually installed vSphere CSI driver included the following:  
    
    ```
    [Labels]
    region = k8s-region
    zone = k8s-zone
    ```
    
    Your new cluster configuration must include the following instead:
    
    ```
    { 
      "csi_topology_labels": { 
          "region: "k8s-region" 
          "zone": "k8s-zone" 
      } 
    }
    ```

* **topology_categories Topology Label**:  
    
    You must continue to use the `topology_categories` label if your manually deployed vSphere CSI Driver was configured using 
    the `topology_categories` topology configuration label.  

    Your revised cluster configuration file must include a `csi_topology_labels` parameter that assigns a `topology_categories` value.  
    
    For example, if your vSphere Secret configuration for the manually installed vSphere CSI driver included the following:  
    
    ```
    [Labels]
    topology-categories = "k8s-region, k8s-zone"

    ```
    
    Your new cluster configuration must include the following instead:  
    
    ```
    { 
      "csi_topology_labels": { 
        "topology_categories": "k8s-region,k8s-zone" 
      } 
    }
    ```
    
* **Topology Deactivated**:  

    You must not enable topology if topology was not enabled while using the manually deployed vSphere CSI Driver.



<br>
### <a id='windows-csi-manifest'></a>Windows CSI Node Manifest
```
---
kind: ServiceAccount
apiVersion: v1
metadata:
  name: vsphere-csi-node
  namespace: vmware-system-csi
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: vsphere-csi-node-cluster-role
rules:
  - apiGroups: ["cns.vmware.com"]
    resources: ["csinodetopologies"]
    verbs: ["create", "watch", "get", "patch"]
  - apiGroups: [""]
    resources: ["nodes"]
    verbs: ["get"]
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: vsphere-csi-node-cluster-role-binding
subjects:
  - kind: ServiceAccount
    name: vsphere-csi-node
    namespace: vmware-system-csi
roleRef:
  kind: ClusterRole
  name: vsphere-csi-node-cluster-role
  apiGroup: rbac.authorization.k8s.io
---
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: vsphere-csi-node-role
  namespace: vmware-system-csi
rules:
  - apiGroups: [""]
    resources: ["configmaps"]
    verbs: ["get", "list", "watch"]
---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: vsphere-csi-node-binding
  namespace: vmware-system-csi
subjects:
  - kind: ServiceAccount
    name: vsphere-csi-node
    namespace: vmware-system-csi
roleRef:
  kind: Role
  name: vsphere-csi-node-role
  apiGroup: rbac.authorization.k8s.io
---
kind: DaemonSet
apiVersion: apps/v1
metadata:
  name: vsphere-csi-node-windows
  namespace: vmware-system-csi
spec:
  selector:
    matchLabels:
      app: vsphere-csi-node-windows
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
  template:
    metadata:
      labels:
        app: vsphere-csi-node-windows
        role: vsphere-csi-windows
    spec:
      priorityClassName: system-node-critical
      nodeSelector:
        kubernetes.io/os: windows
      serviceAccountName: vsphere-csi-node
      containers:
        - name: node-driver-registrar
          image: k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.5.1
          args:
            - "--v=5"
            - "--csi-address=$(ADDRESS)"
            - "--kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)"
          env:
            - name: ADDRESS
              value: 'unix://C:\\csi\\csi.sock'
            - name: DRIVER_REG_SOCK_PATH
              value: 'C:\\var\\lib\\kubelet\\plugins\\csi.vsphere.vmware.com\\csi.sock'
          volumeMounts:
            - name: plugin-dir
              mountPath: /csi
            - name: registration-dir
              mountPath: /registration
          livenessProbe:
            exec:
              command:
              - /csi-node-driver-registrar.exe
              - --kubelet-registration-path=C:\\var\\lib\\kubelet\\plugins\\csi.vsphere.vmware.com\\csi.sock
              - --mode=kubelet-registration-probe
            initialDelaySeconds: 3
        - name: vsphere-csi-node
          image: gcr.io/cloud-provider-vsphere/csi/ci/driver:latest
          args:
            - "--fss-name=internal-feature-states.csi.vsphere.vmware.com"
            - "--fss-namespace=$(CSI_NAMESPACE)"
          imagePullPolicy: "Always"
          env:
            - name: NODE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: spec.nodeName          
            - name: CSI_ENDPOINT
              value: 'unix://C:\\csi\\csi.sock'
            - name: MAX_VOLUMES_PER_NODE
              value: "59" # Maximum number of volumes that controller can publish to the node. If value is not set or zero Kubernetes decide how many volumes can be published by the controller to the node.
            - name: X_CSI_MODE
              value: node
            - name: X_CSI_SPEC_REQ_VALIDATION
              value: 'false'
            - name: X_CSI_SPEC_DISABLE_LEN_CHECK
              value: "true"
            - name: LOGGER_LEVEL
              value: "PRODUCTION" # Options: DEVELOPMENT, PRODUCTION
            - name: X_CSI_LOG_LEVEL
              value: DEBUG
            - name: CSI_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: NODEGETINFO_WATCH_TIMEOUT_MINUTES
              value: "1"
          volumeMounts:
            - name: plugin-dir
              mountPath: 'C:\csi'
            - name: pods-mount-dir
              mountPath: 'C:\var\lib\kubelet'
            - name: csi-proxy-volume-v1
              mountPath: \\.\pipe\csi-proxy-volume-v1
            - name: csi-proxy-filesystem-v1
              mountPath: \\.\pipe\csi-proxy-filesystem-v1
            - name: csi-proxy-disk-v1
              mountPath: \\.\pipe\csi-proxy-disk-v1    
            - name: csi-proxy-system-v1alpha1
              mountPath: \\.\pipe\csi-proxy-system-v1alpha1
          ports:
            - name: healthz
              containerPort: 9808
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /healthz
              port: healthz
            initialDelaySeconds: 10
            timeoutSeconds: 5
            periodSeconds: 5
            failureThreshold: 3
        - name: liveness-probe
          image: k8s.gcr.io/sig-storage/livenessprobe:v2.7.0
          args:
            - "--v=4"
            - "--csi-address=/csi/csi.sock"
          volumeMounts:
            - name: plugin-dir
              mountPath: /csi
      volumes:
        - name: registration-dir
          hostPath:
            path: 'C:\var\lib\kubelet\plugins_registry\'
            type: DirectoryOrCreate
        - name: plugin-dir
          hostPath:
            path: 'C:\var\lib\kubelet\plugins\csi.vsphere.vmware.com\'
            type: DirectoryOrCreate
        - name: pods-mount-dir
          hostPath:
            path: 'C:\var\lib\kubelet'
            type: Directory
        - name: csi-proxy-disk-v1
          hostPath:
            path: \\.\pipe\csi-proxy-disk-v1
            type: ''
        - name: csi-proxy-volume-v1
          hostPath:
            path: \\.\pipe\csi-proxy-volume-v1
            type: ''
        - name: csi-proxy-filesystem-v1
          hostPath:
            path: \\.\pipe\csi-proxy-filesystem-v1
            type: ''
        - name: csi-proxy-system-v1alpha1
          hostPath:
            path: \\.\pipe\csi-proxy-system-v1alpha1
            type: ''
      tolerations:
        - effect: NoExecute
          operator: Exists
        - effect: NoSchedule
          operator: Exists 
 
```
