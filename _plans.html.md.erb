<% if current_page.data.windowsbeta == true %>
A plan defines a set of resource types used for deploying a cluster.   

<p class="note"><strong>Note</strong>: Before configuring your Windows worker plan, you must first activate and configure <strong>Plan 1</strong>.
See <a href="installing-pks-vsphere.html#plans">Plans</a> in <i>Installing Enterprise PKS on vSphere</i> for more information.  
</p>
<% else %>
A plan defines a set of resource types used for deploying a cluster.  

You must first activate and configure <strong>Plan 1</strong>, 
<% end %>
<% if current_page.data.windowsbeta != true && current_page.data.iaas != "vSphere" %>
and afterwards you can optionally activate **Plan 2** through **Plan 10**.  
<% end %>
<% if current_page.data.windowsbeta != true && current_page.data.iaas == "vSphere" %>
and afterwards you can activate up to twelve additional, optional, plans.  
<% end %>

To activate and configure a plan, perform the following steps:

<% if current_page.data.windowsbeta == true %>
1. Click the plan that you want to activate. 
You must activate and configure either **Plan 11**, **Plan 12**, or **Plan 13** to deploy a Windows worker-based cluster.  

1. Select **Active** to activate the plan and make it available to developers deploying clusters.<br>
  ![Plan pane configuration](images/plan11-win.png)
<% end %>

<% if current_page.data.windowsbeta != true && current_page.data.iaas != "vSphere" %>
1. Click the plan that you want to activate.  
  <p class="note"><strong>Note</strong>: Plans 11, 12 and 13 support only Windows worker-based Kubernetes clusters, on vSphere with Flannel.
  </p>
1. Select **Active** to activate the plan and make it available to developers deploying clusters.<br>
  ![Plan pane configuration](images/plan1.png)
<% end %>
<% if current_page.data.windowsbeta != true && current_page.data.iaas == "vSphere" %>
1. Click the plan that you want to activate.  
  <p class="note"><strong>Note</strong>: Plans 11, 12 and 13 support only Windows worker-based Kubernetes clusters, on vSphere with Flannel. 
  To configure a Windows worker plan see <a href="windows-pks-beta.html#plans">Plans</a> in <i>Configuring Windows Worker-Based 
  Kubernetes Clusters (Beta)</i> for more information.    
  </p>
1. Select **Active** to activate the plan and make it available to developers deploying clusters.<br>
  ![Plan pane configuration](images/plan1.png)
<% end %>
1. Under **Name**, provide a unique name for the plan.
1. Under **Description**, edit the description as needed.
The plan description appears in the Services Marketplace, which developers can access by using the PKS CLI.
<% if current_page.data.windowsbeta == true %>
1. Verify that, under **Worker OS**, **Set to Windows** is selected. 
<% end %>
1. Under **Master/ETCD Node Instances**, select the default number of Kubernetes master/etcd nodes to provision for each cluster.
  You can enter <code>1</code>, <code>3</code>, or <code>5</code>.
  <p class="note"><strong>Note</strong>: If you deploy a cluster with multiple master/etcd node VMs,
    confirm that you have sufficient hardware to handle the increased load on disk write and network traffic. For more information, see <a href="https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/hardware.md#example-hardware-configurations">Hardware recommendations</a> in the etcd documentation.<br><br>
    In addition to meeting the hardware requirements for a multi-master cluster, we recommend configuring monitoring for etcd to monitor disk latency, network latency, and other indicators for the health of the cluster. For more information, see <a href="monitor-etcd.html">Monitoring Master/etcd Node VMs</a>.</p>
  <p class="note warning"><strong>WARNING</strong>: To change the number of master/etcd nodes for a plan, you must ensure that no existing clusters use the plan. <%= vars.product_short %> does not support changing the number of master/etcd nodes for plans with existing clusters.
  </p>
1. Under **Master/ETCD VM Type**, select the type of VM to use for Kubernetes master/etcd nodes. For more information, including master node VM customization options, see the [Master Node VM Size](vm-sizing.html#master-sizing) section of _VM Sizing for <%= vars.product_short %> Clusters_.

1. Under **Master Persistent Disk Type**, select the size of the persistent disk for the Kubernetes master node VM.

1. Under **Master/ETCD Availability Zones**, select one or more AZs for the Kubernetes clusters deployed by <%= vars.product_short %>. 
If you select more than one AZ, <%= vars.product_short %> deploys the master VM in the first AZ and the worker VMs across the remaining AZs. 
If you are using multiple masters, <%= vars.product_short %> deploys the master and worker VMs across the AZs in round-robin fashion.

1. Under **Maximum number of workers on a cluster**, set the maximum number of
Kubernetes worker node VMs that <%= vars.product_short %> can deploy for each cluster. Enter any whole number in this field. 
<br>
<% if current_page.data.windowsbeta == true %>
  ![Plan pane configuration, part two](images/plan2-win.png)
<% else %>
  ![Plan pane configuration, part two](images/plan2.png)
<% end %>
<br>
1. Under **Worker Node Instances**, select the default number of Kubernetes worker nodes to provision for each cluster.
  <br><br>
  If the user creating a cluster with the PKS CLI does not specify a number of worker nodes, the cluster is deployed with the default number set in this field. This value cannot be greater than the maximum worker node value you set in the previous field. For more information about creating clusters, see [Creating Clusters](create-cluster.html).
  <br><br>
  For high availability, create clusters with a minimum of three worker nodes, or two per AZ if you intend to use PersistentVolumes (PVs). For example, if you deploy across three AZs, you should have six worker nodes. For more information about PVs, see [PersistentVolumes](maintain-uptime.html#persistent-volumes) in *Maintaining Workload Uptime*. Provisioning a minimum of three worker nodes, or two nodes per AZ is also recommended for stateless workloads.
  <br><br>
  If you later reconfigure the plan to adjust the default number of worker nodes, the existing clusters that have been created from that plan are not automatically upgraded with the new default number of worker nodes.
1. Under **Worker VM Type**, select the type of VM to use for Kubernetes worker node VMs. For more information, including worker node VM customization options, see the [Worker Node VM Number and Size](vm-sizing.html#worker-sizing) section of _VM Sizing for <%= vars.product_short %> Clusters_.
<% if current_page.data.iaas == "vSphere-NSX-T" %>
    <p class="note"><strong>Note</strong>: If you install <%= vars.product_short %> in an NSX-T environment, we recommend that you select a <strong>Worker VM Type</strong> with a minimum disk size of 16&nbsp;GB. 
      The disk space provided by the default <code>medium</code> Worker VM Type is insufficient for <%= vars.product_short %> with NSX-T.</p>
<% end %>
<% if current_page.data.windowsbeta == true %>
    <p class="note"><strong>Note:</strong> BOSH does not support persistent disks for Windows VMs. 
    If specifying **Worker Persistent Disk Type** on a Windows worker is a requirement for you, 
    submit feedback by sending an email to pcf-windows@pivotal.io.
    </p>
<% else %>
1. Under **Worker Persistent Disk Type**, select the size of the persistent disk for the Kubernetes worker node VMs.
<% end %>

1. Under **Worker Availability Zones**, select one or more AZs for the Kubernetes worker nodes. <%= vars.product_short %> deploys worker nodes equally across the AZs you select.

1. Under **Kubelet customization - system-reserved**, enter resource values that Kubelet can use to reserve resources for system daemons. For example, `memory=250Mi, cpu=150m`. For more information about system-reserved values, see the [Kubernetes documentation](https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/#system-reserved).
![Plan pane configuration, part two](images/plan2b.png)
1. Under **Kubelet customization - eviction-hard**, enter threshold limits that Kubelet can use to evict pods when they exceed the limit. Enter limits in the format `EVICTION-SIGNAL=QUANTITY`. For example, `memory.available=100Mi, nodefs.available=10%, nodefs.inodesFree=5%`. For more information about eviction thresholds, see the [Kubernetes documentation](https://kubernetes.io/docs/tasks/administer-cluster/out-of-resource/#hard-eviction-thresholds).
  <p class="note warning"><strong>WARNING</strong>: Use the Kubelet customization fields with caution. If you enter values that are invalid or that exceed the limits the system supports, Kubelet might fail to start. If Kubelet fails to start, you cannot create clusters.</p>
<% if current_page.data.windowsbeta == true %>
1. Under **Kubelet customization - Windows pause image location**, enter the location of your Windows pause image. 
You must configure this field if you want to deploy Windows pods in an air-gapped environment. 
By default, <%= vars.product_short %> pulls the pause image from `mcr.microsoft.com/k8s/core/pause:1.2.0`.
<% end %>
1. Under **Errand VM Type**, select the size of the VM that contains the errand. The smallest instance possible is sufficient, as the only errand running on this VM is the one that applies the **Default Cluster App** YAML configuration.
1. (Optional) Under **(Optional) Add-ons - Use with caution**, enter additional YAML configuration to add custom workloads to each cluster in this plan. You can specify multiple files using `---` as a separator. For more information, see [Adding Custom Workloads](custom-workloads.html).
<% if current_page.data.windowsbeta == true %>
  ![Plan pane configuration](images/plan3-win.png)
<% else %>
  ![Plan pane configuration](images/plan3.png)
1. (Optional) To allow users to create pods with privileged containers, select the **Allow Privileged** option. 
For more information, see [Pods](https://kubernetes.io/docs/concepts/workloads/pods/pod/#privileged-mode-for-pod-containers) 
in the Kubernetes documentation.
<% end %>
<% if current_page.data.windowsbeta == true %>
    <p class="note"><strong>Note:</strong> Windows in Kubernetes does not support privileged containers. 
    See <a href="https://kubernetes.io/docs/setup/production-environment/windows/intro-windows-in-kubernetes/#feature-restrictions">Feature Restrictions</a> 
    in the Kubernetes documentation for additional information.
    </p>
1. (Optional) Enable or disable one or more admission controller plugins: **PodSecurityPolicy**, and **SecurityContextDeny**. 
See [Admission Plugins](./admission-plugins.html) for more information. Windows in Kubernetes does not support the **DenyEscalatingExec** Admission Plugin feature. 
See <a href="https://kubernetes.io/docs/setup/production-environment/windows/intro-windows-in-kubernetes/#api">API</a> 
in the Kubernetes documentation for additional information.</p>
<% else %>
1. (Optional) Enable or disable one or more admission controller plugins: **PodSecurityPolicy**, **DenyEscalatingExec**, and **SecurityContextDeny**. See [Admission Plugins](./admission-plugins.html) for more information.
<% end %>
<% if current_page.data.windowsbeta != true %>
1. (Optional) Under **Node Drain Timeout(mins)**, enter the timeout in minutes for the node to drain pods. If you set this value to `0`, the node drain does not terminate.
    ![Node Drain Timeout fields](images/node-drain.png)
1. (Optional) Under **Pod Shutdown Grace Period (seconds)**, enter a timeout in seconds for the node to wait before it forces the pod to terminate. If you set this value to `-1`, the default timeout is set to the one specified by the pod.

1. (Optional) To configure when the node drains, enable the following:
    * **Force node to drain even if it has running pods not managed by a ReplicationController, ReplicaSet, Job, DaemonSet or StatefulSet.**
    * **Force node to drain even if it has running DaemonSet-managed pods.**
    * **Force node to drain even if it has running running pods using emptyDir.**
    * **Force node to drain even if pods are still running after timeout.**

    <p class="note warning"><strong>Warning:</strong> If you select <strong>Force node to drain even if pods are still running after timeout</strong>, the node kills all running workloads on pods.
    Before enabling this configuration, set <strong>Node Drain Timeout</strong> to a value greater than <code>0</code>.</p>

    For more information about configuring default node drain behavior, see [Worker Node Hangs Indefinitely](./troubleshoot-issues.html#upgrade-drain-hangs) in _Troubleshooting_.
<% end %>

<% if current_page.data.iaas == "vSphere-NSX-T" || "vSphere" %>

   *Configure Tanzu Mission Control Integration**

   VMware Tanzu Mission Control provides a single point of control to manage configuration and policy across all Kubernetes clusters, regardless of where they are running. If you are a participant in the Tanzu Mission Control Beta program, you can integrate your Enterprise PKS deployment with Tanzu Mission Control. When you integrate Enterprise PKS with Tanzu Mission Control, any clusters that you deploy are visible for monitoring and management in the Tanzu Mission Control dashboard.
  
   Before you configure the integration, you must have an API token for an active VMware Tanzu Mission Control account. For more information, see the [VMware Tanzu Mission Control home page](https://cloud.vmware.com/tanzu-mission-control).

   **Tanzu Mission Control Integration**: Select **Yes** to enable.

   **Tanzu Mission Control URL**: Enter the URL of your Tanzu Mission Control Subscription.

   **Tanzu Mission Control API token**: Enter the API token of your Tanzu Mission Control Subscription.

   **Tanzu Mission Control Cluster Group**: Enter the default cluster group in Tanzu Mission Control.

   **Tanzu Mission Control Cluster Name Prefix**: Enter the name with which the PKS cluster names are to be prefixed in Tanzu Mission Control.

<% end %>

1. Click **Save**.

<% if current_page.data.windowsbeta != true %>
To deactivate a plan, perform the following steps:

1. Click the plan that you want to deactivate.
1. Select **Inactive**.
1. Click **Save**.
<% end %>
