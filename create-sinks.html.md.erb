---
title: Creating and Managing Sink Resources
owner: TKGI
---

This topic describes how to create and manage sink resources for a Kubernetes cluster
provisioned with <%= vars.product_full %> (<%= vars.k8s_runtime_abbr %>), or
for a namespace within a cluster.

## <a id='overview'></a>Overview

Sinks collect logs and metrics about Kubernetes worker nodes in your
<%= vars.k8s_runtime_abbr %> deployment and workloads that are running on them.

You can create two types of sinks:

* Log sinks
* Metric sinks

For more conceptual information about sinks, see
[Sink Architecture in <%= vars.product_short %>](sink-architecture.html).

## <a id='prerequisites'></a>Prerequisites

Before creating a sink resource:

1. Review [Sink Types](sink-architecture.html#types) in
_Sink Architecture in <%= vars.product_short %>_.
1. Configure sink resources in the **<%= vars.product_tile %>** tile >
**In-Cluster Monitoring**:
  * If you want to create a `ClusterLogSink` or `LogSink` resource, select
  the **Enable Log Sink Resources** checkbox.
  * If you want to create a `ClusterMetricSink` or `MetricSink` resource,
  select the **Enable Metric Sink Resources** checkbox.
  * If you want to use Node Exporter to send worker node metrics to
  metric sinks of kind `ClusterMetricSink` as described in
  [Create a ClusterMetricSink Resource for Node Exporter Metrics](#node-exporter)
  below, select the **Enable node exporter on workers** checkbox.
  <br><br>
      For more information about these configuration settings,
      see the <%= vars.k8s_runtime_abbr %> installation topic for your IaaS:
      * [Installing <%= vars.product_short %> on vSphere](installing-vsphere.html#cluster-monitoring)
      * [Installing <%= vars.product_short %> on vSphere with NSX-T Integration](installing-nsx-t.html#cluster-monitoring)
      * [Installing <%= vars.product_short %> on GCP](installing-gcp.html#cluster-monitoring)
      * [Installing <%= vars.product_short %> on AWS](installing-aws.html#cluster-monitoring)
      * [Installing <%= vars.product_short %> on Azure](installing-azure.html#cluster-monitoring)

1. Install the Kubernetes CLI, <code>kubectl</code>.
For installation instructions, see <a href="installing-kubectl-cli.html">Installing the Kubernetes CLI</a>.


<br>
## <a id='define-log-sinks'></a><a id='define-resource'></a> Create LogSink or ClusterLogSink Resources

To create `ClusterLogSink` or `LogSink` resources, you can:

* [Create a Syslog ClusterLogSink or LogSink Resource](#syslog)
* [Create a Webhook ClusterLogSink or LogSink Resource ](#webhook)
* [Create a ClusterLogSink or LogSink Resource with a Fluent Bit Output Plugin](#fluentbit-output-plugin)

<p class="note"><strong>Note:</strong> Log sinks created in <%= vars.k8s_runtime_abbr %> do not support UDP connections.</p>

<p class="note"><strong>Note:</strong> <%= vars.k8s_runtime_abbr %> requires a secure connection
for log forwarding when using <code>ClusterLogSink</code> and <code>LogSink</code> resources of type <code>syslog</code> or <code>webhook</code>. To forward logs using an unsecured connection, see
<a href="#unsecured-sink-configuration">Unsecured ClusterLogSink and LogSink Log Forwarding</a> below.
</p>


<br>
### <a id='syslog'></a> Create a Syslog ClusterLogSink or LogSink Resource

`ClusterLogSink` and `LogSink` resources of type `syslog` deliver logs using the TCP-based syslog protocol.  

By default, <%= vars.k8s_runtime_abbr %> uses a system root certificate authority (CA) certificate 
to secure `syslog` `ClusterLogSink` and `LogSink` log forwarding connections, 
but you can optionally use a custom CA certificate to secure the connections.  

To define a `syslog` `ClusterLogSink` or `LogSink` resource, perform the following steps:

1. Create a YAML file that specifies your log destination in one of the following formats:
    * To use the default system root CA certificate to secure log forwarding connections:  
        
        ```
        apiVersion: pksapi.io/v1beta1
        kind: YOUR-SINK-RESOURCE
        metadata:
           name: YOUR-SINK
           namespace: YOUR-NAMESPACE
        spec:
           type: syslog
           host: YOUR-LOG-DESTINATION
           port: YOUR-LOG-DESTINATION-PORT
           enable_tls: true
           insecure_skip_verify: false
        ```
        Where:  

        * `YOUR-SINK-RESOURCE` is the sink resource that you want to create.
        This must be either `ClusterLogSink` or `LogSink`.
        For information about these sink resources, see [Overview](#overview).  
        * `YOUR-SINK` is a name that you choose for your sink.  
        * `YOUR-NAMESPACE` is the name of your namespace.
        Omit this line if you are creating `ClusterLogSink`.  
        * `YOUR-LOG-DESTINATION` is the URL or IP address of your log management service.  
        * `YOUR-LOG-DESTINATION-PORT` is the port number of your log management service.  

        <p class="note"><strong>Note:</strong> <code>enable_tls</code> must be <code>true</code>.</p>
    * To use a custom CA certificate to secure log forwarding connections:  

        ```
        apiVersion: pksapi.io/v1beta1
        kind: YOUR-SINK-RESOURCE
        metadata:
           name: YOUR-SINK
           namespace: YOUR-NAMESPACE
        spec:
           type: syslog
           host: YOUR-LOG-DESTINATION
           port: YOUR-LOG-DESTINATION-PORT
           enable_tls: true
           insecure_skip_verify: false
           fluent_bit_ca_cert: YOUR-CA-CERT
        ```
        Where:  

        * `YOUR-SINK-RESOURCE` is the sink resource that you want to create.
        This must be either `ClusterLogSink` or `LogSink`.
        For information about these sink resources, see [Overview](#overview).  
        * `YOUR-SINK` is a name that you choose for your sink.  
        * `YOUR-NAMESPACE` is the name of your namespace.
        Omit this line if you are creating `ClusterLogSink`.  
        * `YOUR-LOG-DESTINATION` is the URL or IP address of your log management service.  
        * `YOUR-LOG-DESTINATION-PORT` is the port number of your log management service.  
        * `YOUR-CA-CERT` is your custom CA certificate to secure the `syslog` connection.  

        <p class="note"><strong>Note:</strong> <code>enable_tls</code> must be <code>true</code>.</p>

1. (Optional) To filter the logging output, include a `filters` section in your configuration. 
For more information, see [Define a Filter for LogSink and ClusterLogSink Resources](#filter-ls-cls) below.  

1. Save the YAML file with an appropriate file name. For example, `my-cluster-log-sink.yml`.

1. Apply the `ClusterLogSink` or `LogSink` resource to your cluster by running the following command:  

    ```
    kubectl apply -f YOUR-SINK.yml
    ```
    Where `YOUR-SINK.yml` is the name of your YAML file.  
<br>
    For example:  
    <pre class='terminal'>
    $ kubectl apply -f my-cluster-log-sink.yml
    </pre>


<br>
### <a id='webhook'></a> Create a Webhook ClusterLogSink or LogSink Resource

`ClusterLogSink` and `LogSink` resources of type `webhook` batch logs into one-second units,
wrap the resulting payload in JSON, and use the `POST` method to deliver the logs to the address of your
log management service.

To define a webhook `ClusterLogSink` or `LogSink` resource, perform the following steps:

1. Create a YAML file that specifies your log destination in the following format:  

    ```
    apiVersion: pksapi.io/v1beta1
    kind: YOUR-SINK-RESOURCE
    metadata:
      name: YOUR-SINK
      namespace: YOUR-NAMESPACE
    spec:
      type: webhook
      url: YOUR-LOG-DESTINATION
    ```
    Where:  

    * `YOUR-SINK-RESOURCE` is the sink resource you want to create.
    This must be either `ClusterLogSink` or `LogSink`. For information about these sink resources, see [Overview](#overview).  
    * `YOUR-SINK` is a name you choose for your sink.  
    * `YOUR-NAMESPACE` is the name of your namespace.
    Omit this line if you are creating `ClusterLogSink`.  
    * `YOUR-LOG-DESTINATION` is the URL or IP address of your log management service.  

1. (Optional) To filter the logging output, include a `filters` section in your configuration. 
For more information, see [Define a Filter for LogSink and ClusterLogSink Resources](#filter-ls-cls) below.  

1. Save the YAML file with an appropriate filename. For example, `my-cluster-log-sink.yml`.

1. Apply the `ClusterLogSink` or `LogSink` resource to your cluster by running the following command:

    ```
    kubectl apply -f YOUR-SINK.yml
    ```
    Where `YOUR-SINK.yml` is the name of your YAML file.  
<br>
    For example:  
    <pre class='terminal'>
    $ kubectl apply -f my-cluster-log-sink.yml
    </pre>


<br>
### <a id='fluentbit-output-plugin'></a> Create a ClusterLogSink or LogSink Resource with a Fluent Bit Output Plugin

`ClusterLogSink` and `LogSink` resources with a Fluent Bit output plugin deliver logs to the output plugin
that you specify in your resource configuration.

To define a `ClusterLogSink` or `LogSink` resource with a Fluent Bit output plugin,
perform the following steps:

1. Create a YAML file that specifies your log destination in the following format:  

    ```
    apiVersion: pksapi.io/v1beta1
    kind: YOUR-SINK-RESOURCE
    metadata:
      name: YOUR-SINK
      namespace: YOUR-NAMESPACE
    spec:
      type: http
      output_properties:
        Host: example.com
        Format: json
        Port: 443
        tls: on
        tls.verify: off
    ```
    Where:  

    * `YOUR-SINK-RESOURCE` is the sink resource you want to create.
    This must be either `ClusterLogSink` or `LogSink`. For information about these sink resources, see [Overview](#overview).  
    * `YOUR-SINK` is a name you choose for your log sink.  
    * `YOUR-NAMESPACE` is the name of your namespace.
    Omit this line if you are creating `ClusterLogSink`.  

    <p class=note><strong>Note:</strong> This is a sample plugin configuration for <code>http</code>. For a full list of supported plugins, see the <a href="https://docs.fluentbit.io/manual/v/1.3/output">Fluent Bit</a> documentation.</p>

1. (Optional) To filter the logging output, include a `filters` section in your configuration. 
For more information, see [Define a Filter for LogSink and ClusterLogSink Resources](#filter-ls-cls) below.  
1. Save the YAML file with an appropriate filename. For example, `my-cluster-log-sink.yml`.
1. Apply the `ClusterLogSink` or `LogSink` resource to your cluster by running the following command:

    ```
    kubectl apply -f YOUR-SINK.yml
    ```
    Where `YOUR-SINK.yml` is the name of your YAML file.  
<br>
    For example:  
    <pre class='terminal'>
    $ kubectl apply -f my-cluster-log-sink.yml
    </pre>


<br>
### <a id='filter-ls-cls'></a> (Optional) Define a Filter for LogSink and ClusterLogSink Resources

You can set filters on your `LogSink` and `ClusterLogSink` resources:  

* You can include or exclude all logs or all events from sink output. 
For more information, see [Exclude Logs or Events from Sink Output](#filter-log-sinks-evenst) below.  


* If you are using Fluent Bit, you can filter logs using conditions and rules. 
For more information, see [Create a Fluent Bit ClusterLogSink or LogSink Filter](#filter-logs) below.  


<br>
#### <a id='filter-log-sinks-events'></a> Exclude Logs or Events from Sink Output

The `LogSink` and `ClusterLogSink` resources allow users to set filters to
include or exclude all logs or all events from sink output.  

To filter all logs or all events from sink output:  

1. Add a filter properties section to the YAML file that
specifies the sink's log output destination and which types of log entities to include or exclude from the output:  

    ```
    apiVersion: pksapi.io/v1beta1
    kind: YOUR-SINK-RESOURCE
    metadata:
       name: YOUR-SINK
       namespace: YOUR-NAMESPACE
    spec:
       type: syslog
       host: YOUR-LOG-DESTINATION
       port: YOUR-LOG-DESTINATION-PORT
       enable_tls: true
       filters:
        include-events: true
        include-logs: false
    ```
    Where:  

    * `YOUR-SINK-RESOURCE` is the sink resource type that you created. This must be either `ClusterLogSink` or `LogSink`.  
    * `YOUR-SINK` is the name you chose for your sink.  
    * `YOUR-NAMESPACE` is the name of your namespace. Omit this line for `ClusterLogSink` type sink resources.  
    * `YOUR-LOG-DESTINATION` is the URL or IP address of your log management service.  
    * `YOUR-LOG-DESTINATION-PORT` is the port number of your log management service.  

The default values for these filter properties is **true**. 
If you do not specify `filters` properties, both logs and events are included in the sink's output.  

For more information, see
[Monitoring Clusters with Log Sinks](monitor-sinks.html).  

<br>
#### <a id='filter-logs'></a> Fluent Bit ClusterLogSink or LogSink FilterSpecs

As you consider your sink, you might realize that the sink needs a filter more complex than "all logs" or "all events" or even more complex than a single query. 
You might even need to route your log output to more than one destination target.  

If you are using a Fluent Bit ClusterLogSink or LogSink sink resource, 
you can define a filterSpec with condition rules for filtering incoming logs. 
Your Fluent Bit sink resource will selectively direct matching log entries 
to the output destination for the sink. 
The output target for a Fluent Bit sink is the `Output_properties` destination defined for the sink resource.  

For example, you might want only the error log entries of your `Production` Pods routed to a specific service that you are monitoring 
and for the remaining `Production` Pod status log entries to be managed separately from your non-production Pod logs. 
For this scenario, you require the ability to create a complex filtering query and to control the routing destination of filtered log entries.  

**Support for Multiple Filtering Rules**

You can define multiple filtering rules in a filterSpec:  

* Define multiple condition tests in a filterSpec filter:  
    * Define filter conditions that filter based on log content.  
    * Define filter conditions that filter based on the log metadata.
    You can filter log metadata by: Namespace, Host Name, Container Name, Pod Name, and Pod labels.  
   
    
    When you define multiple condition tests in a filterSpec filter, 
    a log entry is considered a match only if all of the defined conditions in the filter are matched.  

* Define multiple filters for a filterSpec:  

    When you define multiple filters in your filterSpec configuration, 
    a log entry is considered a match if one or more of the filters is matched.  
    

**Support for Multiple Target Destinations**

If you have multiple output target destinations, you need to define a separate sink for each destination. 
When you have multiple sinks, log entry output is directed to only one destination, 
no matter how many sinks the log entry is a match for.  

Which destination a log entry is routed to depends on which sinks the entry matches:  

* If one or more of the sinks is configured without a filterSpec, 
all log entries are directed to the output target of the first sink without a filterSpec.  

* If all of the sinks are configured with a filterSpec, 
the log entries are  directed to the output target as follows:  

    <table>
        <tr>
            <th>Log Entry Match Scenario</th>
            <th>Log Entry Target Destination</th>
        </tr>
        <tr>
            <td>Matches a single sink</td>
            <td>The log entry is directed to the output target destination for the sink.</td>
        </tr>
        <tr>
            <td>Matches multiple LogSinks</td>
            <td>The log entry is directed to only the output target destination for the first matched LogSink.</td>
        </tr>
        <tr>
            <td>Matches multiple ClusterLogSinks</td>
            <td>The log entry is directed to only the output target destination for the first matched ClusterLogSink.</td>
        </tr>
        <tr>
            <td>Matches both LogSinks and ClusterLogSinks</td>
            <td>The log entry is directed to only the output target destination for the first matched LogSink.</td>
        </tr>
    </table>


<br>
#### <a id='filter-logs-create'></a> Create a Fluent Bit ClusterLogSink or LogSink FilterSpec

To define a filter for a `ClusterLogSink` or `LogSink` resource, perform the following steps:

1. Configure `include-logs` as `true` in the `filters` section of your Sink resource configuration file.  
1. Add a `filterSpec` section to the `filters` section in your Sink resource configuration file:  

    ```
      filters:
        include-logs: true
        filterSpec:
          - filter:
              - condition: CONDITION-TYPE
                key: KEY
                value: VALUE
        Output_properties:
          output_target
    ```
    
    Where:  

    * `CONDITION-TYPE` is the type of conditional test to apply to the log entries. 
    For more information, see [Chose a Filter Condition Type](#filter-log-conditions) below.   
    * `KEY` is the name of the key to validate. 
    For more information, see [Create a Filter Key](#filter-log-keys) below.  
    * `VALUE` is the value of the key to validate.  

1. Create additional condition sections for each condition you want to apply to your filter.  
1. Create additional filter sections as needed.  


<br>
#### <a id='filter-log-condition-define'></a> Define a Filter Condition

When you define a Fluent Bit Sink resource filter, you must specify the filter's `condition` type, `key`, and `value`:  

* The filter `condition` type is the type of rule to apply to the filter. 
For example, common `condition` types include `key_value_equals` and `key_value_does_not_equal`.  
* The filter `key` indicates which property in the log entry JSON to test against.  
* The filter `value` is the value of the key to test for.  

To build a filter for your filterSpec, see:  

* [Chose a Filter Condition Type](#filter-log-conditions)  
* [Create a Filter Key](#filter-log-keys)  


For example, consider a situation where you wish to include only production Pod log entries in your logs. If the administrator has tagged production Pods as a `production`, the 
output JSON for error log entries might have a format similar to the following:

```
    "log":"[ERROR]counter 2: 2439 Wed Feb 23 06:51:08 UTC 2022",
    "kubernetes":
    {
        "pod_name":"counter-2",
        "namespace_name":"ns-1",
        "pod_id":"05184f35-44ba-45ed-8f75-5016321619ce",
        "labels":{"environment":”production"},
        ...
```
All of the following example filter settings for `condition`, `key`, and `value` match the example JSON-structured log entry above:

* Match against error condition:  

    * condition: `key_value_matches`
    * key: `log`
    * value: `^/[ERROR/]`
    
* Match against pod_id:  

    * condition: `key_value_equals`
    * key: `$kubernetes[‘pod_id’]`
    * value: `05184f35-44ba-45ed-8f75-5016321619ce`
        
* Match against environment = production:  

    * condition: `key_value_equals`
    * key: `$kubernetes[‘labels’][‘environment’]`
    * value: `production`


The resulting filter would be written as:

```
  filters:
    include-logs: true
    filterSpec:
      - filter:
        - condition: key_value_matches
          key: log
          value: ^/[ERROR/]
      - filter:
        - condition: key_value_equals
          key: $kubernetes[‘pod_id’]
          value: 05184f35-44ba-45ed-8f75-5016321619ce
      - filter:
        - condition: key_value_equals
          key: $kubernetes[‘labels’][‘environment’]
          value: production
```


<br>
#### <a id='filter-log-conditions'></a> Chose a Filter Condition Type

The following are the supported filter condition types supported by <%= vars.k8s_runtime_abbr %>:  

<table>
    <tr>
        <th>Condition</th>
        <th>Key</th>
        <th>Value</th>
        <th>Description</th>
    </tr>
    <tr>
        <td><code>key_exists</code></td>
        <td>string:key</td>
        <td>none</td>
        <td><code>true</code> if key exists</td>
    </tr>
    <tr>
        <td><code>key_does_not_exist</code></td>
        <td>string:key</td>
        <td>none</td>
        <td><code>true</code> if key does not exist</td>
    </tr>
    <tr>
        <td><code>a_key_matches</code></td>
        <td>regexp:key</td>
        <td>none</td>
        <td><code>true</code> if a key matches regex</td>
    </tr>
    <tr>
        <td><code>no_key_matches</code></td>
        <td>regexp:key</td>
        <td>none</td>
        <td><code>true</code> if not keys match regex</td>
    </tr>
    <tr>
        <td><code>key_value_equals</code></td>
        <td>string:key</td>
        <td>string:value</td>
        <td><code>true</code> if key exists and its value equals value</td>
    </tr>
    <tr>
        <td><code>key_value_does_not_equal</code></td>
        <td>string:key</td>
        <td>string:value</td>
        <td><code>true</code> if key exists and its value does not equal value</td>
    </tr>
    <tr>
        <td><code>key_value_matches</code></td>
        <td>string:key</td>
        <td>regexp:value</td>
        <td><code>true</code> if key exists and its value matches value</td>
    </tr>
    <tr>
        <td><code>key_value_does_not_match</code></td>
        <td>string:key</td>
        <td>regexp:value</td>
        <td><code>true</code> if key exists and its value does not match value</td>
    </tr>
    <tr>
        <td><code>matching_keys_have_matching_values</code></td>
        <td>regexp:key</td>
        <td>regexp:value</td>
        <td><code>true</code> if all keys matching key have values matching value</td>
    </tr>
    <tr>
        <td><code>matching_keys_do_not_have_matching_values</code></td>
        <td>regexp:key</td>
        <td>regexp:value</td>
        <td><code>true</code> if all keys matching key do not have values matching value</td>
    </tr>
</table>


<br>
#### <a id='filter-log-keys'></a> Create a Filter Key

The filter key follows the pattern:  

```
          key: $ROOTNODE-NAME[OBJECT-NAME][PARAMETER-NAME]`
```

Where:

* `ROOTNODE-NAME` is the name of the root node in the JSON log entry.  
* `OBJECT-NAME` is name of a child node in the root node.  
* `PARAMETER-NAME` is name of a parameter in the child node.  

For an examples of how to create a Filter Key, see [Define a Filter Condition](#filter-log-condition-define) above.  

<br>
### <a name='unsecured-sink-configuration'></a>(Optional) Unsecured ClusterLogSink and LogSink Log Forwarding

By default, <%= vars.k8s_runtime_abbr %> uses a secure connection for log forwarding 
when using `ClusterLogSink` and `LogSink` resources of type `syslog` or `webhook`.

For debugging purposes on a local machine, you may want to temporarily forward logs using an unsecured connection.
To do this, you must:

1. Disable sink forwarding validation by running the following command:

    ```
    kubectl delete validatingwebhookconfigurations validator.pksapi.io
    ```

1. Set `enable_tls` to `false` in your log destination YAML file.  

<p class="note warning"><strong>Warning:</strong> Disabling secure log forwarding is not recommended.</p>

## <a id='define-sinks'></a> Create ClusterMetricSink and MetricSink Resources

ClusterMetricSink and MetricSink resources collect metrics from different sources:  

* `ClusterMetricSink` resources collect metrics from a cluster. 
If you want to collect pod usage metrics, you have to use a ClusterMetricSink.  
* `MetricSink` resources collect metrics from a namespace within a cluster. 
If you want to isolate a certain workload’s metrics to its own output, you have to use a namespaced MetricSink.  


### <a id='define-cluster-metric-sinks'></a> ClusterMetricSink Resources

#### How It Works

By default, a `ClusterMetricSink` resource collects metrics from a cluster using the [Kubernetes Input Plugin]
(https://github.com/influxdata/telegraf/tree/1.13.4/plugins/inputs/kubernetes) and writes them to one or more outputs
that you specify in your `ClusterMetricSink` configuration:  

* ClusterMetricSink DaemonSet collect pod usage metrics using Telegraf agents 
running on every node in a cluster.  
* The ClusterMetricSink is able to fetch pod
metrics using the Kubernetes input plugin, which gives access to pod
usage metrics for each kubernetes node. Pod metrics come from the underlying container runtime, which does not isolate metrics based on namespace. 


Do not use a ClusterMetricSink to isolate a specific workload’s metrics to its own output. A ClusterMetricSink cannot isolate one input from all the others because all of a ClusterMetricSink's configurations are located in a shared ConfigMap. This
means that each ClusterMetricSink's input will also go to all other
ClusterMetricSinks' outputs.  

### <a id='define-metric-sinks'></a> MetricSink Resources  

A `MetricSink` resource runs in a single node only and collects metrics from a namespace within a cluster using
`prometheus.io/scrape` annotations set to `true`.
A MetricSink creates a unique telegraf agent pod and ConfigMap in the namespace, then writes the metrics to one or more outputs that you specify in your `MetricSink` configuration. 

Do not use MetricSinks to collect pod usage metrics. Pod metrics come from the underlying container runtime, which does not isolate 
metrics based on namespace, so only ClusterMetricSink is able to fetch pod 
metrics.

For a list of supported output plugins,
see [Output Plugins](https://github.com/influxdata/telegraf/tree/1.13.4#output-plugins) in the telegraf GitHub repository.

#### When to Use MetricSink vs. ClusterMetricSink

Case 1. Isolating Output => MetricSink

If you want to isolate a certain workload's metrics to its own output, you
have to use a namespaced MetricSink. A MetricSink creates a unique telegraf
agent pod and ConfigMap in the namespace.

ClusterMetricSinks cannot isolate one input from all the others because all
ClusterMetricSinks' configurations are located in a shared ConfigMap. This
means that each ClusterMetricSink's inputs will also go to all other
ClusterMetricSinks' outputs.

Note that MetricSinks do not have access to pod usage metrics, as the
source of those metrics does not filter by namespace and gives usage for all
pods on a node.

Case 2. Pod Usage Metrics (CPU, Memory) => ClusterMetricSink

If you want to get pod usage metrics, you have to use a ClusterMetricSink.

To get pod usage metrics, Telegraf agents need to run on every node in a
cluster, and this is what the ClusterMetricSink DaemonSet does.

Pod metrics come from the underlying container runtime, which does not isolate
metrics based on namespace, so only ClusterMetricSink is able to fetch pod
metrics.

ClusterMetricSinks use the Kubernetes input plugin, which gives access to pod
usage metrics for each kubernetes node.  MetricSinks only run in a single
node, so they do not have access to pod usage metrics.

### <a id='telegraf-plugins'></a> Create a ClusterMetricSink or MetricSink Resource

To define a `ClusterMetricSink` or `MetricSink` resource, perform the following steps:

1. Create a YAML file in the following format:

    ```
    apiVersion: pksapi.io/v1beta1
    kind: YOUR-SINK-RESOURCE
    metadata:
      name: YOUR-SINK
      namespace: YOUR-NAMESPACE
    spec:
      inputs:
      outputs:
      - type: YOUR-OUTPUT-PLUGIN
    telegraf_agent_config:
      interval:  INTERVAL-LENGTH

    ```
    Where:  

    * `YOUR-SINK-RESOURCE` is the sink resource you want to create.
    This must be either `ClusterMetricSink` or `MetricSink`. For information about these sink resources, see [Overview](#overview).  
    * `YOUR-SINK` is a name you choose for your sink.  
    * `YOUR-NAMESPACE` is the name of your namespace.
    Omit this line if you are creating `ClusterMetricSink`.  
    * `YOUR-OUTPUT-PLUGIN` is the name of the output plugin you want to use for your metrics.  
    * (Optional) `INTERVAL-LENGTH` is the interval, in seconds, separating sink collection of input data.
    The assigned `INTERVAL-LENGTH` must be a positive integer less than 9223372037. The default is `10`.  
    <p class="note"><strong>Note:</strong> You can leave the <code>inputs</code> field blank.
    For <code>ClusterMetricSink</code>, this field is configured to include metrics from the kubelet by default. For <code>MetricSink</code>, the field includes all <code>prometheus.io/scrape</code> annotations set to <code>true</code> by default.</p>

    For example:  

    ```
    apiVersion: pksapi.io/v1beta1
    kind: ClusterMetricSink
    metadata:
      name: http
    spec:
      inputs:
      outputs:
      - type: http
        url: https://example.com
        method: POST
        data_format: json
    ```

    This will send all cluster [metrics provided by the kubernetes input plugin](https://github.com/influxdata/telegraf/tree/1.13.4/plugins/inputs/kubernetes#metrics) via json POST to https://example.com


### <a id='node-exporter'></a> Create a ClusterMetricSink Resource for Node Exporter Metrics

To define a `ClusterMetricSink` resource for collecting Node Exporter metrics, perform the following steps:

1. Enable Node Exporter on your cluster workers by selecting the **Enable node exporter on workers**
checkbox in the **<%= vars.product_tile %>** tile > **In-Cluster Monitoring**.
1. Create a YAML file in the following format:  

    ```
    apiVersion: pksapi.io/v1beta1
    kind: ClusterMetricSink
    metadata:
      name: YOUR-SINK
    spec:
      inputs:
      - monitor_kubernetes_pods: true
        type: prometheus
      outputs:
      - type: YOUR-OUTPUT-PLUGIN
    ```
    Where:  

    * `YOUR-SINK` is a name you choose for your sink.  
    * `YOUR-OUTPUT-PLUGIN` is the name of the output plugin you want to use for your metrics.  

    For example:  

    ```
    apiVersion: pksapi.io/v1beta1
    kind: ClusterMetricSink
    metadata:
      name: http
    spec:
      inputs:
      - monitor_kubernetes_pods: true
        type: prometheus
      outputs:
      - type: http
        url: https://example.com
        method: POST
        data_format: json
    ```

1. Save the YAML file with an appropriate filename. For example, `my-cluster-metric-sink.yml`.

1. Apply the `ClusterMetricSink` resource to your cluster by running the following command:

    ```
    kubectl apply -f YOUR-SINK.yml
    ```
    Where `YOUR-SINK.yml` is the name of your YAML file.  
<br>
    For example:  
    <pre class='terminal'>
    $ kubectl apply -f my-cluster-metric-sink.yml
    </pre>


<br>
## <a id='list-sinks'></a> List Sinks

To list sinks for clusters and namespaces, use the commands in the following sections.


<br>
###<a id="log-sinks-list"></a> ClusterLogSink and LogSink Resources

To list cluster log sinks, run the following command:

```
kubectl get clusterlogsinks
```

To list namespace log sinks, run the following command:

```
kubectl -n YOUR-NAMESPACE get logsinks
```

Where `YOUR-NAMESPACE` is the name of your namespace.


<br>
### <a id="metric-sinks-list"></a> ClusterMetricSink and MetricSink Resources

To list cluster metric sinks, run the following command:

```
kubectl get clustermetricsinks
```

To list namespace metric sinks, run the following command:

```
kubectl -n YOUR-NAMESPACE get metricsinks
```

Where `YOUR-NAMESPACE` is the name of your namespace.


<br>
## <a id='delete-sinks'></a> Delete Sinks

To delete sinks for clusters and namespaces, use the commands in the following sections.


<br>
### <a id='log-sinks-delete'></a> ClusterLogSink and LogSink Resources

To delete a cluster log sink, run the following command:  

```
kubectl delete clusterlogsink YOUR-SINK
```

Where `YOUR-SINK` is the name of your sink.  

To delete a namespace log sink, run the following command:  

```
kubectl -n YOUR-NAMESPACE delete logsink YOUR-SINK
```

Where:  

* `YOUR-NAMESPACE` is the name of your namespace.  
* `YOUR-SINK` is the name of your log sink.  


<br>
### <a id="metric-sinks-delete"></a> ClusterMetricSink and MetricSink Resources

To delete a cluster metric sink, use the following command:  

```
kubectl delete clustermetricsink YOUR-SINK
```

Where `YOUR-SINK` is the name of your sink.  

To delete a namespace metric sink, use the following command:  

```
kubectl -n YOUR-NAMESPACE delete metricsink YOUR-SINK
```

Where:  

* `YOUR-NAMESPACE` is the name of your namespace.  
* `YOUR-SINK` is the name of your metric sink.  
