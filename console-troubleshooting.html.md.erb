---
title: Troubleshooting Tanzu Kubernetes Grid Integrated Edition Management Console 
owner: TKGI
---

The following sections describe how to troubleshoot failures to deploy of the VMware <%= vars.product_short %> Management Console and of <%= vars.product_short %> instances from the management console.

For information about how to deploy the management console and install <%= vars.product_short %>, see [Install on vSphere with the Management Console](console-install-vsphere.html).

<hr>

## <a id='ova-fails'></a> Deployment of the <%= vars.product_short %> Management Console Fails

**Problem**

<%= vars.product_short %> Management Console VM fails to deploy from the OVA template.

**Solution**

1. Use SSH to log in to the <%= vars.product_short %> Management Console VM as `root` user.
<br>Use the password that you specified when you  deployed the OVA.
1. Run the following command to obtain the server logs:

    ```
    journalctl -u pks-mgmt-server > server.log
    ```

1. If the logs do not provide the solution, delete the management console VM from vCenter Server and attempt to deploy it again.

<hr>

## <a id='tkgi-deployment-fails'></a>Deployment of <%= vars.product_short %> from the Management Console Fails

**Problem**

<%= vars.product_short %> fails to deploy from the management console.

**Solution**

1. Follow the procedure in [Delete Your Tanzu Kubernetes Grid Integrated Edition Deployment](console-delete-deployment.html) to cleanly remove all <%= vars.product_short %> components from vSphere and to clean up related objects in the management console VM.
1. Attempt to deploy <%= vars.product_short %> again. 

<hr>

## <a id='multi-T0-deployments'></a><%= vars.product_short %> Management Console Cannot Retrieve Cluster Data in a Multi-Tier0 Topology

**Problem**

In a deployment to a multiple-tier0 topology, <%= vars.product_short %> Management Console cannot display cluster information when you go to **TKG Integrated Edition** > **Clusters** and select a cluster. You see errors of the following type:

```
Failed to retrieve current K8s Cluster summary. cannot get cluster details: cannot get cluster namespaces: Get https://<address>:8443/api/v1/namespaces: dial tcp <address>:8443: i/o timeout
Failed to retrieve current K8s Cluster Volumes. cannot get namespaces of cluster 0116663b-f27b-4026-87e3-cddd01af41f2: Get https://<address>:8443/api/v1/namespaces: dial tcp <address>:8443: i/o timeout
```

**Cause**

In a single tier0 topology, <%= vars.product_short %> Management Console is deployed to the same infrastructure network as vSphere and VMware NSX. In a multiple-tier0 topology, due to tenant isolation, the infrastructure network is not routable to tenant tier0 uplink networks. In a multiple-tier0 topology, data from the Kubernetes API is exposed by floating IP addresses on tenant tier0 routers. Consequently, the management console cannot retrieve cluster data from the Kubernetes API because it is not on the same network as the tenants.

**Solution**

Make sure that the <%= vars.product_short %> Management Console can connect to tenant floating IP addresses. 


1. Connect to the management console VM by using `ssh`.
1. Configure a route on the management console VM.
    For example, run the following command:   
    `route add -net <destination_subnet> gw <gateway_address>`
    - **Destination subnet**: The network CIDR of the tenant floating IP addresses.
    - **Gateway**: A VM that can reach the tenant floating IP addresses and the management console.

Because the gateway can reach both the management console and the tenant floating IP addresses, the management console can reach the tenants and retrieve cluster data from the Kubernetes API.

<hr>

## <a id="upgrade-fail-nvds-cvds"></a> <%= vars.k8s_runtime_abbr %> MC is Unable to Upgrade the <%= vars.k8s_runtime_abbr %> Control Plane After Migrating from N-VDS to VDS

If you have an existing <%= vars.k8s_runtime_abbr %> installation on a vSphere N-VDS network and migrate your network to VDS,
<%= vars.k8s_runtime_abbr %> MC will no longer be able to upgrade <%= vars.k8s_runtime_abbr %> to a newer version.  

**Explanation**

<%= vars.k8s_runtime_abbr %> MC uses network resource MOIDs to manage network resources, 
allowing the Management Console to manage network resources with identical names.  

When converting a network from N-VDS to VDS, network resources are assigned new MOIDs.  

Although your existing <%= vars.k8s_runtime_abbr %> Kubernetes cluster workloads continue to function, 
the Management Console configuration maintains stale network resource MOIDs 
and cannot upgrade the <%= vars.k8s_runtime_abbr %> control plane.  


**Workaround**

To upgrade <%= vars.k8s_runtime_abbr %> MC after converting a network from N-VDS to VDS:  

1. Log in to the VM of your existing <%= vars.k8s_runtime_abbr %> MC installation.  
1. To collect your network's current configuration:  
    
    1. Export the `pks-management-server` container's IP as an environment variable:  
    
        ```
        export MGMT_IP=`docker inspect  pks-mgmt-server --format='{{.NetworkSettings.Networks.pks.IPAddress}}'`
        ```
        
    1. Query and save your data center list as a json file named `datacenter.json`:  
    
        ```
        curl -u root  http://{$MGMT_IP}:8080/api/v1/inventory/vcenter/datacenter -X GET  -k  -H \
        "Content-type:application/json" > datacenter.json
        ```
        
    1. Review the exported `datacenter.json` file and note the data center MOID.  
    1. Query and save your network list as a json file named `networks.json`:  
    
        ```
        curl -u root  http://{$MGMT_IP}:8080/api/v1/inventory/vcenter/network?dc=DC-MOID -X GET   -k  -H \
        "Content-type:application/json" > networks.json
        ```
        Where `DC-MOID` is the data center MOID you noted in the previous step.  
        
1. To reconfigure <%= vars.k8s_runtime_abbr %> MC with the current network resources MOIDs, do one of the following:  
    * Configure <%= vars.k8s_runtime_abbr %> MC in the <%= vars.k8s_runtime_abbr %> MC YAML Editor, replacing the old MOIDs in your configuration with the current MOIDs.  
    * Manually configure <%= vars.k8s_runtime_abbr %> MC using a manifest file:  
        1. Manually create a <%= vars.k8s_runtime_abbr %> MC manifest file with the current network resources MOIDs:

            1. Export the <%= vars.k8s_runtime_abbr %> MC manifest as a manifest file named `manifest.json`:  

                ```
                curl -u root:Admin\ADMIN-PASSWORD  https://localhost/api/v1/deployment/manifest -X GET   -k  -H "Content-type:application/json" > manifest.json
                ```
                Where `ADMIN-PASSWORD` is the password for the admin account used in the command.  
                
            1. Back up the exported manifest file.  
            1. Review the `networks.json` file you exported above and note the `moid` and `type` values.  
            1. Open the exported manifest file in an editor and change the `dep_network_moid` and `dep_network_type` values 
               to the `moid` and `type` values you collected from the `networks.json` file.  
            1. Save the revised manifest file.     
        1. To restore your <%= vars.k8s_runtime_abbr %> MC's ability to manage the <%= vars.k8s_runtime_abbr %> network resources:  

            1. Update <%= vars.k8s_runtime_abbr %> MC with the revised manifest:  

                ```
                curl -u root:Admin\ADMIN-PASSWORD  http://{$MGMT_IP}:8080/api/v1/deployment -X POST -d @manifest.json  -k  -H "Content-type:application/json"
                ```
                
                Where `ADMIN-PASSWORD` is the password for the admin account used in the command.  
                
1. To validate your <%= vars.k8s_runtime_abbr %> MC's configuration:  

    1. Open the <%= vars.k8s_runtime_abbr %> MC UI.  
    1. Review **Configuration** > **Deployment** and confirm that all statuses are either `SUCCESS` or `SKIPPED`.  
    1. Review the **<%= vars.k8s_runtime_abbr %> MC Wizard** and confirm the displayed **Network Information** is correct.  
    
1. To upgrade <%= vars.k8s_runtime_abbr %> MC, deploy the <%= vars.k8s_runtime_abbr %> MC OVA for the desired <%= vars.k8s_runtime_abbr %> MC version.

<hr>

## <a id='log-insight'></a>Obtain the vRealize Log Insight Agent ID for <%= vars.product_short %> Management Console 

If you enabled integration with VMware vRealize Log Insight, <%= vars.product_short %> Management Console generates a unique vRealize Log Insight agent ID for the management console VM. You must provide this agent ID to vRealize Log Insight so that it can pull the appropriate logs from the management console.

You obtain the vRealize Log Insight agent ID as follows:

1. Use SSH to log in to the <%= vars.product_short %> Management Console VM as `root` user.
1. Run the following command to obtain the ID:

    ```
    grep LOGINSIGHT_ID /etc/vmware/environment | cut -d= -f2
    ```
	 The resulting ID will be similar to `59debec7-daba-4770-9d21-226ffd743843`.

1. Log in to the vRealize Log Insight Web user interface as administrator and add the agent ID to your list of agents.