---
title: Prerequisites for a Bring Your Own Topology Deployment to NSX-T Data Center
owner: TKGI
---

This topic provides requirements for installing the <%= vars.product_short %> Management Console (<%= vars.k8s_runtime_abbr %> MC) 
in a bring your own topology environment.

##<a id='overview'></a> Overview

A bring your own topology environment is an NSX-T Data Center instance that you have fully configured yourself for use with <%= vars.product_short %>. 
For example, an NSX-T Data Center instance that you have used in a previous deployment of <%= vars.product_short %>.  

The following objects must be in place before you start a production deployment.

- 3 NSX Manager Nodes deployed
- NSX Management Cluster formed
- Virtual IP address assigned for Management Cluster or load balancer

For information about the supported versions of NSX-T Data Center, see the [release notes](./release-notes.html).  

<p class="note"><strong>Note</strong>: In BYOT mode, <%= vars.product_short %> Management Console automatically retrieves the tier0 HA mode from your NSX-T Data Center environment and 
creates NAT rules on the tier 0 or tier 1 router.</p>

## <a id='general-requirements'></a> General Requirements

The following are requirements for deploying the <%= vars.product_short %> Management Console:  

- Do not use the network on which you deploy the <%= vars.product_short %> Management Console VM 
as the network for the management plane when you deploy <%= vars.product_short %>. 
    <p class="note"><strong>Note</strong>: Using the same network for the management console VM 
    and the management plane requires additional NSX-T Data Center configuration and is not recommended.</p>
- A router. The <%= vars.k8s_runtime_abbr %> Management Console supports Tier-0 and Tier-1 routers in either Active-Active or Active-Standby mode.
    <p class="note"><strong>Note</strong>: If you are deploying <%= vars.product_short %> in a multiple-tier0 topology, 
    	additional post-deployment configuration of the management console VM is required. For information, 
    	see <a href="console-troubleshooting.html#multi-T0-deployments"><%= vars.product_short %> Management Console Cannot Retrieve Cluster Data in a Multi-Tier0 Topology</a> 
    	in <em>Troubleshooting the Management Console</em>.</p>

- A logical switch on an NSX-T Virtual Distributed Switch (N-VDS) for use by the <%= vars.k8s_runtime_abbr %> management plane is prepared. The switch must be either under the Tier-0 router, or under the Tier-1 router if the Tier-1 router is directly under the Tier-0 router.
- Edge Cluster with at least 2 NSX-T Data Center Edge Nodes deployed in Active-Standby mode, with connectivity to an uplink network configured.
- Overlay Transport Zone created, with the edge nodes included.
- VLAN Transport Zone created, with the edge nodes included.
- MTU of all transport nodes and physical interfaces configured to 1600 or more.
- If your NSX-T Data Center environment uses custom certificates, obtain the CA certificate for NSX Manager.
    <p class="note"><strong>Note</strong>: If NSX-T Data Center uses custom certificates and you do not provide the CA certificate for NSX Manager, 
    	<%= vars.product_short %> Management Console automatically generates one and registers it with NSX Manager. 
    This can cause other services that are integrated with NSX Manager not to function correctly.</p>

## <a id='nsxt-config-requirements'></a> NSX-T Data Center Configuration Requirements

The following are NSX-T Data Center requirements for deploying the <%= vars.product_short %> Management Console: 

- Virtual IP for the Tier-0 Router configured
- Floating IP Pool configured
- Pod IP Block ID created
- Node IP Block ID created
- Logical Switch configured for <%= vars.k8s_runtime_abbr %> Management Plane
- Tier-1 Router configured and connected to the Tier-0 Router
- Routing for <%= vars.k8s_runtime_abbr %> Floating IPs configured to point to the Tier-0 HA Virtual IP

## <a id='poc-deployments'></a>Proof-of-Concept Deployments

The requirements above are for production environments. In proof-of-concept deployments one NSX Manager node is sufficient. The NSX management cluster and load balancer are also optional for proof-of-concept deployments.