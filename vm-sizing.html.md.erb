---
title: VM Sizing for Enterprise PKS Clusters
owner: PKS
---

This topic describes how <%= vars.product_full %> recommends you approach the sizing of VMs for cluster components.

##<a id="overview"></a>Overview

When you configure plans in the <%= vars.product_tile %> tile, you provide VM sizes for the master and worker node VMs.
For more information about configuring plans, see the Plans section of _Installing <%= vars.product_short %>_ for your IaaS:

* [vSphere](installing-pks-vsphere.html#plans)
* [vSphere with NSX-T Integration](installing-nsx-t.html#plans)
* [Google Cloud Platform (GCP)](installing-pks-gcp.html#plans)
* [Amazon Web Services (AWS)](installing-pks-aws.html#plans)
* [Azure](installing-pks-azure.html#plans)

You select the number of master nodes when you configure the plan.

For worker node VMs, you select the number and size based on the needs of your workload.
The sizing of master and worker node VMs is highly dependent on the characteristics of the workload. Adapt the recommendations in this topic based on your own workload requirements.

##<a id="master-sizing"></a> Master Node VM Size

The master node VM size is linked to the number of worker nodes.
The VM sizing shown in the following table is per master node:

<p class="note"><strong>Note</strong>: If there are multiple master nodes, 
  all master node VMs are the same size. To configure the number of master nodes, 
  see the Plans section of <em>Installing <%= vars.product_short %></em> for your IaaS.
</p> 

To customize the size of the Kubernetes master node VM, 
see [Customize Master and Worker Node VM Size and Type](#node-sizing-custom).

<table border="1" class="nice">
  <thead>
    <tr>
    <th>Number of Workers</th>
    <th>CPU</th>
    <th>RAM (GB)</th>
    </tr>
  </thead>
  </tbody>
    <tr><td>1-5</td><td>1</td><td>3.75</td></tr>
    <tr><td>6-10</td><td>2</td><td>7.5</td></tr>
    <tr><td>11-100</td><td>4</td><td>15</td></tr>
    <tr><td>101-250</td><td>8</td><td>30</td></tr>
    <tr><td>251-500</td><td>16</td><td>60</td></tr>
    <tr><td>500+</td><td>32</td><td>120</td></tr>
  </tbody>
</table>

Do not overload your 
master node VMs by exceeding the recommended maximum number of worker node VMs 
or by downsizing from the recommended VM sizings listed above. 
These recommendations support both a typical workload managed by a VM and 
the higher than usual workload managed by the VM while other VM's in the cluster are upgrading.  

<p class="note warning"><strong>Warning</strong>: 
  Upgrading an overloaded Kubernetes cluster master node VM can result in downtime. 
</p> 

##<a id="worker-sizing"></a> Worker Node VM Number and Size

A maximum of 100 pods can run on a single worker node.
The actual number of pods that each worker node runs depends on the workload type as well as the CPU and memory requirements of the workload.

To calculate the number and size of worker VMs you require, determine the following for your workload:

* Maximum number of pods you expect to run [`p`]
* Memory requirements per pod [`m`]
* CPU requirements per pod [`c`]

Using the values above, you can calculate the following:

* Minimum number of workers [`W`] = `p / 100`
* Minimum RAM per worker = `m * 100`
* Minimum number of CPUs per worker = `c * 100`

This calculation gives you the minimum number of worker nodes your workload requires.
We recommend that you increase this value to account for failures and upgrades.

For example, increase the number of worker nodes by at least one to maintain workload uptime during an upgrade.
Additionally, increase the number of worker nodes to fit your own failure tolerance criteria.

The maximum number of worker nodes that you can create for a plan in an <%= vars.product_short %>-provisioned Kubernetes cluster is set by the **Maximum number of workers on a cluster** field in the **Plans** pane of the <%= vars.product_tile %> tile. To customize the size of the Kubernetes worker node VM, see [Customize Master and Worker Node VM Size and Type](#node-sizing-custom).

###<a id="worker-example"></a> Example Worker Node Requirement Calculation

An example app has the following minimum requirements:

* Number of pods [`p`] = 1000
* RAM per pod [`m`] = 1&nbsp;GB
* CPU per pod [`c`] = 0.10

To determine how many worker node VMs the app requires, do the following:

1. Calculate the number of workers using `p / 100`:
  <pre>1000/100 = 10 workers</pre>
1. Calculate the minimum RAM per worker using `m * 100`:
  <pre>1 * 100 = 100 GB</pre>
1. Calculate the minimum number of CPUs per worker using `c * 100`:
  <pre>0.10 * 100 = 10 CPUs</pre>
1. For upgrades, increase the number of workers by one:
  <pre>10 workers + 1 worker = 11 workers</pre>
1. For failure tolerance, increase the number of workers by two:
  <pre>11 workers + 2 workers = 13 workers</pre>

In total, this app workload requires 13 workers with 10 CPUs and 100&nbsp;GB RAM.

##<a id="node-sizing-custom"></a> Customize Master and Worker Node VM Size and Type

You select the CPU, memory, and disk space for the Kubernetes node VMs from
a set list in the <%= vars.product_tile %> tile. Master and worker node VM sizes and types are selected on a per-plan
basis. For more information, see the Plans section of the <%= vars.product_short %> installation topic
for your IaaS. For example, [Installing <%= vars.product_short %> on vSphere with NSX-T](./installing-nsx-t.html#plans).

While the list of available node VM types and sizes is extensive, the list may
not provide the exact type and size of VM that you want. You can use the Ops Manager
API to customize the size and types of the master and worker node VMs. For more information, see
[How to Create or Remove Custom VM_TYPE Template using the Operations Manager API]
(https://community.pivotal.io/s/article/how-to-create-or-remove-custom-vmtype-template-using-the-ops-manager-api) in the Knowledge Base.

<p class="note warning"><strong>Warning</strong>: Do not reduce the size of your 
Kubernetes master node VMs below the recommended sizes 
listed in <a href="#master-sizing">Master Node VM Size</a>, above.
  Upgrading an overloaded Kubernetes cluster master node VM can result in downtime. 
</p> 
