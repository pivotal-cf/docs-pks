---
title: About Tanzu Kubernetes Grid Integrated Edition Upgrades
owner: TKGI
---

This topic provides conceptual information about upgrading <%= vars.product_short %> (<%= vars.k8s_runtime_abbr %>),
including upgrading the <%= vars.k8s_runtime_abbr %> control plane and <%= vars.k8s_runtime_abbr %>-provisioned Kubernetes clusters.

For step-by-step instructions on upgrading <%= vars.product_short %>
and <%= vars.k8s_runtime_abbr %>-provisioned Kubernetes clusters, see:

* [Upgrading <%= vars.product_short %> (Antrea and Flannel Networking)](upgrade.html)
* [Upgrading <%= vars.product_short %> (NSX-T Networking)](upgrade-nsxt.html)
* [Upgrading Clusters](upgrade-clusters.html)

<br>
<br>
## <a id="overview"></a>Overview

An <%= vars.product_short %> upgrade modifies the <%= vars.k8s_runtime_abbr %> version,
for example, upgrading <%= vars.k8s_runtime_abbr %> from <%= vars.product_version_prev %>.x to <%= vars.product_version %>.0 or from <%= vars.product_version %>.0 to <%= vars.product_version %>.1.

There are two ways you can upgrade <%= vars.k8s_runtime_abbr %>:

* **Full Upgrade**: By default, <%= vars.k8s_runtime_abbr %> is set to perform a full upgrade,
which upgrades both the <%= vars.k8s_runtime_abbr %> control plane and all <%= vars.k8s_runtime_abbr %>-provisioned Kubernetes clusters.

* **Control Plane Only Upgrade**: You can choose to upgrade <%= vars.k8s_runtime_abbr %> in two phases
by upgrading the <%= vars.k8s_runtime_abbr %> control plane first and
then upgrading your <%= vars.k8s_runtime_abbr %>-provisioned Kubernetes clusters later.

<br>
### <a id="decide"></a> Deciding Between Full and Two-Phase Upgrade

When deciding whether to perform the default full upgrade or
to upgrade the <%= vars.k8s_runtime_abbr %> control plane and <%= vars.k8s_runtime_abbr %>-provisioned Kubernetes clusters separately,
consider your organization's needs.

You might prefer to upgrade <%= vars.k8s_runtime_abbr %> in two phases because of the advantages it provides: 

* If your organization runs <%= vars.k8s_runtime_abbr %>-provisioned Kubernetes clusters
in both development and production environments and you want to upgrade
only one environment first, you can achieve your goal by
upgrading the <%= vars.k8s_runtime_abbr %> control plane and <%= vars.k8s_runtime_abbr %>-provisioned Kubernetes separately.

* Faster <%= vars.product_tile %> tile upgrades.
If you have a large number of clusters in your <%= vars.k8s_runtime_abbr %> deployment,
performing a full upgrade can significantly increase the amount of time required to
upgrade the <%= vars.product_tile %> tile.

* More granular control over cluster upgrades.
In addition to enabling you to upgrade subsets of clusters,
the <%= vars.k8s_runtime_abbr %> CLI supports upgrading each cluster individually.

* Not a monolithic upgrade.
This helps isolate the root cause of an error when troubleshooting upgrades.
For example, when a cluster-related upgrade error occurs during a full upgrade,
the entire <%= vars.product_tile %> tile upgrade might fail.

<%# Note: The formatting on this page breaks when notes are configured the normal way.%>
<p class="note warning">
<strong>Warning:</strong> If you deactivate the default full upgrade
and upgrade only the <%= vars.k8s_runtime_abbr %> control plane,
you must upgrade all your <%= vars.k8s_runtime_abbr %>-provisioned Kubernetes clusters before the next <%= vars.product_tile %> tile
upgrade. Deactivating the default full upgrade
and upgrading only the <%= vars.k8s_runtime_abbr %> control plane cause the <%= vars.k8s_runtime_abbr %> version
tagged in your Kubernetes clusters to fall behind the <%= vars.product_tile %> tile version.
If your <%= vars.k8s_runtime_abbr %>-provisioned Kubernetes clusters fall more than one version behind the tile,
<%= vars.k8s_runtime_abbr %> cannot upgrade the clusters.
</p>
<%# Note: The formatting on this page breaks when notes are configured the normal way.%>

<br>
### <a id="decide-method"></a> Deciding Between Tile or CLI Upgrade

You can use either the <%= vars.product_tile %> tile or the <%= vars.k8s_runtime_abbr %> CLI to perform <%= vars.k8s_runtime_abbr %> upgrades:  

* To perform a full upgrade of the <%= vars.k8s_runtime_abbr %> control plane and
<%= vars.k8s_runtime_abbr %>-provisioned Kubernetes clusters, use the <%= vars.product_tile %> tile . 
* To upgrade the <%= vars.k8s_runtime_abbr %> control plane only, use the <%= vars.product_tile %> tile. 
* To upgrade <%= vars.k8s_runtime_abbr %>-provisioned Kubernetes clusters, use either the <%= vars.k8s_runtime_abbr %> CLI 
or the <%= vars.product_tile %> tile.

<table>
<col width="20%">
<col width="20%">
<col width="27%">
<col width="27%">
  <tr>
    <th rowspan=2>Upgrade Method</th>
    <th colspan=3 style="text-align:center">Supported Upgrade Types</th>
  </tr>
  <tr>
    <th>Full <%= vars.k8s_runtime_abbr %> upgrade</th>
    <th><%= vars.k8s_runtime_abbr %> control plane only</th>
    <th>Kubernetes clusters only</th>
  </tr>
  <tr>
    <td><%= vars.k8s_runtime_abbr %> Tile</td>
    <td style="text-align:center">&#10004;</td>
    <td style="text-align:center">&#10004;</td>
    <td style="text-align:center">&#10004;</td>
  </tr>
  <tr>
    <td><%= vars.k8s_runtime_abbr %> CLI</td>
    <td style="text-align:center">&#10006;</td>
    <td style="text-align:center">&#10006;</td>
    <td style="text-align:center">&#10004;</td>
  </tr>
</table>

Typically, if you choose to upgrade <%= vars.k8s_runtime_abbr %>-provisioned Kubernetes clusters only,
you will upgrade them through the <%= vars.k8s_runtime_abbr %> CLI.

<br>
<br>
## <a id="what-happens"></a> What Happens During Full <%= vars.k8s_runtime_abbr %> and <%= vars.k8s_runtime_abbr %> Control Plane Only Upgrades

After you add a new <%= vars.product_tile %> tile version to your staging area
on the Ops Manager Installation Dashboard,
Ops Manager automatically migrates your configuration settings into the new tile version.  

You can perform a full <%= vars.k8s_runtime_abbr %> upgrade or a <%= vars.k8s_runtime_abbr %> control plane only upgrade:  

* [Full <%= vars.k8s_runtime_abbr %> Upgrades](#full-upgrades)
* [<%= vars.k8s_runtime_abbr %> Control Plane Only Upgrades](#control-plane-upgrades)


### <a name="full-upgrades"></a>Full <%= vars.k8s_runtime_abbr %> Upgrades

During a **full <%= vars.k8s_runtime_abbr %> upgrade**,
the <%= vars.product_tile %> tile does the following:

1. **Recreates the Control Plane VMs**:  
    * Upgrades the <%= vars.k8s_runtime_abbr %> version on the <%= vars.k8s_runtime_abbr %> control plane.  
    * For more information, see [What Happens During Control Plane Upgrades](#control-plane-upgrades-details) below.  

1. **Upgrades Clusters**: 
    * Upgrades all of the <%= vars.k8s_runtime_abbr %>-provisioned Kubernetes clusters.  
    * Requires the **Upgrade all clusters errand** check box is activated in the **Errands** pane on the <%= vars.product_tile %> tile.  
    * For more information, see [What Happens During Cluster Upgrades](#cluster-upgrades) below.  
    
### <a name="control-plane-upgrades"></a><%= vars.k8s_runtime_abbr %> Control Plane Only Upgrades

During a **<%= vars.k8s_runtime_abbr %> control plane only** upgrade, 
the <%= vars.product_tile %> tile does the following:  

1. **Recreates the Control Plane VMs**:  
    * Upgrades the <%= vars.k8s_runtime_abbr %> version on the <%= vars.k8s_runtime_abbr %> control plane.  
    * For more information, see [What Happens During Control Plane Upgrades](#control-plane-upgrades-details) below.  

1. **Does Not Upgrade Clusters**:  
    * Does not automatically upgrade <%= vars.k8s_runtime_abbr %>-provisioned Kubernetes clusters after upgrading the <%= vars.k8s_runtime_abbr %> control plane.  
    * Requires the **Upgrade all clusters errand** check box is deactivated in the **Errands** pane on the <%= vars.product_tile %> tile.  
    * The <%= vars.k8s_runtime_abbr %>-provisioned Kubernetes clusters remain on the previous <%= vars.k8s_runtime_abbr %> version until you manually upgrade them.
    For more information, see [What Happens During Cluster Upgrades](#cluster-upgrades) below, and [Upgrading Clusters](upgrade-clusters.html).  
    * Some cluster management tasks are not supported for clusters that are running the previous <%= vars.k8s_runtime_abbr %> version. 
    For more information, see [Tasks Supported Following a <%= vars.k8s_runtime_abbr %> Control Plane Only Upgrade](#control-plane-upgrades-supported-tasks) below.  



<br>
<br>
## <a id="control-plane-upgrades-details"></a>What Happens During Control Plane Upgrades

Note the following when upgrading the <%= vars.k8s_runtime_abbr %> control plane:  

* Upgrading the <%= vars.k8s_runtime_abbr %> control plane includes upgrading the 
<%= vars.control_plane %> server, UAA server, and the <%= vars.k8s_runtime_abbr %> database.  
* If the <%= vars.k8s_runtime_abbr %> installation is not scaled for high availability (beta), the control plane upgrade causes temporary outages as described in [Control Plane Outages](#outages) below.  
* The control plane upgrade will halt if a control plane canary instance encounters an error. 
For more information, see [Canary Instances](#canary) below.  

<br>
### <a name="outages"></a> Control Plane Outages

When the <%= vars.k8s_runtime_abbr %> control plane is not scaled for high availability (beta), upgrading the control plane temporarily interrupts the following:  

* Logging in to the <%= vars.k8s_runtime_abbr %> CLI and using all `tkgi` commands.  
* Using the <%= vars.control_plane %> to retrieve information about clusters.  
* Using the <%= vars.control_plane %> to create and delete clusters.  
* Using the <%= vars.control_plane %> to resize clusters.  

These outages do not affect the Kubernetes clusters themselves.
During a <%= vars.k8s_runtime_abbr %> control plane upgrade, you can still interact with clusters and their workloads using the Kubernetes Command Line Interface, `kubectl`.

For more information about the <%= vars.k8s_runtime_abbr %> control plane and high availability (beta), see [<%= vars.k8s_runtime_abbr %> Control Plane Overview](control-plane.html#control-plane) in _<%= vars.product_short %> Architecture_.

<br>
### <a name="canary"></a>Canary Instances

The <%= vars.product_tile %> tile is a BOSH deployment.

BOSH-deployed products can set a number of canary instances to upgrade first, before the rest of the deployment VMs.
BOSH continues the upgrade only if the canary instance upgrade succeeds.
If the canary instance encounters an error, the upgrade stops running and other VMs are not affected.

The <%= vars.product_tile %> tile uses one canary instance when deploying or upgrading <%= vars.product_short %>.  

<br>
### <a name="control-plane-upgrades-supported-tasks"></a>Tasks Supported Following a <%= vars.k8s_runtime_abbr %> Control Plane Only Upgrade

<%= vars.k8s_runtime_abbr %> supports running <%= vars.k8s_runtime_abbr %> CLI commands on <%= vars.k8s_runtime_abbr %>-provisioned Kubernetes clusters. The <%= vars.k8s_runtime_abbr %> CLI supports clusters running the <%= vars.k8s_runtime_abbr %> control plane's <%= vars.k8s_runtime_abbr %> version or the previous <%= vars.k8s_runtime_abbr %> version.  

Not all <%= vars.k8s_runtime_abbr %> CLI commands can be supported on clusters running the previous <%= vars.k8s_runtime_abbr %> version. The following tables summarize which  <%= vars.k8s_runtime_abbr %> CLI commands support the previous <%= vars.k8s_runtime_abbr %> version:

* [<%= vars.k8s_runtime_abbr %> CLI Utility Tasks](#control-plane-upgrades-supported-tasks-utility)  
* [<%= vars.k8s_runtime_abbr %> CLI Cluster Management Tasks](#control-plane-upgrades-supported-tasks-management)  

<br>
#### <a name="control-plane-upgrades-supported-tasks-utility"></a><%= vars.k8s_runtime_abbr %> CLI Utility Tasks

The following table summarizes the <%= vars.k8s_runtime_abbr %> CLI utility commands that support clusters running the previous <%= vars.k8s_runtime_abbr %> version.  

<table style="width:100%">
  <tr>
    <th width="20%">Task&nbsp;Status</th>
    <th width="50%"></th>
    <th width="30%">Notes</th>
  </tr>
  <tr>
    <th>Supported Tasks</th> 
    <td>
      <ul>
        <li><code>tkgi cancel-task</code></li>
        <li><code>tkgi clusters</code></li>
        <li><code>tkgi compute-profile</code></li>
        <li><code>tkgi compute-profiles</code></li>
        <li><code>tkgi create-compute-profile</code></li>
        <li><code>tkgi create-kubernetes-profile</code></li>
        <li><code>tkgi create-network-profile</code></li>
        <li><code>tkgi delete-compute-profile</code></li>
        <li><code>tkgi delete-kubernetes-profile</code></li>
        <li><code>tkgi delete-network-profile</code></li>
        <li><code>tkgi kubernetes-profile</code></li>
        <li><code>tkgi kubernetes-profiles</code></li>
        <li><code>tkgi login</code></li>
        <li><code>tkgi logout</code></li>
        <li><code>tkgi network-profile</code></li>
        <li><code>tkgi network-profiles</code></li>
        <li><code>tkgi plans</code></li>
        <li><code>tkgi task</code></li>
        <li><code>tkgi tasks</code></li>
      </ul>
    </td>
    <td>
    </td>
  </tr>
</table>

<br>
#### <a name="control-plane-upgrades-supported-tasks-management"></a><%= vars.k8s_runtime_abbr %> CLI Cluster Management Tasks

The following table summarizes the <%= vars.k8s_runtime_abbr %> CLI cluster management commands that support clusters running the previous <%= vars.k8s_runtime_abbr %> version.  

<p class="note"><strong>Note</strong>: <%= vars.recommended_by %> recommends against performing cluster management tasks on clusters running the previous <%= vars.k8s_runtime_abbr %> version.
</p>

<table style="width:100%">
  <tr>
    <th width="20%">Task&nbsp;Status</th>
    <th width="50%"></th>
    <th width="30%">Notes</th>
  </tr>
  <tr>
    <th>Supported Tasks</th> 
    <td>
      <ul>
        <li><code>tkgi certificates</code></li>
        <li><code>tkgi cluster</code></li>
        <li><code>tkgi create-cluster</code></li>
        <li><code>tkgi delete-cluster</code></li>
        <li><code>tkgi get-credentials</code></li>
        <li><code>tkgi get-kubeconfig</code></li>
        <li><code>tkgi upgrade-cluster</code></li>
        <li><code>tkgi upgrade-clusters</code></li>
      </ul>
    </td>
    <td>
    </td>
  </tr>
  <tr>
    <th>Partially-Supported Tasks</th> 
    <td>
      <ul>
        <li><code>tkgi update-cluster</code><br>For more information, see the <strong>Notes</strong> column.</li>  
      </ul>
    </td>
    <td>
      Supported <code>tkgi update-cluster</code> flags:
      <ul>
        <li><code>--num-nodes INT32</code> &#42;</li>
        <li><code>--node-pool-instances</code> &#42;</li>
      </ul>
      <br><br><strong>Unsupported</strong> <code>tkgi&nbsp;update-cluster</code> <strong>Flags</strong>:  
      <ul>
        <li><code>--compute-profile</code></li>
        <li><code>--kubelet-drain-timeout</code></li>
        <li><code>--kubelet-drain-grace-period</code></li>
        <li><code>--kubelet-drain-force</code></li>
        <li><code>--kubelet-drain-ignore-daemonsets</code></li>
        <li><code>--kubelet-drain-delete-local-data</code></li>
        <li><code>--kubelet-drain-force-node</code></li>
        <li><code>--kubernetes-profile</code></li>
        <li><code>--network-profile</code></li>
        <li><code>--tags []ClusterTag</code> &#42;</li>
        <li><code>--config-file</code></li>
      </ul>
      <br>&#42; Clusters running the previous <%= vars.k8s_runtime_abbr %> version 
      and configured with <code>&#8209;&#8209;tags</code> do not support any <code>tkgi&nbsp;update-cluster</code> operations.  
    </td>
  </tr>
  <tr>
    <th>Unsupported Tasks</th> 
    <td>
      <ul>
        <li><code>tkgi rotate-certificates</code></li>  
      </ul>
    </td>
    <td>
    </td>
  </tr>
</table>

<br>
<br>
## <a id="cluster-upgrades"></a>What Happens During Cluster Upgrades

Upgrading a <%= vars.k8s_runtime_abbr %>-provisioned Kubernetes cluster upgrades the cluster to the <%= vars.k8s_runtime_abbr %> version of the <%= vars.k8s_runtime_abbr %> control plane and tags the cluster with the upgrade version.  

Upgrading the cluster also upgrades the cluster's Kubernetes version to the version
included with the <%= vars.product_tile %> tile.  

During an upgrade of <%= vars.k8s_runtime_abbr %>-provisioned clusters,
<%= vars.k8s_runtime_abbr %> recreates your clusters.
This includes the following stages for each cluster you upgrade:

1. Control Plane nodes are recreated.
1. Worker nodes are recreated.

Depending on your cluster configuration,
these recreations might cause [Cluster Control Plane Nodes Outage](#master) or [Worker Nodes Outage](#worker)
as described below.  

<%# Note: The formatting on this page breaks when notes are configured the normal way.%>
<p class="note">
<strong>Note:</strong> When the <strong>Upgrade all clusters errand</strong>
is enabled in the <%= vars.product_tile %> tile, updating the tile with
a new Linux or Windows stemcell rolls every Linux or Windows VM in each Kubernetes cluster.
This automatic rolling ensures that all your VMs are patched.
To avoid workload downtime, use the resource configuration recommended
in <a href="#master">Control Plane Nodes Outage</a> 
and <a href="#worker">Worker Nodes Outage</a> below and
in <a href="./maintain-uptime.html">Maintaining Workload Uptime</a>.
</p>
<%# Note: The formatting on this page breaks when notes are configured the normal way.%>  

You can upgrade <%= vars.k8s_runtime_abbr %>-provisioned Kubernetes clusters either through the <%= vars.product_tile %> tile 
or the <%= vars.k8s_runtime_abbr %> CLI. See the table below.

<table>
<col width="50%">
<col width="50%">
  <tr>
    <th>This method</th>
    <th>Upgrades</th>
  </tr>
  <tr>
    <td>The <strong>Upgrade all clusters errand</strong> in<br>
    the <strong><%= vars.product_tile %></strong> tile > <strong>Errands</strong></td>
    <td>All clusters. Clusters are upgraded serially.</td>
  </tr>
  <tr>
    <td><code>tkgi upgrade-cluster</code></td>
    <td>One cluster.</td>
  </tr>
  <tr>
    <td><code>tkgi upgrade-clusters</code></td>
    <td>Multiple clusters. Clusters are upgraded serially or in parallel.</td>
  </tr>
</table>



<br>
###<a name="master"></a>Cluster Control Plane Nodes Outage

When <%= vars.k8s_runtime_abbr %> upgrades a single-control plane node cluster,
you cannot interact with your cluster, use `kubectl`, or push new workloads.

To avoid this loss of functionality,
<%= vars.recommended_by %> recommends using multi-control plane node clusters.

###<a name="worker"></a>Worker Nodes Outage

When <%= vars.k8s_runtime_abbr %> upgrades a worker node,
the node stops running containers.
If your workloads run on a single node, they will experience downtime.

To avoid downtime for stateless workloads,
<%= vars.recommended_by %> recommends using at least one worker node
per availability zone (AZ).
For stateful workloads, <%= vars.recommended_by %> recommends
using a minimum of two worker nodes per AZ.  

<br>
<br>
##<a id="cni"></a> About Switching from the Flannel CNI to the Antrea CNI

<%= vars.product_short %> supports Antrea, Flannel, and NSX-T 
as the Container Network Interfaces (CNIs) for <%= vars.k8s_runtime_abbr %>-provisioned clusters.  

<%= vars.recommended_by %> recommends the Antrea CNI over Flannel. 
The Antrea CNI provides Kubernetes Network Policy support for non-NSX-T environments. 
Antrea CNI-configured clusters are supported on AWS, Azure, and vSphere without NSX-T environments.  

For more information about Antrea, see [Antrea](https://antrea.io/) in the Antrea documentation.  

<p class="note">
<strong>Note:</strong> Support for the Flannel Container Networking Interface (CNI) is deprecated.
</p>

<%= vars.recommended_by %> recommends that you configure Antrea as the default <%= vars.k8s_runtime_abbr %>-provisioned cluster CNI, 
and that you switch your Flannel CNI-configured clusters to the Antrea CNI.  



<br>
###<a id="upgrade-the-cni"></a> Switch from the Flannel CNI to Antrea

You can configure <%= vars.k8s_runtime_abbr %> to network newly created <%= vars.k8s_runtime_abbr %>-provisioned clusters 
with the Antrea CNI.  

Configure the <%= vars.k8s_runtime_abbr %> default CNI during <%= vars.k8s_runtime_abbr %> installation and upgrade only.  

During <%= vars.k8s_runtime_abbr %> installation:  

* Configure the <%= vars.k8s_runtime_abbr %> default CNI as either Antrea or vSphere with NSX-T.  

During <%= vars.k8s_runtime_abbr %> upgrades:  

* You can optionally change the <%= vars.k8s_runtime_abbr %> default CNI from the deprecated Flannel CNI to Antrea.  
* Do not change the CNI configuration from Antrea to Flannel.  
* Do not change the CNI configuration from or to NSX-T after your initial <%= vars.k8s_runtime_abbr %> installation.  


If you initially configured <%= vars.k8s_runtime_abbr %> to use Flannel as the default CNI and 
switch to Antrea as the default CNI during a <%= vars.k8s_runtime_abbr %> upgrade:  

* Existing Flannel-configured clusters remain networked using Flannel. 
    Your existing Flannel clusters will not be migrated to Antrea.  
* New clusters created after the upgrade are created using Antrea as their CNI.  

<p class="note warning">
<strong>Warning:</strong>  
Do not change the <%= vars.k8s_runtime_abbr %> default CNI configuration between upgrades.  
</p>

<br>
For information about selecting and configuring a CNI for <%= vars.k8s_runtime_abbr %>, see the _Networking_ section of the installation documentation for your environment:  

* [Installing <%= vars.product_full %> on AWS](installing-aws.html#networking)  
* [Installing <%= vars.product_full %> on Azure](installing-azure.html#networking)  
* [Installing <%= vars.product_full %> on vSphere](installing-vsphere.html#networking)  

<%# Note: The formatting on this page breaks when notes are configured the normal way.%>