---
title: About Tanzu Kubernetes Grid Integrated Edition Upgrades
owner: TKGI
---

This topic provides conceptual information about <%= vars.product_short %> upgrades,
including upgrading the <%= vars.k8s_runtime_abbr %> control plane and <%= vars.k8s_runtime_abbr %>-provisioned Kubernetes clusters.

For step-by-step instructions on upgrading <%= vars.product_short %>
and <%= vars.k8s_runtime_abbr %>-provisioned Kubernetes clusters, see:

* [Upgrading <%= vars.product_short %> (Antrea and Flannel Networking)](upgrade.html)
* [Upgrading <%= vars.product_short %> (NSX-T Networking)](upgrade-nsxt.html)
* [Upgrading Clusters](upgrade-clusters.html)

## <a id="overview"></a>Overview

An <%= vars.product_short %> upgrade modifies the version of <%= vars.product_short %>,
for example, from v1.11.x to v1.12.0 or from v1.12.0 to v1.12.1.

By default, <%= vars.product_short %> is set to perform a full upgrade,
which upgrades both the <%= vars.k8s_runtime_abbr %> control plane and all <%= vars.k8s_runtime_abbr %>-provisioned Kubernetes clusters.

However, you can choose to upgrade <%= vars.product_short %> in two phases
by upgrading the <%= vars.k8s_runtime_abbr %> control plane first and
then upgrading your <%= vars.k8s_runtime_abbr %>-provisioned Kubernetes clusters later.

You can use either the <%= vars.product_tile %> tile or the <%= vars.k8s_runtime_abbr %> CLI to perform <%= vars.k8s_runtime_abbr %> upgrades:  

* To perform a full upgrade of the <%= vars.k8s_runtime_abbr %> control plane and
<%= vars.k8s_runtime_abbr %>-provisioned Kubernetes clusters, use the <%= vars.product_tile %> tile . 
* To upgrade the <%= vars.k8s_runtime_abbr %> control plane only, use the <%= vars.product_tile %> tile. 
* To upgrade <%= vars.k8s_runtime_abbr %>-provisioned Kubernetes clusters, use either the <%= vars.k8s_runtime_abbr %> CLI 
or the <%= vars.product_tile %> tile. 

<table>
<col width="20%">
<col width="20%">
<col width="27%">
<col width="27%">
  <tr>
    <th rowspan=2>Upgrade Method</th>
    <th colspan=3 style="text-align:center">Supported Upgrade Types</th>
  </tr>
  <tr>
    <th>Full <%= vars.k8s_runtime_abbr %> upgrade</th>
    <th><%= vars.k8s_runtime_abbr %> control plane only</th>
    <th>Kubernetes clusters only</th>
  </tr>
  <tr>
    <td><%= vars.k8s_runtime_abbr %> Tile</td>
    <td style="text-align:center">&#10004;</td>
    <td style="text-align:center">&#10004;</td>
    <td style="text-align:center">&#10004;</td>
  </tr>
  <tr>
    <td><%= vars.k8s_runtime_abbr %> CLI</td>
    <td style="text-align:center">&#10006;</td>
    <td style="text-align:center">&#10006;</td>
    <td style="text-align:center">&#10004;</td>
  </tr>
</table>

Typically, if you choose to upgrade <%= vars.k8s_runtime_abbr %>-provisioned Kubernetes clusters only,
you will upgrade them through the <%= vars.k8s_runtime_abbr %> CLI.

## <a id="decide"></a> Deciding Between Full and Two-Phase Upgrade

When deciding whether to perform the default full upgrade or
to upgrade the <%= vars.k8s_runtime_abbr %> control plane and <%= vars.k8s_runtime_abbr %>-provisioned Kubernetes clusters separately,
consider your organization needs.

For example, if your organization runs <%= vars.k8s_runtime_abbr %>-provisioned Kubernetes clusters
in both development and production environments and you want to upgrade
only one environment first, you can achieve your goal by
upgrading the <%= vars.k8s_runtime_abbr %> control plane and <%= vars.k8s_runtime_abbr %>-provisioned Kubernetes separately
instead of performing a full upgrade.

Examples of other advantages of
upgrading <%= vars.product_short %> in two phases include:

* Faster <%= vars.product_tile %> tile upgrades.
If you have a large number of clusters in your <%= vars.product_short %> deployment,
performing a full upgrade can significantly increase the amount of time required to
upgrade the <%= vars.product_tile %> tile.

* More granular control over cluster upgrades.
In addition to enabling you to upgrade subsets of clusters,
the <%= vars.k8s_runtime_abbr %> CLI supports upgrading each cluster individually.

* Not a monolithic upgrade.
This helps isolate the root cause of an error when troubleshooting upgrades.
For example, when a cluster-related upgrade error occurs during a full upgrade,
the entire <%= vars.product_tile %> tile upgrade might fail.

<%# Note: The formatting on this page breaks when notes are configured the normal way.%>
<p class="note warning">
<strong>Warning:</strong> If you deactivate the default full upgrade
and upgrade only the <%= vars.k8s_runtime_abbr %> control plane,
you must upgrade all your <%= vars.k8s_runtime_abbr %>-provisioned Kubernetes clusters before the next <%= vars.product_tile %> tile
upgrade. Deactivating the default full upgrade
and upgrading only the <%= vars.k8s_runtime_abbr %> control plane cause the <%= vars.k8s_runtime_abbr %> version
tagged in your Kubernetes clusters to fall behind the <%= vars.product_tile %> tile version.
If your <%= vars.k8s_runtime_abbr %>-provisioned Kubernetes clusters fall more than one version behind the tile,
<%= vars.product_short %> cannot upgrade the clusters.
</p>
<%# Note: The formatting on this page breaks when notes are configured the normal way.%>

## <a id="what-happens"></a> What Happens During Full <%= vars.k8s_runtime_abbr %> and <%= vars.k8s_runtime_abbr %> Control Plane Upgrades

During a <%= vars.product_short %> control plane upgrade to v1.12, the <%= vars.product_tile %> tile does the following:

1. **Recreates the Control Plane VMs** which host the TKGI API and UAA servers.
  * If the <%= vars.product_short %> installation is not scaled for high availability (beta), this control plane recreation causes temporary outages as described in [Non-HA Control Plane Outages](#outages), below.

1. **(Optional) Upgrades Clusters**
  * Upgrading <%= vars.product_short %> only upgrades its Kubernetes clusters if the **Upgrade all clusters errand** check box is enabled in the **Errands** pane.
  * The cluster upgrade process recreates all clusters, which might cause cluster outages.  
  For more information, see the [What Happens During Cluster Upgrades](./upgrade-clusters.html#what) section of the _Upgrading Clusters_ topic.

You can perform full <%= vars.k8s_runtime_abbr %> upgrades and <%= vars.k8s_runtime_abbr %> control plane upgrades only
through the <%= vars.product_tile %> tile.

After you add a new <%= vars.product_tile %> tile version to your staging area
on the Ops Manager Installation Dashboard,
Ops Manager automatically migrates your configuration settings into the new tile version.

For more information, see:

* [Full <%= vars.k8s_runtime_abbr %> Upgrades](#full-upgrades)
* [<%= vars.k8s_runtime_abbr %> Control Plane Upgrades](#control-plane-upgrades)

### <a name="full-upgrades"></a>Full <%= vars.k8s_runtime_abbr %> Upgrades

During a **full <%= vars.k8s_runtime_abbr %> upgrade**,
the <%= vars.product_tile %> tile does the following:

1. Upgrades the <%= vars.k8s_runtime_abbr %> control plane, which includes the
<%= vars.control_plane %> and UAA servers and the <%= vars.k8s_runtime_abbr %>
database.
This control plane upgrade causes temporary outages as described in
[Control Plane Outages](#outages) below.

1. Upgrades <%= vars.k8s_runtime_abbr %>-provisioned Kubernetes clusters.
  * Upgrading <%= vars.k8s_runtime_abbr %>-provisioned Kubernetes clusters is controlled by the **Upgrade all clusters errand** in the <%= vars.product_tile %> tile.
  * The cluster upgrade process recreates all clusters, which might cause cluster outages.
  For more information, see [What Happens During Cluster Upgrades](#cluster-upgrades) below.

### <a name="control-plane-upgrades"></a><%= vars.k8s_runtime_abbr %> Control Plane Upgrades

During a **<%= vars.k8s_runtime_abbr %> control plane only** upgrade, 
the <%= vars.product_tile %> tile does the following:  

1. Upgrades the <%= vars.k8s_runtime_abbr %> control plane, which includes the 
<%= vars.control_plane %> and UAA servers and the <%= vars.k8s_runtime_abbr %> 
database. 
This control plane upgrade causes temporary outages as described in 
[Control Plane Outages](#outages) below.  

The upgrade process does not upgrade <%= vars.k8s_runtime_abbr %>-provisioned Kubernetes clusters 
and they will remain on the previous <%= vars.k8s_runtime_abbr %> version until you manually upgrade them.  

Some cluster management tasks are not supported for <%= vars.k8s_runtime_abbr %>-provisioned clusters that are running the previous <%= vars.k8s_runtime_abbr %> version. 
For more information, see [Tasks Supported Following a <%= vars.k8s_runtime_abbr %> Control Plane Upgrade](#control-plane-upgrades-supported-tasks) below.  

#### <a name="control-plane-upgrades-supported-tasks"></a>Tasks Supported Following a <%= vars.k8s_runtime_abbr %> Control Plane Upgrade

Some cluster management tasks are not supported for <%= vars.k8s_runtime_abbr %>-provisioned clusters that are running the previous <%= vars.k8s_runtime_abbr %> version.  

The following summarizes the <%= vars.k8s_runtime_abbr %> CLI cluster management tasks supported for <%= vars.k8s_runtime_abbr %>-provisioned clusters that are running the previous <%= vars.k8s_runtime_abbr %> version:  

<table>
  <tr>
    <th>Support Status</th>
    <th>Tasks</th>
    <th>Notes</th>
  </tr>
  <tr>
    <th>Supported</th> 
    <td>
      <ul>
        <li><code>tkgi certificates</code></li>
        <li><code>tkgi cluster</code></li>
        <li><code>tkgi create-cluster</code></li>
        <li><code>tkgi delete-cluster</code></li>
        <li><code>tkgi get-credentials</code></li>
        <li><code>tkgi get-kubeconfig</code></li>
        <li><code>tkgi upgrade-cluster</code></li>
        <li><code>tkgi upgrade-clusters</code></li>
      </ul>
    </td>
    <td>
    </td>
  </tr>
  <tr>
    <th>Partially-Supported</th> 
    <td>
      <ul>
        <li><code>tkgi update-cluster</code><br>(For more information, see the <strong>Notes</strong> column).</li>  
      </ul>
    </td>
    <td>
      Supported <code>tkgi update-cluster</code> flags:
      <ul>
        <li><code>--num-nodes INT32</code> (Requires <%= vars.k8s_runtime_abbr %> v 1.15.5 or later.)</li>  
        <li><code>--node-pool-instances</code></li>  
      </ul>
      Unsupported <code>tkgi update-cluster</code> flags:
      <ul>
        <li><code>--compute-profile</code></li>  
        <li><code>--kubelet-drain-timeout</code></li>  
        <li><code>--kubelet-drain-grace-period</code></li>  
        <li><code>--kubelet-drain-force</code></li>  
        <li><code>--kubelet-drain-ignore-daemonsets</code></li>  
        <li><code>--kubelet-drain-delete-local-data</code></li>  
        <li><code>--kubelet-drain-force-node</code></li>  
        <li><code>--kubernetes-profile</code></li>   
        <li><code>--network-profile</code></li>  
        <li><code>--tags []ClusterTag</code></li>  
        <li><code>--config-file</code></li>  
      </ul>
    </td>
  </tr>
  <tr>
    <th>Unsupported</th> 
    <td>
      <ul>
        <li><code>tkgi rotate-certificates</code></li>  
      </ul>
    </td>
    <td>
    </td>
  </tr>
</table>

<p class="note"><strong>Note</strong>: <%= vars.recommended_by %> recommends against performing cluster management tasks, including the supported cluster management tasks listed above, on a cluster that is running an older <%= vars.k8s_runtime_abbr %> version than the current <%= vars.k8s_runtime_abbr %> control plane version.
</p>

The following <%= vars.k8s_runtime_abbr %> CLI utility tasks remain fully-supported while <%= vars.k8s_runtime_abbr %> clusters are running the previous version of <%= vars.k8s_runtime_abbr %>:  

* `tkgi cancel-task`  
* `tkgi clusters`  
* `tkgi compute-profile`  
* `tkgi compute-profiles`  
* `tkgi create-compute-profile`  
* `tkgi create-kubernetes-profile`  
* `tkgi create-network-profile`  
* `tkgi delete-compute-profile`  
* `tkgi delete-kubernetes-profile`  
* `tkgi delete-network-profile`  
* `tkgi kubernetes-profile`  
* `tkgi kubernetes-profiles`  
* `tkgi login`  
* `tkgi logout`  
* `tkgi network-profile`  
* `tkgi network-profiles`  
* `tkgi plans`  
* `tkgi task`  
* `tkgi tasks`  





### <a name="outages"></a> Control Plane Outages

When the <%= vars.product_short %> control plane is not scaled for high availability (beta), upgrading it temporarily interrupts the following:

  * Logging in to the <%= vars.k8s_runtime_abbr %> CLI and using all `tkgi` commands
  * Using the <%= vars.control_plane %> to retrieve information about clusters
  * Using the <%= vars.control_plane %> to create and delete clusters
  * Using the <%= vars.control_plane %> to resize clusters

These outages do not affect the Kubernetes clusters themselves.
During a <%= vars.k8s_runtime_abbr %> control plane upgrade, you can still interact with clusters and their workloads using the Kubernetes Command Line Interface, `kubectl`.

For more information about the <%= vars.k8s_runtime_abbr %> control plane and high availability (beta), see [<%= vars.k8s_runtime_abbr %> Control Plane Overview](control-plane.html#control-plane) in _<%= vars.product_short %> Architecture_.

### <a name="canary"></a>Canary Instances

The <%= vars.product_tile %> tile is a BOSH deployment.

BOSH-deployed products can set a number of canary instances to upgrade first, before the rest of the deployment VMs.
BOSH continues the upgrade only if the canary instance upgrade succeeds.
If the canary instance encounters an error, the upgrade stops running and other VMs are not affected.

The <%= vars.product_tile %> tile uses one canary instance when deploying or upgrading <%= vars.product_short %>.

## <a id="cluster-upgrades"></a>What Happens During Cluster Upgrades

Upgrading <%= vars.k8s_runtime_abbr %>-provisioned Kubernetes clusters
updates their Kubernetes version to the version
included with the <%= vars.product_tile %> tile.
It also updates the <%= vars.k8s_runtime_abbr %> version tagged in your clusters
to the <%= vars.product_tile %> tile version.

You can upgrade <%= vars.k8s_runtime_abbr %>-provisioned Kubernetes clusters either through the <%= vars.product_tile %> tile
or the <%= vars.k8s_runtime_abbr %> CLI. See the table below.

<table>
<col width="50%">
<col width="50%">
  <tr>
    <th>This method</th>
    <th>Upgrades</th>
  </tr>
  <tr>
    <td>The <strong>Upgrade all clusters errand</strong> in<br>
    the <strong><%= vars.product_tile %></strong> tile > <strong>Errands</strong></td>
    <td>All clusters. Clusters are upgraded serially.</td>
  </tr>
  <tr>
    <td><code>tkgi upgrade-cluster</code></td>
    <td>One cluster.</td>
  </tr>
  <tr>
    <td><code>tkgi upgrade-clusters</code></td>
    <td>Multiple clusters. Clusters are upgraded serially or in parallel.</td>
  </tr>
</table>

During an upgrade of <%= vars.k8s_runtime_abbr %>-provisioned clusters,
<%= vars.product_short %> recreates your clusters.
This includes the following stages for each cluster you upgrade:

1. Control Plane nodes are recreated.
1. Worker nodes are recreated.

Depending on your cluster configuration,
these recreations might cause [Control Plane Nodes Outage](#master) or [Worker Nodes Outage](#worker)
as described below.

###<a name="master"></a>Control Plane Nodes Outage

When <%= vars.product_short %> upgrades a single-control plane node cluster,
you cannot interact with your cluster, use `kubectl`, or push new workloads.

To avoid this loss of functionality,
<%= vars.recommended_by %> recommends using multi-control plane node clusters.

###<a name="worker"></a>Worker Nodes Outage

When <%= vars.product_short %> upgrades a worker node,
the node stops running containers.
If your workloads run on a single node, they will experience downtime.

To avoid downtime for stateless workloads,
<%= vars.recommended_by %> recommends using at least one worker node
per availability zone (AZ).
For stateful workloads, <%= vars.recommended_by %> recommends
using a minimum of two worker nodes per AZ.

<%# Note: The formatting on this page breaks when notes are configured the normal way.%>
<p class="note">
<strong>Note:</strong> When the <strong>Upgrade all clusters errand</strong>
is enabled in the <%= vars.product_tile %> tile, updating the tile with
a new Linux or Windows stemcell rolls every Linux or Windows VM in each Kubernetes cluster.
This automatic rolling ensures that all your VMs are patched.
To avoid workload downtime, use the resource configuration recommended
in <a href="#master">Control Plane Nodes Outage</a>
and <a href="#worker">Worker Nodes Outage</a> above and
in <a href="./maintain-uptime.html">Maintaining Workload Uptime</a>.
</p>
<%# Note: The formatting on this page breaks when notes are configured the normal way.%>


##<a id="cni"></a> About Switching from the Flannel CNI to the Antrea CNI

<%= vars.product_short %> supports Antrea, Flannel, and NSX-T 
as the Container Network Interfaces (CNIs) for <%= vars.k8s_runtime_abbr %>-provisioned clusters.  

<%= vars.recommended_by %> recommends the Antrea CNI over Flannel. 
The Antrea CNI provides Kubernetes Network Policy support for non-NSX-T environments. 
Antrea CNI-configured clusters are supported on AWS, Azure, and vSphere without NSX-T environments.  

For more information about Antrea, see [Antrea](https://antrea.io/) in the Antrea documentation.  

<p class="note">
<strong>Note:</strong> Support for the Flannel Container Networking Interface (CNI) is deprecated.
</p>

<%= vars.recommended_by %> recommends that you configure Antrea as the default <%= vars.k8s_runtime_abbr %>-provisioned cluster CNI, 
and that you switch your Flannel CNI-configured clusters to the Antrea CNI.  




###<a id="upgrade-the-cni"></a> Switch from the Flannel CNI to Antrea

You can configure <%= vars.k8s_runtime_abbr %> to network newly created <%= vars.k8s_runtime_abbr %>-provisioned clusters 
with the Antrea CNI.  

Configure the <%= vars.k8s_runtime_abbr %> default CNI during <%= vars.k8s_runtime_abbr %> installation and upgrade only.  

During <%= vars.k8s_runtime_abbr %> installation:  

* Configure the <%= vars.k8s_runtime_abbr %> default CNI as either Antrea or vSphere with NSX-T.  

During <%= vars.k8s_runtime_abbr %> upgrades:  

* You can optionally change the <%= vars.k8s_runtime_abbr %> default CNI from the deprecated Flannel CNI to Antrea.  
* Do not change the CNI configuration from Antrea to Flannel.  
* Do not change the CNI configuration from or to NSX-T after your initial <%= vars.k8s_runtime_abbr %> installation.  


If you initially configured <%= vars.k8s_runtime_abbr %> to use Flannel as the default CNI and 
switch to Antrea as the default CNI during a <%= vars.k8s_runtime_abbr %> upgrade:  

* Existing Flannel-configured clusters remain networked using Flannel. 
    Your existing Flannel clusters will not be migrated to Antrea.  
* New clusters created after the upgrade are created using Antrea as their CNI.  

<p class="note warning">
<strong>Warning:</strong>  
Do not change the <%= vars.k8s_runtime_abbr %> default CNI configuration between upgrades.  
</p>

<br>
For information about selecting and configuring a CNI for <%= vars.k8s_runtime_abbr %>, see the _Networking_ section of the installation documentation for your environment:  

* [Installing <%= vars.product_full %> on AWS](installing-aws.html#networking)  
* [Installing <%= vars.product_full %> on Azure](installing-azure.html#networking)  
* [Installing <%= vars.product_full %> on vSphere](installing-vsphere.html#networking)  

<%# Note: The formatting on this page breaks when notes are configured the normal way.%>